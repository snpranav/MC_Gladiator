{"episode_reward_max": NaN, "episode_reward_min": NaN, "episode_reward_mean": NaN, "episode_len_mean": NaN, "episode_media": {}, "episodes_this_iter": 0, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [], "episode_lengths": []}, "sampler_perf": {}, "off_policy_estimator": {}, "num_healthy_workers": 2, "timesteps_total": 1000, "timesteps_this_iter": 256, "agent_timesteps_total": 2000, "timers": {"load_time_ms": 1.353, "load_throughput": 189238.954, "learn_time_ms": 383.286, "learn_throughput": 667.909, "update_time_ms": 2.517}, "info": {"learner": {"policy_01": {"custom_metrics": {}, "learner_stats": {"mean_q": 0.5356957912445068, "min_q": -4.712375164031982, "max_q": 5.039112567901611, "cur_lr": 0.0005}, "model": {}, "td_error": [0.15828895568847656, -9.270929336547852, -4.018104076385498, -5.743231773376465, -2.43561053276062, -3.5972352027893066, -0.06519913673400879, -2.9353747367858887, -6.752025604248047, -3.386932373046875, -2.942467451095581, -4.678939342498779, 0.09242558479309082, -5.247453689575195, -1.4466345310211182, -7.159849166870117, -0.46846985816955566, -1.8593862056732178, -6.497836112976074, -0.6005642414093018, -1.1928915977478027, -2.043834686279297, -0.1731877326965332, -10.928971290588379, -2.2519655227661133, -5.736308574676514, -0.242462158203125, -0.8075158596038818, -8.473240852355957, -6.536278247833252, -5.320801258087158, -2.9807095527648926, -10.989129066467285, -0.1731877326965332, -6.094322204589844, -0.20065808296203613, -6.217169284820557, -0.28720831871032715, -0.032193899154663086, -4.00097131729126, -0.8445847034454346, -7.130099296569824, -0.8445847034454346, 0.07783699035644531, -1.9058016538619995, -1.0984876155853271, -7.374180316925049, 0.04082345962524414, -0.6405453681945801, -5.184638500213623, -5.107182025909424, -5.208194732666016, -2.197652816772461, -6.752025604248047, -9.28361988067627, -1.8663649559020996, -5.123574256896973, -5.817095756530762, -8.670783996582031, -8.025617599487305, -9.308708190917969, -1.2588942050933838, -1.6752617359161377, -0.02174854278564453, -3.269744873046875, -5.117046356201172, -8.015742301940918, -0.3191378116607666, -1.6194536685943604, -6.368309020996094, -2.5385777950286865, -2.8433194160461426, -9.40119743347168, -1.7487006187438965, -8.59826946258545, -5.80699348449707, -1.342587947845459, 0.06457066535949707, -3.517086982727051, 0.045523643493652344, -6.93430757522583, 0.0499873161315918, -3.1037349700927734, 0.048317909240722656, -1.901077389717102, -8.795316696166992, -2.2225470542907715, -0.594355583190918, -0.7128276824951172, -5.103682041168213, -0.5243954658508301, -0.5719883441925049, -5.80280065536499, 0.04522848129272461, -6.482274055480957, -1.0787596702575684, -4.723369598388672, -1.5615577697753906, -2.3143999576568604, 0.5174226760864258, -6.453914165496826, -5.925230979919434, -4.00097131729126, -1.2701544761657715, -8.60984992980957, -0.1028895378112793, -0.6697895526885986, -3.365222454071045, -7.750481605529785, -6.294156074523926, -0.30843400955200195, -1.5708930492401123, -6.5042009353637695, -0.7515096664428711, 0.008791923522949219, -9.760869026184082, -9.270929336547852, 0.036101579666137695, -2.5182149410247803, -6.929851055145264, 0.03560495376586914, -2.044431686401367, -5.8566789627075195, -4.676835536956787, -7.547184944152832, -3.026437282562256, -7.758416175842285, -7.945739269256592, -4.748883247375488, -0.32344818115234375, 0.11734151840209961, 0.0703892707824707, -7.579251289367676, -0.06732964515686035, -2.3441672325134277, -9.273517608642578, -0.46846985816955566, -6.32042121887207, -8.245458602905273, -3.9840407371520996, -8.155729293823242, -0.6711328029632568, -1.7605714797973633, 0.028317928314208984, -6.505124092102051, -0.4060380458831787, -6.215555667877197, -0.517819881439209, -0.2515287399291992, -6.6251702308654785, -8.08729076385498, -1.3780102729797363, -0.5915958881378174, -0.3748433589935303, -1.1438078880310059, -2.658883571624756, -2.5385777950286865, -1.901628017425537, -1.889650821685791, -11.043486595153809, -4.678939342498779, -2.3143999576568604, -9.342048645019531, -2.1029105186462402, -4.720180034637451, -8.370777130126953, -1.5708930492401123, -6.18536376953125, 0.051911115646362305, -5.232757568359375, -9.314523696899414, -0.6102755069732666, -2.043834686279297, -0.6798782348632812, -4.705605983734131, -7.555084705352783, 0.03192901611328125, -0.5303976535797119, -6.635424613952637, -2.6494007110595703, -1.558424711227417, -6.455705642700195, -0.6671805381774902, -7.911850929260254, -6.020942211151123, -4.947351932525635, -6.884831428527832, -1.752651572227478, -0.22052240371704102, -8.015742301940918, -0.09000062942504883, -0.6804320812225342, -1.889650821685791, -3.036107063293457, -6.217169284820557, -6.316031455993652, -2.7178397178649902, -0.9218566417694092, -1.2515597343444824, -0.7223422527313232, -2.6676785945892334, -1.583568811416626, -6.662045478820801, -0.46846985816955566, -0.14734983444213867, 0.04346418380737305, -8.596481323242188, -3.9840407371520996, -3.386932373046875, -1.8919739723205566, -1.888150691986084, -1.4349784851074219, -0.8804333209991455, -1.571420669555664, -0.5456933975219727, -0.8850522041320801, -0.6697895526885986, -1.8895349502563477, -3.6916446685791016, -1.583568811416626, -4.420754909515381, -7.95302152633667, -0.2849764823913574, -6.452451705932617, -2.7069315910339355, -7.601454257965088, -7.066658020019531, -8.234137535095215, -1.8892464637756348, -4.623369216918945, -0.7101702690124512, -1.495624303817749, -5.151693344116211, -1.1928915977478027, -0.7537472248077393, -1.9080771207809448, -3.980496883392334, -7.6373491287231445, -6.662045478820801, -0.7537472248077393, -1.648247480392456, 0.06467199325561523, -6.5042009353637695, -3.0811948776245117, -1.253782033920288, -1.8063156604766846, -2.322150707244873, -3.123406171798706, -6.831768989562988, -7.9112958908081055, -7.788908958435059, -6.946499824523926, -4.926897048950195, -0.5815982818603516, -1.5284831523895264, -7.031108856201172], "mean_td_error": -3.5708305835723877}}, "num_steps_sampled": 1000, "num_agent_steps_sampled": 2000, "num_steps_trained": 256, "num_steps_trained_this_iter": 256, "num_agent_steps_trained": 512, "last_target_update_ts": 1000, "num_target_updates": 1}, "done": false, "episodes_total": 0, "training_iteration": 1, "trial_id": "e21e6_00000", "experiment_id": "434cd42d33fd4b638f17fc69fa01d74f", "date": "2022-06-14_19-02-08", "timestamp": 1655247728, "time_this_iter_s": 9.579994678497314, "time_total_s": 9.579994678497314, "pid": 2568314, "hostname": "lambda-dual", "node_ip": "10.104.51.19", "config": {"num_workers": 2, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.0005, "train_batch_size": 256, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "cnet", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "1v1env", "observation_space": null, "action_space": null, "env_config": {}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": true, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": null, "evaluation_duration": 10, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"num_workers": 2, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.0005, "train_batch_size": 256, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "cnet", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "1v1env", "observation_space": null, "action_space": null, "env_config": {}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": false, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": null, "evaluation_duration": 10, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"explore": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": true, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": null, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 2, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"policy_01": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]], [[[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]], (3, 64, 64), float32)", "Discrete(7)", {}], "policy_02": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]], [[[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]], (3, 64, 64), float32)", "Discrete(7)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": "<function <lambda> at 0x7f1aa37475f0>", "policies_to_train": ["policy_01"], "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": 100000, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": false, "hiddens": [], "double_q": false, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": true, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": null, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 2, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"policy_01": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]], [[[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]], (3, 64, 64), float32)", "Discrete(7)", {}], "policy_02": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]], [[[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]], (3, 64, 64), float32)", "Discrete(7)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": "<function <lambda> at 0x7f1aa37475f0>", "policies_to_train": ["policy_01"], "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": 100000, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": false, "hiddens": [], "double_q": false, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "time_since_restore": 9.579994678497314, "timesteps_since_restore": 256, "iterations_since_restore": 1, "warmup_time": 45.46659183502197, "perf": {"cpu_util_percent": 32.057142857142864, "ram_util_percent": 9.099999999999998}}
{"episode_reward_max": 204.0, "episode_reward_min": 0.0, "episode_reward_mean": 102.0, "episode_len_mean": 601.0, "episode_media": {}, "episodes_this_iter": 2, "policy_reward_min": {"policy_01": 0.0, "policy_02": 0.0}, "policy_reward_max": {"policy_01": 204.0, "policy_02": 0.0}, "policy_reward_mean": {"policy_01": 102.0, "policy_02": 0.0}, "custom_metrics": {}, "hist_stats": {"episode_reward": [0.0, 204.0], "episode_lengths": [601, 601], "policy_policy_01_reward": [0.0, 204.0], "policy_policy_02_reward": [0.0, 0.0]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.3257663576276629, "mean_inference_ms": 5.876916271823269, "mean_action_processing_ms": 0.08822035241674829, "mean_env_wait_ms": 8.984044357970522, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 2, "timesteps_total": 2000, "timesteps_this_iter": 256, "agent_timesteps_total": 4000, "timers": {"load_time_ms": 1.3, "load_throughput": 196904.848, "learn_time_ms": 11.588, "learn_throughput": 22092.68, "update_time_ms": 2.026}, "info": {"learner": {"policy_01": {"custom_metrics": {}, "learner_stats": {"mean_q": 34.8156852722168, "min_q": 19.404399871826172, "max_q": 38.4355583190918, "cur_lr": 0.0005}, "model": {}, "td_error": [-0.2028961181640625, 0.20676422119140625, 0.4315834045410156, 0.2166290283203125, -0.4438285827636719, 0.4751167297363281, -0.33466339111328125, -0.0049285888671875, 0.3029937744140625, 0.30210113525390625, 0.17427444458007812, 0.14056396484375, -0.5577239990234375, 0.877960205078125, 0.2872123718261719, 0.21869659423828125, 0.10702133178710938, 0.7952651977539062, -0.01433563232421875, -3.723255157470703, -0.5156631469726562, -1.605438232421875, -0.425079345703125, -4.076385498046875, 0.054836273193359375, 0.2548942565917969, 0.2967109680175781, -0.3238525390625, -2.1714096069335938, -0.2710723876953125, 0.3006172180175781, -3.984283447265625, -0.6875267028808594, 0.21821212768554688, -0.9455986022949219, -1.2854366302490234, 0.19915390014648438, 0.2396087646484375, 0.5538291931152344, -0.8744888305664062, -0.2508125305175781, 0.3319816589355469, -0.3569450378417969, 0.314453125, -0.1825103759765625, 0.027561187744140625, 0.06488800048828125, -0.4578514099121094, -0.11879348754882812, -0.0714263916015625, -0.2655372619628906, -0.22194671630859375, -2.2402610778808594, 0.1335601806640625, -0.5905952453613281, -0.061267852783203125, 0.5604934692382812, 0.2867279052734375, -6.442188262939453, 0.45909881591796875, -0.259368896484375, -0.065765380859375, 0.4187278747558594, -0.11867523193359375, -0.4732933044433594, 0.30687713623046875, 0.14007949829101562, 0.12474441528320312, 0.624267578125, -0.12646102905273438, -6.199733734130859, -0.15340423583984375, -0.3298530578613281, -0.03954315185546875, 0.5006446838378906, 0.363189697265625, -0.2736854553222656, 0.3212547302246094, 2.554300308227539, 0.30342864990234375, -0.0929412841796875, -0.9011688232421875, 0.006198883056640625, 0.38492584228515625, -0.3124504089355469, -0.41078948974609375, -0.20470046997070312, 0.3593482971191406, -0.1646575927734375, -0.08452224731445312, 0.2857627868652344, -0.11879348754882812, -0.19585800170898438, -0.18853378295898438, -0.2651329040527344, 0.4151268005371094, -0.8303489685058594, 0.10137939453125, -1.86590576171875, -2.2429275512695312, -0.576690673828125, -4.545230865478516, 0.23436355590820312, 0.3137664794921875, 0.8509674072265625, 0.2897377014160156, 0.0654296875, -0.44886016845703125, 0.013957977294921875, 0.7512779235839844, -1.4052047729492188, 1.1205101013183594, -0.11429977416992188, -0.07791519165039062, 0.12255096435546875, 0.2903785705566406, 0.4613037109375, -0.2871513366699219, -0.0989990234375, 0.2017822265625, -2.1714096069335938, 0.26941680908203125, -0.20848464965820312, -0.4967041015625, -0.8303489685058594, -0.6060867309570312, 0.22406768798828125, 0.9494438171386719, 0.4185600280761719, 0.3404350280761719, 0.0751190185546875, 0.18931198120117188, 0.2606239318847656, 1.1626205444335938, 0.14559555053710938, 0.2746734619140625, 0.45775604248046875, 0.7213706970214844, -0.41078948974609375, 20.18405532836914, -3.7109146118164062, 0.5528411865234375, 1.0472068786621094, -0.3924713134765625, 0.35198211669921875, 0.4219093322753906, 1.2048187255859375, 0.47554779052734375, 0.2903785705566406, -5.792808532714844, -5.040046691894531, 0.28462982177734375, -3.7780799865722656, -0.3124504089355469, 0.14056396484375, -0.425048828125, 0.465911865234375, 0.5567855834960938, 0.401519775390625, 0.5034523010253906, 0.3011474609375, -3.350738525390625, -0.9771614074707031, -0.28893280029296875, -5.525581359863281, -0.5695304870605469, -0.2736854553222656, -0.13797760009765625, 0.1807270050048828, -0.3124504089355469, -0.7461776733398438, -0.015960693359375, 0.20331192016601562, 0.46428680419921875, -1.0641803741455078, -0.21587181091308594, -0.9061965942382812, -0.45259857177734375, 0.4369964599609375, 0.9382858276367188, 0.16448974609375, -2.5280838012695312, -0.1889801025390625, 0.5261688232421875, 0.29532623291015625, -1.627035140991211, 0.04841041564941406, 0.47168731689453125, -5.792808532714844, 0.21058273315429688, -0.45259857177734375, -0.43027496337890625, 0.22710800170898438, 0.4698486328125, -1.1990890502929688, -0.4299507141113281, -0.10864639282226562, -0.0904388427734375, -7.007892608642578, 0.19792938232421875, -4.679004669189453, 0.17810440063476562, 0.43280792236328125, 1.2938804626464844, -4.203056335449219, -0.36350250244140625, 0.2975883483886719, 0.02036285400390625, 0.4826164245605469, 0.9660987854003906, -0.425079345703125, -0.17576217651367188, 0.47713470458984375, -0.05185699462890625, 1.039794921875, -1.86590576171875, -0.7461776733398438, -0.3524436950683594, 0.39270782470703125, 0.3275489807128906, 0.7438430786132812, -4.076385498046875, 20.18405532836914, -4.35969352722168, -2.8017234802246094, 0.18686294555664062, 0.46428680419921875, 0.20331192016601562, 0.3535652160644531, 0.3029937744140625, 0.11954116821289062, -0.4147300720214844, -0.19874954223632812, -1.62298583984375, -1.3661613464355469, -0.00717926025390625, 0.24910354614257812, 0.8509674072265625, 0.47088623046875, 0.13554000854492188, 0.18294525146484375, 0.28769683837890625, -0.08913040161132812, 0.26694488525390625, 0.13156890869140625, 0.052978515625, -1.0404853820800781, 0.18136215209960938, -0.24788284301757812, 0.1193695068359375, -0.6060867309570312, -0.0929412841796875, 0.30342864990234375, -4.203056335449219, -0.5903816223144531, -0.24662399291992188], "mean_td_error": -0.23793143033981323}}, "num_steps_sampled": 2000, "num_agent_steps_sampled": 4000, "num_steps_trained": 32256, "num_steps_trained_this_iter": 256, "num_agent_steps_trained": 64512, "last_target_update_ts": 1504, "num_target_updates": 2}, "done": false, "episodes_total": 2, "training_iteration": 2, "trial_id": "e21e6_00000", "experiment_id": "434cd42d33fd4b638f17fc69fa01d74f", "date": "2022-06-14_19-02-29", "timestamp": 1655247749, "time_this_iter_s": 20.731234550476074, "time_total_s": 30.31122922897339, "pid": 2568314, "hostname": "lambda-dual", "node_ip": "10.104.51.19", "config": {"num_workers": 2, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.0005, "train_batch_size": 256, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "cnet", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "1v1env", "observation_space": null, "action_space": null, "env_config": {}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": true, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": null, "evaluation_duration": 10, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"num_workers": 2, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.0005, "train_batch_size": 256, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "cnet", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "1v1env", "observation_space": null, "action_space": null, "env_config": {}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": false, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": null, "evaluation_duration": 10, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"explore": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": true, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": null, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 2, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"policy_01": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]], [[[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]], (3, 64, 64), float32)", "Discrete(7)", {}], "policy_02": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]], [[[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]], (3, 64, 64), float32)", "Discrete(7)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": "<function <lambda> at 0x7f1aa375f950>", "policies_to_train": ["policy_01"], "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": 100000, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": false, "hiddens": [], "double_q": false, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": true, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": null, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 2, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"policy_01": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]], [[[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]], (3, 64, 64), float32)", "Discrete(7)", {}], "policy_02": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]], [[[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]], (3, 64, 64), float32)", "Discrete(7)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": "<function <lambda> at 0x7f1aa375f950>", "policies_to_train": ["policy_01"], "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": 100000, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": false, "hiddens": [], "double_q": false, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "time_since_restore": 30.31122922897339, "timesteps_since_restore": 512, "iterations_since_restore": 2, "warmup_time": 45.46659183502197, "perf": {"cpu_util_percent": 25.37666666666667, "ram_util_percent": 9.276666666666669}}
{"episode_reward_max": 204.0, "episode_reward_min": 0.0, "episode_reward_mean": 51.0, "episode_len_mean": 601.0, "episode_media": {}, "episodes_this_iter": 2, "policy_reward_min": {"policy_01": 0.0, "policy_02": 0.0}, "policy_reward_max": {"policy_01": 204.0, "policy_02": 0.0}, "policy_reward_mean": {"policy_01": 51.0, "policy_02": 0.0}, "custom_metrics": {}, "hist_stats": {"episode_reward": [0.0, 204.0, 0.0, 0.0], "episode_lengths": [601, 601, 601, 601], "policy_policy_01_reward": [0.0, 204.0, 0.0, 0.0], "policy_policy_02_reward": [0.0, 0.0, 0.0, 0.0]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.3327565752635209, "mean_inference_ms": 5.898611273413472, "mean_action_processing_ms": 0.08887791049960837, "mean_env_wait_ms": 8.87109299276342, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 2, "timesteps_total": 3000, "timesteps_this_iter": 256, "agent_timesteps_total": 6000, "timers": {"load_time_ms": 1.318, "load_throughput": 194265.057, "learn_time_ms": 12.062, "learn_throughput": 21223.424, "update_time_ms": 2.207}, "info": {"learner": {"policy_01": {"custom_metrics": {}, "learner_stats": {"mean_q": 35.46007537841797, "min_q": 19.61315155029297, "max_q": 39.53843307495117, "cur_lr": 0.0005}, "model": {}, "td_error": [-0.2447662353515625, -0.6902313232421875, -1.8902702331542969, -0.17037200927734375, -0.4588127136230469, -1.3357696533203125, 0.18288040161132812, 0.2679023742675781, -0.4034423828125, -0.055309295654296875, -0.55657958984375, -0.04302978515625, -4.054988861083984, 0.043865203857421875, -0.23273468017578125, 0.2001190185546875, 0.20178604125976562, -2.4002304077148438, -0.21949005126953125, -0.30356788635253906, -0.21352767944335938, 0.3456611633300781, -0.27823638916015625, 0.19799041748046875, -0.6281280517578125, 0.06624984741210938, -0.3218574523925781, 0.23502349853515625, 34.7331657409668, -0.007415771484375, 0.2530097961425781, 0.19015121459960938, 0.2676811218261719, -0.2551460266113281, 0.00353240966796875, -0.2663459777832031, -0.31341552734375, -0.17716598510742188, 34.71372985839844, -0.07316207885742188, 0.17574310302734375, 0.045688629150390625, -0.3286895751953125, 0.4390296936035156, -4.2584075927734375, 34.71372985839844, -0.07448959350585938, 0.18288040161132812, 0.3102874755859375, -0.22016143798828125, -4.3895111083984375, -0.0641326904296875, 0.2422027587890625, 0.20229339599609375, 0.6597061157226562, 0.7590522766113281, -0.3021736145019531, -0.587371826171875, 0.31650543212890625, 0.5126762390136719, 0.20303726196289062, 0.21411895751953125, -0.3193817138671875, 0.15537261962890625, -0.288818359375, 0.0582427978515625, 0.34769439697265625, -0.14116287231445312, 0.5529747009277344, -1.5238418579101562, -0.2301788330078125, -4.189189910888672, 0.17413711547851562, 0.3165435791015625, -0.21474456787109375, -1.6783580780029297, 7.2479248046875e-05, 0.06393814086914062, 0.2504539489746094, -0.19624710083007812, -5.9671783447265625, 0.5598182678222656, -4.7228240966796875, 0.18743515014648438, -0.01108551025390625, -0.052104949951171875, 0.15900039672851562, -0.1992969512939453, -0.15305709838867188, 0.233795166015625, 0.374481201171875, 0.2867393493652344, 0.7710342407226562, 0.2748222351074219, -0.2355194091796875, 0.167999267578125, 0.009685516357421875, -0.356475830078125, 0.2768974304199219, 0.5678024291992188, 0.3469734191894531, 0.1009368896484375, -4.9747314453125, -0.13193893432617188, 0.2675514221191406, -0.5915412902832031, -0.13696670532226562, 0.4495697021484375, 0.11156845092773438, -0.044414520263671875, 0.6614761352539062, 0.3952217102050781, 0.14175796508789062, 0.3592948913574219, -0.3034782409667969, -0.10404586791992188, -0.03797149658203125, -0.16127777099609375, 0.0949249267578125, 0.027713775634765625, -0.043182373046875, -2.4002304077148438, -0.12876129150390625, 0.14740753173828125, -0.13570785522460938, -0.10312271118164062, 0.3672142028808594, 0.19047164916992188, -0.0760345458984375, 0.28009033203125, 0.17975997924804688, -4.942256927490234, 0.2857246398925781, -0.2936820983886719, -0.3957252502441406, 0.07602691650390625, 0.8334274291992188, -0.4568634033203125, 0.08643341064453125, 0.17307662963867188, 0.6545486450195312, -0.47541046142578125, -0.3117218017578125, 0.17844390869140625, 0.11415863037109375, 0.4347648620605469, -0.00772857666015625, 0.2153167724609375, 0.045116424560546875, 0.33647918701171875, 0.7870521545410156, -5.584087371826172, 1.1307945251464844, -0.3107757568359375, -0.44960784912109375, -0.6831207275390625, 0.13144302368164062, -0.09337615966796875, -0.000286102294921875, 0.3484153747558594, -0.007415771484375, 0.49767303466796875, 0.029529571533203125, -0.056915283203125, 0.015899658203125, -5.195049285888672, -0.18861770629882812, 0.2508430480957031, -0.09182357788085938, 0.03510284423828125, 0.6004867553710938, -0.17892074584960938, -0.5108680725097656, 0.18597412109375, -0.09769630432128906, 0.16133499145507812, 0.21056747436523438, -0.4132194519042969, -0.11503982543945312, -5.2596282958984375, -0.18707656860351562, -0.19094467163085938, 0.13583755493164062, 0.27588653564453125, 0.4493255615234375, -0.16175460815429688, 35.8895149230957, -0.06937789916992188, -1.8902702331542969, -0.3477363586425781, -0.6139450073242188, 0.024078369140625, -0.176422119140625, 0.07511520385742188, 1.0896263122558594, 0.164703369140625, 0.12435531616210938, 34.7331657409668, 0.8460121154785156, 0.24993896484375, 0.0232696533203125, 0.09395599365234375, -0.02272796630859375, 0.30194854736328125, -0.20843505859375, -0.33591461181640625, 0.10540008544921875, 34.7331657409668, -0.138275146484375, 0.20985794067382812, 0.12237548828125, -0.6448822021484375, -0.19094467163085938, -0.1192779541015625, -3.0071487426757812, -0.3258934020996094, -0.7425498962402344, -1.8787422180175781, -2.523242950439453, -0.21025466918945312, 0.5745315551757812, 0.11543655395507812, -0.9976806640625, 0.2714195251464844, 0.1499786376953125, 0.14686203002929688, 0.8533973693847656, 0.2867393493652344, -0.3094215393066406, -0.5356369018554688, 0.11775588989257812, -0.102325439453125, 0.3484153747558594, -0.6146774291992188, -0.27783203125, -1.3357696533203125, 0.3914222717285156, -0.3157768249511719, 0.7051544189453125, -0.14332199096679688, 1.9778404235839844, -0.8280143737792969, -5.683742523193359, -0.5218162536621094, -0.264404296875, 0.7590522766113281, 0.12919998168945312, 34.7331657409668, 0.28113555908203125, -0.8211212158203125, -0.34605979919433594, 0.15383529663085938, -0.020580291748046875, 0.024143218994140625, 0.43770599365234375, -0.3258934020996094], "mean_td_error": 0.6698921918869019}}, "num_steps_sampled": 3000, "num_agent_steps_sampled": 6000, "num_steps_trained": 64256, "num_steps_trained_this_iter": 256, "num_agent_steps_trained": 128512, "last_target_update_ts": 2512, "num_target_updates": 4}, "done": false, "episodes_total": 4, "training_iteration": 3, "trial_id": "e21e6_00000", "experiment_id": "434cd42d33fd4b638f17fc69fa01d74f", "date": "2022-06-14_19-02-50", "timestamp": 1655247770, "time_this_iter_s": 20.866604566574097, "time_total_s": 51.177833795547485, "pid": 2568314, "hostname": "lambda-dual", "node_ip": "10.104.51.19", "config": {"num_workers": 2, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.0005, "train_batch_size": 256, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "cnet", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "1v1env", "observation_space": null, "action_space": null, "env_config": {}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": true, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": null, "evaluation_duration": 10, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"num_workers": 2, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.0005, "train_batch_size": 256, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "cnet", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "1v1env", "observation_space": null, "action_space": null, "env_config": {}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": false, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": null, "evaluation_duration": 10, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"explore": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": true, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": null, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 2, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"policy_01": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]], [[[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]], (3, 64, 64), float32)", "Discrete(7)", {}], "policy_02": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]], [[[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]], (3, 64, 64), float32)", "Discrete(7)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": "<function <lambda> at 0x7f1aa375fef0>", "policies_to_train": ["policy_01"], "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": 100000, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": false, "hiddens": [], "double_q": false, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": true, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": null, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 2, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"policy_01": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]], [[[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]], (3, 64, 64), float32)", "Discrete(7)", {}], "policy_02": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]], [[[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]], (3, 64, 64), float32)", "Discrete(7)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": "<function <lambda> at 0x7f1aa375fef0>", "policies_to_train": ["policy_01"], "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": 100000, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": false, "hiddens": [], "double_q": false, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "time_since_restore": 51.177833795547485, "timesteps_since_restore": 768, "iterations_since_restore": 3, "warmup_time": 45.46659183502197, "perf": {"cpu_util_percent": 24.970000000000006, "ram_util_percent": 9.376666666666665}}
{"episode_reward_max": 204.0, "episode_reward_min": 0.0, "episode_reward_mean": 34.0, "episode_len_mean": 601.0, "episode_media": {}, "episodes_this_iter": 2, "policy_reward_min": {"policy_01": 0.0, "policy_02": 0.0}, "policy_reward_max": {"policy_01": 204.0, "policy_02": 0.0}, "policy_reward_mean": {"policy_01": 34.0, "policy_02": 0.0}, "custom_metrics": {}, "hist_stats": {"episode_reward": [0.0, 204.0, 0.0, 0.0, 0.0, 0.0], "episode_lengths": [601, 601, 601, 601, 601, 601], "policy_policy_01_reward": [0.0, 204.0, 0.0, 0.0, 0.0, 0.0], "policy_policy_02_reward": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.33723598291037327, "mean_inference_ms": 5.90294213574627, "mean_action_processing_ms": 0.08912855256787892, "mean_env_wait_ms": 8.778264414814386, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 2, "timesteps_total": 4000, "timesteps_this_iter": 256, "agent_timesteps_total": 8000, "timers": {"load_time_ms": 1.339, "load_throughput": 191169.517, "learn_time_ms": 12.495, "learn_throughput": 20487.503, "update_time_ms": 2.211}, "info": {"learner": {"policy_01": {"custom_metrics": {}, "learner_stats": {"mean_q": 36.52475357055664, "min_q": 22.0546875, "max_q": 43.53099060058594, "cur_lr": 0.0005}, "model": {}, "td_error": [0.5284309387207031, -0.05971527099609375, 0.3660926818847656, -0.06536102294921875, 1.5816268920898438, 0.0449676513671875, 0.22092056274414062, 34.226863861083984, 0.5527153015136719, 0.172149658203125, 0.3884124755859375, 0.3144035339355469, 0.14973831176757812, 0.18046188354492188, 1.2046623229980469, 0.06342315673828125, -0.096588134765625, 0.2678260803222656, 0.40244293212890625, 0.384368896484375, 0.0980682373046875, 0.9101943969726562, 0.006072998046875, 0.198638916015625, 0.6446151733398438, 0.18061065673828125, 0.18262863159179688, 0.003398895263671875, 0.949951171875, 0.07460784912109375, 0.3253746032714844, 0.7738189697265625, 0.8719673156738281, 0.36507415771484375, -0.052051544189453125, -1.999359130859375, 0.412506103515625, 0.3201713562011719, 0.2990760803222656, 0.5362472534179688, 0.21059036254882812, -0.48708343505859375, 0.185943603515625, 0.172576904296875, -0.039520263671875, 0.64892578125, 0.2545814514160156, 0.07244873046875, 0.6071739196777344, 0.15297317504882812, -0.06624603271484375, 0.6532974243164062, 0.37410736083984375, 0.5221099853515625, -0.22501754760742188, -0.021549224853515625, -5.507987976074219, -0.1915130615234375, 0.3542594909667969, -5.215412139892578, 0.015636444091796875, 0.3301200866699219, -0.3258628845214844, -0.025348663330078125, -0.14269256591796875, 0.3122673034667969, 0.2698822021484375, 0.3123054504394531, 0.3643951416015625, 0.002475738525390625, 1.8375244140625, 0.16716384887695312, 0.10200119018554688, 0.4059715270996094, 0.9101943969726562, -0.02082061767578125, -0.11834716796875, 0.6349105834960938, -5.4835662841796875, 0.08739852905273438, -0.012451171875, 0.17956924438476562, -0.2128143310546875, 0.27304840087890625, 0.046966552734375, 0.15807342529296875, 0.3554878234863281, 0.3243293762207031, 0.046649932861328125, -0.27791595458984375, 0.14409255981445312, -0.07306671142578125, 0.5050468444824219, 0.31392669677734375, 0.6372146606445312, 0.2790489196777344, 0.2528419494628906, 1.0068073272705078, 36.25990676879883, 0.305450439453125, -0.15018081665039062, -1.6393890380859375, -0.12747573852539062, 0.32422637939453125, 0.14558792114257812, -1.999359130859375, -0.12298202514648438, -0.4949493408203125, 0.10619354248046875, 0.16483688354492188, -0.482513427734375, -0.7572860717773438, 1.0201950073242188, 0.2258453369140625, 0.10327529907226562, 0.6622791290283203, 1.3500595092773438, -0.002490997314453125, 0.5364303588867188, 0.19346237182617188, 0.949951171875, -1.2313041687011719, -5.230319976806641, -0.102996826171875, -0.2878456115722656, -0.05019378662109375, -0.036640167236328125, 1.6558074951171875, 0.20888519287109375, 0.3459320068359375, 0.33348846435546875, 0.241455078125, 0.045986175537109375, -0.35878944396972656, 0.2560577392578125, 0.0128936767578125, 0.3746337890625, -0.3309211730957031, 0.2411041259765625, -0.7865028381347656, 0.4637184143066406, -0.2741813659667969, -4.78692626953125, 34.226863861083984, -0.2803497314453125, -0.21486282348632812, 0.056392669677734375, 0.7616539001464844, 0.4828834533691406, -5.507987976074219, 0.03420257568359375, -0.23865890502929688, 0.49619293212890625, 0.338165283203125, 0.161376953125, 0.6621932983398438, 0.4644203186035156, 0.01779937744140625, 36.25990676879883, 0.03420257568359375, 0.2906036376953125, 0.029567718505859375, 0.19124984741210938, 0.9052352905273438, 0.6151924133300781, -0.23711776733398438, 0.3166656494140625, 0.08544540405273438, 0.14075851440429688, 0.3762245178222656, 0.5726585388183594, -0.5087089538574219, 0.41182708740234375, 0.20551300048828125, -0.17359161376953125, 0.1640777587890625, 0.18435287475585938, 0.7982101440429688, 0.14810562133789062, 0.3943519592285156, 1.0050125122070312, -0.0361480712890625, 0.4638671875, -0.48325347900390625, -0.10232925415039062, -0.06624603271484375, 0.4172706604003906, -0.23693466186523438, 1.0634689331054688, 0.1610107421875, 0.16513442993164062, 0.20730972290039062, 0.005298614501953125, 0.3532600402832031, 0.6932182312011719, 0.4122467041015625, -0.000804901123046875, 0.17281341552734375, -1.7395782470703125, -3.6384010314941406, 0.36141204833984375, 0.6039810180664062, -0.25513458251953125, -0.09867095947265625, 0.73809814453125, 0.36196136474609375, 0.3296928405761719, -0.06912612915039062, -0.046482086181640625, 0.6349105834960938, 0.8211021423339844, 0.14932632446289062, 0.4013710021972656, -4.78692626953125, 0.3059654235839844, 34.226863861083984, 0.17832183837890625, -0.2803497314453125, 36.25990676879883, 0.4797782897949219, -0.000522613525390625, 0.7295379638671875, 0.3442802429199219, 0.9101943969726562, 0.05921173095703125, 0.0283355712890625, 0.21638870239257812, 0.6043319702148438, -3.7467193603515625, 0.09573745727539062, 0.6710853576660156, 0.19052886962890625, 0.181671142578125, 0.3680267333984375, -0.3666648864746094, 0.15595245361328125, 0.22142791748046875, 0.02117919921875, 1.3500595092773438, 0.4057807922363281, 0.3564910888671875, 0.4199714660644531, 0.14191436767578125, 0.232757568359375, 0.2607765197753906, 0.14999008178710938, 0.6439456939697266, 0.02973175048828125, 0.003398895263671875, -0.2966880798339844, 0.8647956848144531, -0.15621566772460938, -0.5493621826171875, 0.015796661376953125, -0.2927093505859375, 0.49619293212890625], "mean_td_error": 0.8429641723632812}}, "num_steps_sampled": 4000, "num_agent_steps_sampled": 8000, "num_steps_trained": 96256, "num_steps_trained_this_iter": 256, "num_agent_steps_trained": 192512, "last_target_update_ts": 3520, "num_target_updates": 6}, "done": false, "episodes_total": 6, "training_iteration": 4, "trial_id": "e21e6_00000", "experiment_id": "434cd42d33fd4b638f17fc69fa01d74f", "date": "2022-06-14_19-03-11", "timestamp": 1655247791, "time_this_iter_s": 20.906896114349365, "time_total_s": 72.08472990989685, "pid": 2568314, "hostname": "lambda-dual", "node_ip": "10.104.51.19", "config": {"num_workers": 2, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.0005, "train_batch_size": 256, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "cnet", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "1v1env", "observation_space": null, "action_space": null, "env_config": {}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": true, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": null, "evaluation_duration": 10, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"num_workers": 2, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.0005, "train_batch_size": 256, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "cnet", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "1v1env", "observation_space": null, "action_space": null, "env_config": {}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": false, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": null, "evaluation_duration": 10, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"explore": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": true, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": null, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 2, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"policy_01": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]], [[[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]], (3, 64, 64), float32)", "Discrete(7)", {}], "policy_02": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]], [[[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]], (3, 64, 64), float32)", "Discrete(7)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": "<function <lambda> at 0x7f1aa3747c20>", "policies_to_train": ["policy_01"], "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": 100000, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": false, "hiddens": [], "double_q": false, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": true, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": null, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 2, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"policy_01": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]], [[[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]], (3, 64, 64), float32)", "Discrete(7)", {}], "policy_02": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]], [[[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]], (3, 64, 64), float32)", "Discrete(7)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": "<function <lambda> at 0x7f1aa3747c20>", "policies_to_train": ["policy_01"], "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": 100000, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": false, "hiddens": [], "double_q": false, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "time_since_restore": 72.08472990989685, "timesteps_since_restore": 1024, "iterations_since_restore": 4, "warmup_time": 45.46659183502197, "perf": {"cpu_util_percent": 24.94827586206896, "ram_util_percent": 9.46206896551724}}
{"episode_reward_max": 204.0, "episode_reward_min": 0.0, "episode_reward_mean": 25.5, "episode_len_mean": 601.0, "episode_media": {}, "episodes_this_iter": 2, "policy_reward_min": {"policy_01": 0.0, "policy_02": 0.0}, "policy_reward_max": {"policy_01": 204.0, "policy_02": 0.0}, "policy_reward_mean": {"policy_01": 25.5, "policy_02": 0.0}, "custom_metrics": {}, "hist_stats": {"episode_reward": [0.0, 204.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "episode_lengths": [601, 601, 601, 601, 601, 601, 601, 601], "policy_policy_01_reward": [0.0, 204.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "policy_policy_02_reward": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.33978621922467384, "mean_inference_ms": 5.901126118227836, "mean_action_processing_ms": 0.08931593556673265, "mean_env_wait_ms": 8.695797621357698, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 2, "timesteps_total": 5000, "timesteps_this_iter": 256, "agent_timesteps_total": 10000, "timers": {"load_time_ms": 1.297, "load_throughput": 197328.229, "learn_time_ms": 11.652, "learn_throughput": 21970.85, "update_time_ms": 2.269}, "info": {"learner": {"policy_01": {"custom_metrics": {}, "learner_stats": {"mean_q": 37.06907653808594, "min_q": 27.250564575195312, "max_q": 47.24889373779297, "cur_lr": 0.0005}, "model": {}, "td_error": [0.28594207763671875, 0.36767578125, 0.3165130615234375, 0.7696075439453125, 0.4713325500488281, 0.4762229919433594, 0.7733802795410156, 0.9220390319824219, 0.7632865905761719, 0.678436279296875, -0.061859130859375, 0.6812019348144531, 0.4434700012207031, 1.5606536865234375, 0.11896896362304688, -5.347553253173828, 0.9102020263671875, 0.3660430908203125, -0.25156593322753906, 0.4424285888671875, 0.09495162963867188, 0.18026351928710938, -1.7756385803222656, 0.43082427978515625, -0.23364639282226562, 0.6482658386230469, 0.3878898620605469, 0.48210906982421875, -0.13278961181640625, 0.5441856384277344, 0.7473297119140625, -0.3538970947265625, 0.2229461669921875, 0.7850532531738281, 0.7991218566894531, 0.6286392211914062, 0.11362457275390625, 0.9711990356445312, 0.2751197814941406, 0.559112548828125, -8.203506469726562, 0.5049057006835938, 0.003337860107421875, 0.8739547729492188, 2.1730575561523438, 0.4230308532714844, 36.20061492919922, 0.13233184814453125, 0.5741615295410156, 0.28594207763671875, 0.4230194091796875, 0.28943634033203125, 0.4516868591308594, 0.5065765380859375, -0.3731689453125, 0.017261505126953125, 0.6509056091308594, 0.22562408447265625, 34.478004455566406, 0.12197113037109375, 0.23793792724609375, 1.5245704650878906, 0.4007301330566406, 0.09234619140625, 0.010631561279296875, 0.7260971069335938, -0.19966888427734375, -0.09334182739257812, 0.5953025817871094, 0.20734786987304688, 0.6300468444824219, 0.5673675537109375, -0.9006729125976562, 0.7611541748046875, 0.5398902893066406, -0.4259490966796875, 0.22443008422851562, -0.003509521484375, 0.7333984375, 0.21509552001953125, 0.39019012451171875, -6.8127899169921875, 0.4363517761230469, 0.45475006103515625, -1.1077041625976562, 1.2702140808105469, 0.43259620666503906, 0.1656036376953125, 0.2544517517089844, 0.4379730224609375, 0.006465911865234375, 0.9317359924316406, 0.2589683532714844, 2.3028640747070312, 0.5803184509277344, 0.12920761108398438, 0.4891166687011719, 0.3355293273925781, 0.6240997314453125, 0.3574371337890625, 0.6498298645019531, 0.5271568298339844, 0.2877655029296875, 1.0640869140625, 0.7493095397949219, 0.8265037536621094, -0.00298309326171875, 0.5311622619628906, 0.206787109375, 0.4990348815917969, 0.12775039672851562, -0.5120582580566406, 0.4664764404296875, 0.7317352294921875, 0.7791824340820312, 0.9226722717285156, 0.3369598388671875, -0.7766513824462891, 1.4502410888671875, 0.4616050720214844, 3.3281478881835938, 0.061267852783203125, 0.3531227111816406, 0.6907463073730469, -0.0804901123046875, 0.19152069091796875, 0.137054443359375, 0.6231727600097656, 0.7776870727539062, 0.71820068359375, 0.4820289611816406, 1.6049880981445312, 0.698638916015625, 0.7606430053710938, 0.0254669189453125, 0.23659133911132812, 36.20061492919922, 0.9102020263671875, -0.3731689453125, 0.003337860107421875, 0.38394927978515625, 0.5822067260742188, 0.3940277099609375, 0.10996246337890625, 0.5019264221191406, 0.5292739868164062, 0.3937225341796875, 0.39487457275390625, 0.7025718688964844, 0.39257049560546875, 0.4973640441894531, 0.5667724609375, 0.15852737426757812, 0.17119598388671875, 0.21390914916992188, 0.39096832275390625, -2.27569580078125, 0.007785797119140625, -0.15016937255859375, 0.8339614868164062, 0.5849266052246094, 0.4531517028808594, 0.0106048583984375, 0.2444610595703125, 0.4336738586425781, 0.4417915344238281, 0.8516044616699219, 0.6240348815917969, 0.23610305786132812, -3.6794815063476562, 0.24837493896484375, -0.19966888427734375, 0.5303916931152344, 0.730743408203125, 0.7965164184570312, 0.5393218994140625, 0.4230194091796875, 0.4923858642578125, 0.18526458740234375, 0.6999473571777344, -0.14699172973632812, 0.07014083862304688, 0.4249839782714844, 0.7987442016601562, 36.25017166137695, 0.32538604736328125, 0.3571357727050781, 0.7028617858886719, 0.14293289184570312, 0.2805213928222656, 0.32927703857421875, 0.4744148254394531, 0.4658393859863281, -0.653900146484375, 0.5812492370605469, 0.5835380554199219, 1.4869804382324219, 0.22637176513671875, -5.170146942138672, 0.25453948974609375, 0.3137474060058594, 0.4099006652832031, 0.6472206115722656, 0.20227432250976562, 0.228973388671875, 0.8121719360351562, 0.6268463134765625, 0.5670204162597656, 0.2690773010253906, -0.7837982177734375, 0.2086639404296875, 0.2637290954589844, -0.12726593017578125, 0.7583999633789062, 0.10126113891601562, 1.0240287780761719, -0.4259490966796875, 0.22428512573242188, 0.7329216003417969, 0.3267822265625, 0.6618499755859375, 0.7868614196777344, 0.7161178588867188, 0.3953437805175781, 0.5338325500488281, 0.6299362182617188, 0.21833419799804688, 0.13113021850585938, 0.3940277099609375, -0.11834716796875, 0.7374420166015625, 0.11841964721679688, 0.329193115234375, -0.03244781494140625, 0.040348052978515625, 0.40575408935546875, 0.428436279296875, 0.18785476684570312, 1.5408248901367188, 0.61358642578125, 0.6307792663574219, 0.3021087646484375, 1.2164230346679688, 0.3215751647949219, 0.6671257019042969, 1.1709403991699219, 0.20807266235351562, 0.4422187805175781, 0.29045867919921875, 0.13624191284179688, -5.102268218994141, 0.19458389282226562, 0.8476791381835938, 0.17388153076171875, 0.36074066162109375, 0.8727531433105469], "mean_td_error": 0.8177350759506226}}, "num_steps_sampled": 5000, "num_agent_steps_sampled": 10000, "num_steps_trained": 128256, "num_steps_trained_this_iter": 256, "num_agent_steps_trained": 256512, "last_target_update_ts": 4528, "num_target_updates": 8}, "done": false, "episodes_total": 8, "training_iteration": 5, "trial_id": "e21e6_00000", "experiment_id": "434cd42d33fd4b638f17fc69fa01d74f", "date": "2022-06-14_19-03-32", "timestamp": 1655247812, "time_this_iter_s": 20.69129252433777, "time_total_s": 92.77602243423462, "pid": 2568314, "hostname": "lambda-dual", "node_ip": "10.104.51.19", "config": {"num_workers": 2, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.0005, "train_batch_size": 256, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "cnet", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "1v1env", "observation_space": null, "action_space": null, "env_config": {}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": true, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": null, "evaluation_duration": 10, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"num_workers": 2, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.0005, "train_batch_size": 256, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "cnet", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "1v1env", "observation_space": null, "action_space": null, "env_config": {}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": false, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": null, "evaluation_duration": 10, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"explore": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": true, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": null, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 2, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"policy_01": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]], [[[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]], (3, 64, 64), float32)", "Discrete(7)", {}], "policy_02": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]], [[[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]], (3, 64, 64), float32)", "Discrete(7)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": "<function <lambda> at 0x7f1aa375fa70>", "policies_to_train": ["policy_01"], "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": 100000, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": false, "hiddens": [], "double_q": false, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": true, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": null, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 2, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"policy_01": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]], [[[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]], (3, 64, 64), float32)", "Discrete(7)", {}], "policy_02": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]], [[[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]], (3, 64, 64), float32)", "Discrete(7)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": "<function <lambda> at 0x7f1aa375fa70>", "policies_to_train": ["policy_01"], "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": 100000, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": false, "hiddens": [], "double_q": false, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "time_since_restore": 92.77602243423462, "timesteps_since_restore": 1280, "iterations_since_restore": 5, "warmup_time": 45.46659183502197, "perf": {"cpu_util_percent": 24.900000000000002, "ram_util_percent": 9.54666666666667}}
{"episode_reward_max": 204.0, "episode_reward_min": 0.0, "episode_reward_mean": 25.5, "episode_len_mean": 601.0, "episode_media": {}, "episodes_this_iter": 0, "policy_reward_min": {"policy_01": 0.0, "policy_02": 0.0}, "policy_reward_max": {"policy_01": 204.0, "policy_02": 0.0}, "policy_reward_mean": {"policy_01": 25.5, "policy_02": 0.0}, "custom_metrics": {}, "hist_stats": {"episode_reward": [0.0, 204.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "episode_lengths": [601, 601, 601, 601, 601, 601, 601, 601], "policy_policy_01_reward": [0.0, 204.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "policy_policy_02_reward": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.33978621922467384, "mean_inference_ms": 5.901126118227836, "mean_action_processing_ms": 0.08931593556673265, "mean_env_wait_ms": 8.695797621357698, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 2, "timesteps_total": 6000, "timesteps_this_iter": 256, "agent_timesteps_total": 12000, "timers": {"load_time_ms": 1.324, "load_throughput": 193376.405, "learn_time_ms": 11.973, "learn_throughput": 21381.996, "update_time_ms": 2.205}, "info": {"learner": {"policy_01": {"custom_metrics": {}, "learner_stats": {"mean_q": 37.294151306152344, "min_q": 22.878711700439453, "max_q": 51.2706413269043, "cur_lr": 0.0005}, "model": {}, "td_error": [0.5717048645019531, 0.317138671875, 0.3080024719238281, 0.7807807922363281, 0.5123863220214844, 0.6028251647949219, -0.4934806823730469, -3.427398681640625, 0.21824264526367188, 0.4659767150878906, 0.9445686340332031, 0.6129837036132812, 0.4393310546875, 0.3223915100097656, 1.0296554565429688, -0.11700057983398438, -0.20067596435546875, 0.003803253173828125, -0.15404891967773438, -0.3177833557128906, -1.6081619262695312, -0.5964393615722656, -0.2018585205078125, -0.14461517333984375, -0.080078125, 0.6118278503417969, -6.223293304443359, 23.184078216552734, 0.9445686340332031, 2.0844764709472656, -0.1133270263671875, -0.16669082641601562, -0.4095611572265625, 0.22483062744140625, 0.34191131591796875, 0.10247039794921875, -0.06551742553710938, 0.7570114135742188, -0.22751235961914062, -6.005104064941406, 0.3026313781738281, 0.5545310974121094, 0.22861480712890625, 0.5624122619628906, 0.19565200805664062, 0.0884246826171875, 0.060516357421875, 0.90625, 0.3048744201660156, 0.751739501953125, 0.19578933715820312, 0.3839149475097656, 0.3617706298828125, -0.10153579711914062, 0.07625961303710938, 0.4365348815917969, -1.1009712219238281, 0.21691131591796875, -0.4501533508300781, 0.45850372314453125, -0.205078125, -0.174652099609375, -3.8109817504882812, 0.5884513854980469, 0.2990264892578125, 0.20245361328125, 0.8565711975097656, 0.38993072509765625, 0.5076217651367188, 2.4402809143066406, 0.4579811096191406, 0.30924224853515625, 0.06204986572265625, -0.06960296630859375, 0.5360641479492188, 0.0378875732421875, 0.6516914367675781, 0.3095703125, 0.5421562194824219, 0.21567916870117188, 0.4494514465332031, -0.2863616943359375, 0.953887939453125, -0.17278671264648438, -0.5107269287109375, 0.18132400512695312, 0.17664337158203125, 0.3844795227050781, -0.048736572265625, 0.5352020263671875, -0.378936767578125, 0.2615509033203125, 0.27497100830078125, 0.19995498657226562, 0.2535972595214844, -0.14677810668945312, -0.015598297119140625, 0.09337234497070312, 0.10247039794921875, -0.06195831298828125, -0.4658203125, -0.01058197021484375, -0.9782524108886719, 0.4770050048828125, 0.2542381286621094, 0.095062255859375, -0.416595458984375, 35.70937728881836, 0.23361968994140625, -0.17779922485351562, 0.5187606811523438, -0.29712677001953125, 0.16059112548828125, 0.17083740234375, -0.2567710876464844, -0.06466293334960938, 1.4712905883789062, -0.13270950317382812, 0.4272003173828125, -0.35265541076660156, 0.08686065673828125, 0.10247039794921875, 0.395721435546875, -0.11558914184570312, -1.4165153503417969, -1.0588760375976562, 0.5280189514160156, 0.5583572387695312, -0.15716934204101562, 8.392333984375e-05, 0.2347412109375, -7.0652923583984375, 0.5900001525878906, 0.7216453552246094, -0.003604888916015625, 0.4471435546875, 0.2248382568359375, 0.2311248779296875, 0.32329559326171875, 0.7635269165039062, 3.4898452758789062, 0.2206878662109375, 2.2280197143554688, -0.0577239990234375, -0.24286651611328125, 0.013095855712890625, 0.15069580078125, -0.18617630004882812, 0.386016845703125, 0.589630126953125, 0.13799667358398438, -5.164409637451172, 0.3314170837402344, 0.270599365234375, -0.2906646728515625, 0.4579811096191406, -0.6871356964111328, 0.4262809753417969, 0.4490013122558594, -7.312469482421875, -0.2378082275390625, -0.4616966247558594, 23.184078216552734, -0.0863800048828125, 0.3664398193359375, -1.6544227600097656, 0.3844795227050781, -0.07718658447265625, 0.23334503173828125, -6.087329864501953, 0.19519424438476562, 34.86702346801758, 0.5452423095703125, 0.5990219116210938, 0.8335151672363281, -0.1751251220703125, -0.37026214599609375, 0.17252349853515625, -0.44434356689453125, 0.386016845703125, -0.3059844970703125, 0.29592132568359375, 0.00836181640625, 0.5318717956542969, -0.004619598388671875, 0.2861900329589844, -0.07718658447265625, -0.0532379150390625, 0.08420181274414062, 0.21364593505859375, -0.23143386840820312, 0.7472305297851562, 0.19990158081054688, -0.1662139892578125, 0.751739501953125, 0.14807510375976562, 0.2542381286621094, -2.9484786987304688, 0.6365776062011719, -4.606288909912109, -0.3429412841796875, 0.43088531494140625, 0.4676055908203125, -5.2499847412109375, 36.42547607421875, 0.15611648559570312, 0.3031120300292969, 0.7542152404785156, -1.9466133117675781, -0.042461395263671875, 0.11040878295898438, -2.373157501220703, -0.1407623291015625, 0.5824623107910156, 0.1861114501953125, -0.06231689453125, -0.16950607299804688, 0.08088302612304688, -2.373157501220703, 0.29592132568359375, 1.2147064208984375, 0.2641563415527344, 0.512603759765625, 1.6430549621582031, 0.29650115966796875, -0.31365966796875, 0.6902427673339844, 0.12931442260742188, -1.0588760375976562, -0.06615447998046875, -0.3257408142089844, -0.14890289306640625, 0.7621955871582031, 0.43759918212890625, 3.4898452758789062, 0.2860870361328125, 0.22464752197265625, 0.38803863525390625, 0.6261405944824219, 0.2674369812011719, 0.059764862060546875, 0.2583656311035156, 0.2256622314453125, -0.07996749877929688, -1.9953384399414062, -2.4921607971191406, -0.23139190673828125, 0.45850372314453125, 0.6593246459960938, -0.09773635864257812, 0.18168258666992188, 0.5305976867675781, 0.5646133422851562, 0.6684989929199219, -0.07193756103515625, 0.3317070007324219], "mean_td_error": 0.531002402305603}}, "num_steps_sampled": 6000, "num_agent_steps_sampled": 12000, "num_steps_trained": 160256, "num_steps_trained_this_iter": 256, "num_agent_steps_trained": 320512, "last_target_update_ts": 5536, "num_target_updates": 10}, "done": false, "episodes_total": 8, "training_iteration": 6, "trial_id": "e21e6_00000", "experiment_id": "434cd42d33fd4b638f17fc69fa01d74f", "date": "2022-06-14_19-03-52", "timestamp": 1655247832, "time_this_iter_s": 20.617212533950806, "time_total_s": 113.39323496818542, "pid": 2568314, "hostname": "lambda-dual", "node_ip": "10.104.51.19", "config": {"num_workers": 2, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.0005, "train_batch_size": 256, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "cnet", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "1v1env", "observation_space": null, "action_space": null, "env_config": {}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": true, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": null, "evaluation_duration": 10, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"num_workers": 2, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.0005, "train_batch_size": 256, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "cnet", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "1v1env", "observation_space": null, "action_space": null, "env_config": {}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": false, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": null, "evaluation_duration": 10, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"explore": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": true, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": null, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 2, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"policy_01": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]], [[[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]], (3, 64, 64), float32)", "Discrete(7)", {}], "policy_02": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]], [[[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]], (3, 64, 64), float32)", "Discrete(7)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": "<function <lambda> at 0x7f1aa375f5f0>", "policies_to_train": ["policy_01"], "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": 100000, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": false, "hiddens": [], "double_q": false, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": true, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": null, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 2, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"policy_01": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]], [[[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]], (3, 64, 64), float32)", "Discrete(7)", {}], "policy_02": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]], [[[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]], (3, 64, 64), float32)", "Discrete(7)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": "<function <lambda> at 0x7f1aa375f5f0>", "policies_to_train": ["policy_01"], "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": 100000, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": false, "hiddens": [], "double_q": false, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "time_since_restore": 113.39323496818542, "timesteps_since_restore": 1536, "iterations_since_restore": 6, "warmup_time": 45.46659183502197, "perf": {"cpu_util_percent": 24.68275862068965, "ram_util_percent": 9.631034482758619}}
{"episode_reward_max": 204.0, "episode_reward_min": 0.0, "episode_reward_mean": 20.4, "episode_len_mean": 601.0, "episode_media": {}, "episodes_this_iter": 2, "policy_reward_min": {"policy_01": 0.0, "policy_02": 0.0}, "policy_reward_max": {"policy_01": 204.0, "policy_02": 0.0}, "policy_reward_mean": {"policy_01": 20.4, "policy_02": 0.0}, "custom_metrics": {}, "hist_stats": {"episode_reward": [0.0, 204.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "episode_lengths": [601, 601, 601, 601, 601, 601, 601, 601, 601, 601], "policy_policy_01_reward": [0.0, 204.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "policy_policy_02_reward": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.33872257529226757, "mean_inference_ms": 5.893351688049324, "mean_action_processing_ms": 0.0893865517513208, "mean_env_wait_ms": 8.610013555170173, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 2, "timesteps_total": 7000, "timesteps_this_iter": 256, "agent_timesteps_total": 14000, "timers": {"load_time_ms": 1.305, "load_throughput": 196124.392, "learn_time_ms": 11.18, "learn_throughput": 22898.09, "update_time_ms": 1.996}, "info": {"learner": {"policy_01": {"custom_metrics": {}, "learner_stats": {"mean_q": 37.77433776855469, "min_q": 24.353601455688477, "max_q": 58.655418395996094, "cur_lr": 0.0005}, "model": {}, "td_error": [2.7324295043945312, 0.2047882080078125, -2.2603683471679688, 0.041431427001953125, 0.2683372497558594, 0.08325576782226562, 1.3355674743652344, -0.044422149658203125, 0.11530303955078125, 0.33698272705078125, 0.39288330078125, -0.0591278076171875, 0.8529281616210938, 0.020824432373046875, -0.1185302734375, -0.7418365478515625, 1.4157257080078125, 0.17104721069335938, -1.8158683776855469, 0.22031784057617188, 1.2365798950195312, 0.21526718139648438, -0.5902786254882812, -0.036865234375, 0.157928466796875, 0.4160118103027344, -0.335662841796875, 1.2779617309570312, -1.8158683776855469, -0.18412399291992188, 0.15053558349609375, 0.7461776733398438, -1.1336784362792969, 0.042163848876953125, -0.6505126953125, 0.8502998352050781, -0.37610626220703125, 2.09405517578125, 1.0152244567871094, 0.5620384216308594, -0.11730575561523438, -0.20129776000976562, 0.12617111206054688, -0.3821144104003906, 36.25469970703125, 0.4622955322265625, 0.3581085205078125, 0.44792938232421875, -0.004726409912109375, 0.4141693115234375, 0.4779167175292969, 0.5376510620117188, 0.5633087158203125, 0.42990875244140625, 0.19771957397460938, 0.400177001953125, 0.11408233642578125, 0.529388427734375, 35.32951354980469, -0.035694122314453125, 1.1016159057617188, -0.06726455688476562, 35.32951354980469, -0.22918319702148438, 0.6039619445800781, 0.16331100463867188, 0.17318344116210938, -0.93048095703125, 0.5710067749023438, 0.08916473388671875, 0.6991844177246094, -0.5977554321289062, 0.2117156982421875, 0.6453514099121094, 0.44189453125, 0.8369712829589844, -0.13277435302734375, 0.4428443908691406, -0.24852371215820312, -0.052684783935546875, -0.2167816162109375, 0.334442138671875, 0.3142967224121094, 0.22793960571289062, 0.44625091552734375, 0.26189422607421875, -2.3807029724121094, 0.5435371398925781, -0.11506271362304688, -0.5196571350097656, 0.15971755981445312, 0.39772796630859375, -0.06449127197265625, 1.0128211975097656, -4.656730651855469, -0.5671844482421875, -2.6388816833496094, 0.8771705627441406, 0.6127128601074219, -0.057323455810546875, 0.4138603210449219, -0.25341033935546875, 0.2117156982421875, -0.001148223876953125, -0.002155303955078125, 0.0375213623046875, -0.30023193359375, 0.14910888671875, -0.007694244384765625, -0.048000335693359375, -0.4494972229003906, 0.20566558837890625, 0.3749427795410156, 0.1320648193359375, 0.0565032958984375, 0.22669219970703125, -0.6816940307617188, 1.1016159057617188, 1.0072059631347656, 0.9661827087402344, 0.18771743774414062, -0.06702423095703125, 0.010166168212890625, 0.18818283081054688, 0.028400421142578125, -2.550201416015625, 0.5596046447753906, 0.9607505798339844, -0.6480636596679688, 0.7101287841796875, -1.3861465454101562, 0.001369476318359375, -0.1070556640625, -0.06306076049804688, -0.041290283203125, 0.4004783630371094, 0.4141693115234375, 2.7163963317871094, 0.5383415222167969, 0.3095207214355469, -3.9781227111816406, 0.3208045959472656, 0.6581077575683594, -0.13491058349609375, -0.8576622009277344, -0.10071182250976562, -0.07866287231445312, 0.16278076171875, -0.1706085205078125, -0.3682212829589844, 0.4658927917480469, 1.4197883605957031, -0.00247955322265625, 1.0152244567871094, 0.12636566162109375, 2.2885093688964844, -2.6388816833496094, 0.10432815551757812, -0.318115234375, 0.583221435546875, -0.24831008911132812, 1.4042510986328125, 0.5336456298828125, 0.45279693603515625, -0.4461822509765625, -0.3491859436035156, 0.155731201171875, 0.060283660888671875, -0.022308349609375, 0.20566177368164062, 0.4393806457519531, 0.09771728515625, -0.4231986999511719, -0.3232421875, 0.4738349914550781, 0.6034622192382812, -1.3891410827636719, -0.43299102783203125, -0.036808013916015625, 0.22513198852539062, 0.2487335205078125, -0.43084716796875, 0.0565032958984375, -0.20161819458007812, 2.039081573486328, -2.3838882446289062, -0.3097991943359375, 0.055782318115234375, -0.30957794189453125, -4.656730651855469, 0.6992111206054688, -0.660430908203125, 0.8559627532958984, -0.5310935974121094, 0.71746826171875, -0.004726409912109375, -0.048252105712890625, -0.9849815368652344, -2.009113311767578, -0.5018844604492188, -0.7953605651855469, 0.16380691528320312, 1.5440139770507812, -0.29441070556640625, 0.2936553955078125, -0.46561431884765625, 0.17356491088867188, -0.0877838134765625, -0.018524169921875, 0.326019287109375, 2.7324295043945312, 0.08699417114257812, -0.20467758178710938, 0.6962471008300781, 0.5055503845214844, 0.3332557678222656, 0.41188812255859375, -0.003078460693359375, -0.06325912475585938, 0.3376007080078125, 2.4167213439941406, 1.2355461120605469, -0.3504180908203125, 0.16270065307617188, 2.4839935302734375, 0.2678985595703125, -0.1818084716796875, 35.87046813964844, 0.3332557678222656, -4.787967681884766, -0.40120697021484375, -0.5376968383789062, -0.573150634765625, 0.2637481689453125, 0.2883415222167969, -3.436695098876953, 0.06429290771484375, 0.4878120422363281, 0.2643013000488281, 0.4475288391113281, -0.18104934692382812, 0.17204666137695312, 0.08062744140625, -0.3740196228027344, 0.231353759765625, 0.3108940124511719, -2.0740127563476562, 0.4141693115234375, 0.30450439453125, -0.9292564392089844, 0.9129524230957031, -0.5317306518554688, 0.10353469848632812, -0.19748306274414062, 0.12302780151367188, 0.249847412109375], "mean_td_error": 0.5888338088989258}}, "num_steps_sampled": 7000, "num_agent_steps_sampled": 14000, "num_steps_trained": 192256, "num_steps_trained_this_iter": 256, "num_agent_steps_trained": 384512, "last_target_update_ts": 6544, "num_target_updates": 12}, "done": false, "episodes_total": 10, "training_iteration": 7, "trial_id": "e21e6_00000", "experiment_id": "434cd42d33fd4b638f17fc69fa01d74f", "date": "2022-06-14_19-04-13", "timestamp": 1655247853, "time_this_iter_s": 20.719460487365723, "time_total_s": 134.11269545555115, "pid": 2568314, "hostname": "lambda-dual", "node_ip": "10.104.51.19", "config": {"num_workers": 2, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.0005, "train_batch_size": 256, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "cnet", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "1v1env", "observation_space": null, "action_space": null, "env_config": {}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": true, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": null, "evaluation_duration": 10, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"num_workers": 2, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.0005, "train_batch_size": 256, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "cnet", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "1v1env", "observation_space": null, "action_space": null, "env_config": {}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": false, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": null, "evaluation_duration": 10, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"explore": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": true, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": null, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 2, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"policy_01": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]], [[[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]], (3, 64, 64), float32)", "Discrete(7)", {}], "policy_02": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]], [[[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]], (3, 64, 64), float32)", "Discrete(7)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": "<function <lambda> at 0x7f1aa372e4d0>", "policies_to_train": ["policy_01"], "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": 100000, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": false, "hiddens": [], "double_q": false, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": true, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": null, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 2, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"policy_01": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]], [[[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]], (3, 64, 64), float32)", "Discrete(7)", {}], "policy_02": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]], [[[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]], (3, 64, 64), float32)", "Discrete(7)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": "<function <lambda> at 0x7f1aa372e4d0>", "policies_to_train": ["policy_01"], "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": 100000, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": false, "hiddens": [], "double_q": false, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "time_since_restore": 134.11269545555115, "timesteps_since_restore": 1792, "iterations_since_restore": 7, "warmup_time": 45.46659183502197, "perf": {"cpu_util_percent": 24.79666666666667, "ram_util_percent": 9.699999999999998}}
{"episode_reward_max": 441.0, "episode_reward_min": 0.0, "episode_reward_mean": 53.75, "episode_len_mean": 601.0, "episode_media": {}, "episodes_this_iter": 2, "policy_reward_min": {"policy_01": 0.0, "policy_02": 0.0}, "policy_reward_max": {"policy_01": 204.0, "policy_02": 441.0}, "policy_reward_mean": {"policy_01": 17.0, "policy_02": 36.75}, "custom_metrics": {}, "hist_stats": {"episode_reward": [0.0, 204.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0], "episode_lengths": [601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601], "policy_policy_01_reward": [0.0, 204.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "policy_policy_02_reward": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.33836227069976627, "mean_inference_ms": 5.888402796846698, "mean_action_processing_ms": 0.08942646297181774, "mean_env_wait_ms": 8.543840906669903, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 2, "timesteps_total": 8000, "timesteps_this_iter": 256, "agent_timesteps_total": 16000, "timers": {"load_time_ms": 1.308, "load_throughput": 195699.022, "learn_time_ms": 11.568, "learn_throughput": 22129.562, "update_time_ms": 2.11}, "info": {"learner": {"policy_01": {"custom_metrics": {}, "learner_stats": {"mean_q": 38.92588806152344, "min_q": 28.630382537841797, "max_q": 68.63752746582031, "cur_lr": 0.0005}, "model": {}, "td_error": [0.2997322082519531, -0.9599037170410156, 0.19347000122070312, 0.3135337829589844, 0.10689926147460938, 5.453731536865234, 0.1623077392578125, 6.730953216552734, 35.823577880859375, 0.3866310119628906, -0.160308837890625, -0.10163497924804688, 0.5710601806640625, 0.14086151123046875, 0.34098052978515625, -0.13025283813476562, -2.3399810791015625, 0.0367889404296875, -0.3412628173828125, 0.16124343872070312, 0.39768218994140625, 0.480438232421875, 0.3135108947753906, 0.03900909423828125, 0.11687088012695312, 0.2457733154296875, 0.2511787414550781, 2.2016830444335938, 35.823577880859375, 0.8849258422851562, 2.440448760986328, 0.1054229736328125, -4.306709289550781, -0.0250396728515625, 0.27751922607421875, -0.23506546020507812, 0.1736907958984375, 0.116912841796875, 0.3423500061035156, -1.4103507995605469, -0.3415641784667969, 0.381103515625, -0.15677261352539062, 0.35356903076171875, 35.823577880859375, -0.0924072265625, -0.8829154968261719, -0.01361083984375, 0.1858062744140625, 0.08324813842773438, -0.055110931396484375, -0.12735366821289062, 0.170074462890625, 0.5396232604980469, 0.38880157470703125, -0.14519119262695312, -0.12390518188476562, 37.04967498779297, -0.41745758056640625, -0.1313629150390625, 0.06151580810546875, 0.17432785034179688, 0.33545684814453125, -0.2526969909667969, 1.3389091491699219, -0.0068511962890625, 0.31740570068359375, 1.4155693054199219, 0.5591850280761719, 0.4270782470703125, 0.14777374267578125, -0.032932281494140625, -0.1604156494140625, -0.16636276245117188, 0.147125244140625, -0.22707366943359375, -0.10838699340820312, 0.25287628173828125, -0.09975433349609375, -0.7099494934082031, 0.8009719848632812, 0.1638031005859375, 30.627138137817383, 0.08103561401367188, -0.07205963134765625, 0.17966461181640625, -0.10285568237304688, -0.046268463134765625, 0.13948440551757812, 0.4846611022949219, 0.6090240478515625, 0.26128387451171875, 0.2528648376464844, 0.5276298522949219, 0.1638031005859375, -0.11815643310546875, -1.0333175659179688, 0.21976089477539062, -0.07054519653320312, -0.14564132690429688, -2.7538299560546875, 0.4604644775390625, -0.24036788940429688, 0.720550537109375, 0.3325843811035156, 3.9956283569335938, 0.28701019287109375, -0.0981597900390625, -0.06827163696289062, -10.430427551269531, 0.09908294677734375, 0.389129638671875, 0.3253211975097656, 0.40053558349609375, -0.11815643310546875, -0.13641357421875, -0.864898681640625, -0.060276031494140625, -0.4406013488769531, 33.67976379394531, -0.3073463439941406, 0.14472198486328125, -0.17578125, -0.28018951416015625, -0.021152496337890625, 0.6377182006835938, 1.4476203918457031, 0.021587371826171875, 0.12901687622070312, 0.4447021484375, 0.17697906494140625, -0.1040191650390625, 0.055057525634765625, -0.15895843505859375, 0.16141128540039062, -0.09607315063476562, 0.3244171142578125, -0.5116310119628906, 5.614582061767578, -0.17037582397460938, -0.2547798156738281, 0.014667510986328125, -0.24054336547851562, -0.08852386474609375, -0.0391998291015625, 0.3320274353027344, -0.14023208618164062, 0.13988876342773438, 0.45008087158203125, -0.06713104248046875, 4.338504791259766, 0.11450958251953125, 1.4476203918457031, 0.2842864990234375, 0.009449005126953125, 0.14899826049804688, 0.6147117614746094, 0.057575225830078125, -0.5685768127441406, 36.97858428955078, -0.0313262939453125, 0.6934547424316406, -0.09292221069335938, -0.20161056518554688, 0.014667510986328125, -0.023387908935546875, -0.058376312255859375, -0.22751235961914062, -0.15250778198242188, 0.4581184387207031, -0.0658416748046875, -0.20330047607421875, 0.25617218017578125, 0.3112373352050781, -1.3314018249511719, -0.17083740234375, -0.15464401245117188, 0.19954299926757812, -0.4465522766113281, 4.392490386962891, 0.6018257141113281, -0.176544189453125, -0.011585235595703125, -0.023700714111328125, 0.8632545471191406, -0.42140960693359375, -0.44103431701660156, 37.16984176635742, 0.08205795288085938, -0.09292221069335938, 0.6295166015625, -0.08084869384765625, -0.2523651123046875, 0.02469635009765625, 0.19540786743164062, -0.09774398803710938, -0.57586669921875, -0.3092803955078125, 0.3852348327636719, -0.28943634033203125, -0.115509033203125, 0.1314239501953125, -0.18088150024414062, -0.14778518676757812, -1.1825809478759766, 0.293365478515625, 3.064136505126953, -0.037853240966796875, 0.022003173828125, -0.07785797119140625, -0.07785797119140625, 0.2844963073730469, 0.6497154235839844, 0.2818489074707031, 0.46434783935546875, -0.12065505981445312, 2.699474334716797, 30.627138137817383, 0.3844871520996094, 0.13509750366210938, 0.48252105712890625, -0.08289337158203125, -0.010711669921875, 0.3724403381347656, -2.0674476623535156, -0.24643325805664062, -0.09691619873046875, -0.260009765625, 0.4185791015625, -0.11138153076171875, 0.45009613037109375, -5.1747589111328125, 0.06298065185546875, -0.09425735473632812, 0.041233062744140625, 0.41683197021484375, -0.09264373779296875, 30.627138137817383, 0.4784660339355469, 0.6585655212402344, -0.2976875305175781, 11.268150329589844, 0.02700042724609375, 0.5131797790527344, -0.032054901123046875, 0.389556884765625, 4.392490386962891, 0.2822608947753906, 4.338504791259766, -0.2509727478027344, -0.4920082092285156, 0.07013320922851562, 0.09940338134765625, 0.18932723999023438, -0.13335418701171875, 1.7130661010742188], "mean_td_error": 1.5444436073303223}}, "num_steps_sampled": 8000, "num_agent_steps_sampled": 16000, "num_steps_trained": 224256, "num_steps_trained_this_iter": 256, "num_agent_steps_trained": 448512, "last_target_update_ts": 7552, "num_target_updates": 14}, "done": false, "episodes_total": 12, "training_iteration": 8, "trial_id": "e21e6_00000", "experiment_id": "434cd42d33fd4b638f17fc69fa01d74f", "date": "2022-06-14_19-04-34", "timestamp": 1655247874, "time_this_iter_s": 20.98993682861328, "time_total_s": 155.10263228416443, "pid": 2568314, "hostname": "lambda-dual", "node_ip": "10.104.51.19", "config": {"num_workers": 2, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.0005, "train_batch_size": 256, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "cnet", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "1v1env", "observation_space": null, "action_space": null, "env_config": {}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": true, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": null, "evaluation_duration": 10, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"num_workers": 2, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.0005, "train_batch_size": 256, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "cnet", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "1v1env", "observation_space": null, "action_space": null, "env_config": {}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": false, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": null, "evaluation_duration": 10, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"explore": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": true, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": null, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 2, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"policy_01": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]], [[[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]], (3, 64, 64), float32)", "Discrete(7)", {}], "policy_02": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]], [[[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]], (3, 64, 64), float32)", "Discrete(7)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": "<function <lambda> at 0x7f1aa3726680>", "policies_to_train": ["policy_01"], "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": 100000, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": false, "hiddens": [], "double_q": false, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": true, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": null, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 2, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"policy_01": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]], [[[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]], (3, 64, 64), float32)", "Discrete(7)", {}], "policy_02": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]], [[[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]], (3, 64, 64), float32)", "Discrete(7)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": "<function <lambda> at 0x7f1aa3726680>", "policies_to_train": ["policy_01"], "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": 100000, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": false, "hiddens": [], "double_q": false, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "time_since_restore": 155.10263228416443, "timesteps_since_restore": 2048, "iterations_since_restore": 8, "warmup_time": 45.46659183502197, "perf": {"cpu_util_percent": 24.493333333333332, "ram_util_percent": 9.72}}
{"episode_reward_max": 441.0, "episode_reward_min": 0.0, "episode_reward_mean": 46.07142857142857, "episode_len_mean": 601.0, "episode_media": {}, "episodes_this_iter": 2, "policy_reward_min": {"policy_01": 0.0, "policy_02": 0.0}, "policy_reward_max": {"policy_01": 204.0, "policy_02": 441.0}, "policy_reward_mean": {"policy_01": 14.571428571428571, "policy_02": 31.5}, "custom_metrics": {}, "hist_stats": {"episode_reward": [0.0, 204.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0], "episode_lengths": [601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601], "policy_policy_01_reward": [0.0, 204.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "policy_policy_02_reward": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.3382297264909558, "mean_inference_ms": 5.884855140411479, "mean_action_processing_ms": 0.0894557550568055, "mean_env_wait_ms": 8.490418986244654, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 2, "timesteps_total": 9000, "timesteps_this_iter": 256, "agent_timesteps_total": 18000, "timers": {"load_time_ms": 1.333, "load_throughput": 192013.917, "learn_time_ms": 11.748, "learn_throughput": 21791.643, "update_time_ms": 2.186}, "info": {"learner": {"policy_01": {"custom_metrics": {}, "learner_stats": {"mean_q": 38.536773681640625, "min_q": 28.220233917236328, "max_q": 62.33760452270508, "cur_lr": 0.0005}, "model": {}, "td_error": [-0.3008880615234375, 0.41947174072265625, 0.4820365905761719, 0.020778656005859375, -0.5420913696289062, 0.10953140258789062, 0.11650466918945312, -0.2581367492675781, -0.4793434143066406, 0.23907470703125, 0.5256690979003906, -3.1049766540527344, 35.77465057373047, 0.014743804931640625, -0.5806808471679688, -0.3534126281738281, 35.410362243652344, -0.4794120788574219, 3.19647216796875, -0.5627326965332031, -0.15785980224609375, -0.04467010498046875, -0.5406837463378906, 0.3665046691894531, -0.5372543334960938, -0.5410919189453125, -0.7490005493164062, -0.30682373046875, 38.96872329711914, 0.012195587158203125, 0.8837623596191406, -0.4455451965332031, 0.09612655639648438, -3.3849525451660156, -0.42467498779296875, 0.10733795166015625, -0.7509613037109375, 0.16278076171875, 0.001972198486328125, -0.20338058471679688, 0.09555816650390625, -0.12702178955078125, -0.5482521057128906, -0.30005645751953125, -0.3988227844238281, -0.16003799438476562, -0.7537765502929688, -0.33441162109375, -0.16157150268554688, -0.3151588439941406, -0.5324211120605469, -0.20443344116210938, -0.2910881042480469, -0.16861343383789062, 0.05519866943359375, -0.6125602722167969, 0.4205131530761719, 0.2702903747558594, -0.2759666442871094, 0.09135055541992188, -0.09758377075195312, 0.29293060302734375, -0.5454635620117188, -1.0135154724121094, -0.10729598999023438, -0.4276618957519531, -1.1606979370117188, -0.310089111328125, 0.022174835205078125, 0.7225189208984375, 0.3820343017578125, 0.29116058349609375, -0.4085121154785156, -1.4566535949707031, -1.1630134582519531, -0.34545135498046875, -8.144302368164062, 0.03170013427734375, -0.09222030639648438, -0.9427871704101562, -0.5968246459960938, -0.06552886962890625, -0.5406875610351562, -0.2445831298828125, -0.07626724243164062, -0.6147079467773438, -0.3441963195800781, -0.4728202819824219, 0.5641860961914062, 0.018573760986328125, -1.2456550598144531, 0.748626708984375, -0.17889785766601562, -0.20159149169921875, -0.31032752990722656, -0.2695465087890625, -0.6451148986816406, -0.07540512084960938, 35.41978073120117, -0.5911636352539062, -0.29846954345703125, -0.2663764953613281, -0.37065887451171875, 37.265567779541016, 0.6748580932617188, -0.3955192565917969, -0.29016876220703125, -0.33527374267578125, -0.7581939697265625, 0.050167083740234375, 1.3627662658691406, -0.6773033142089844, -1.2985954284667969, -0.8628196716308594, -0.4289093017578125, -0.07509994506835938, 0.10733795166015625, -0.665771484375, -0.005664825439453125, -0.5502128601074219, 0.6324462890625, -0.5296974182128906, -0.23936843872070312, 0.036678314208984375, -0.47109222412109375, -0.6115264892578125, -0.7286033630371094, -0.23552322387695312, -0.4610862731933594, -0.5941543579101562, -0.17425155639648438, -0.17778778076171875, 0.3476295471191406, 36.8707389831543, -0.08734130859375, -0.7265510559082031, 0.1578369140625, -0.19481277465820312, -0.10348892211914062, 30.60105323791504, -0.9498023986816406, 35.77417755126953, -2.7539024353027344, -0.10582733154296875, -0.4843940734863281, -0.7926139831542969, -0.8126029968261719, -0.318328857421875, -0.17778778076171875, 0.22217178344726562, -0.519012451171875, -0.1363372802734375, -1.4560317993164062, 0.79388427734375, -0.21429443359375, -10.385936737060547, 0.3026084899902344, -0.2841072082519531, 0.6562004089355469, 35.410362243652344, 1.6993980407714844, -0.11886978149414062, 36.8707389831543, -0.08733749389648438, -0.1735382080078125, -0.2857322692871094, -2.0610694885253906, 35.41978073120117, -0.172698974609375, -0.2832221984863281, -0.7111740112304688, -0.19260787963867188, 0.05682373046875, -1.467987060546875, 0.38323211669921875, -0.35208892822265625, -0.7364845275878906, -0.04833221435546875, 0.4820365905761719, -0.34258270263671875, -0.5601615905761719, -0.09484481811523438, -0.7167396545410156, 1.2533760070800781, -0.6986770629882812, -0.40478515625, -0.6648139953613281, -0.8677864074707031, -0.13651275634765625, -0.3032264709472656, 0.5006942749023438, -0.31787109375, 0.259063720703125, 0.3442268371582031, 0.3705024719238281, -0.4912300109863281, 0.7053604125976562, -1.0363845825195312, -0.487640380859375, -0.24229812622070312, -0.16717910766601562, -0.5379447937011719, 0.2190704345703125, -0.4989585876464844, -0.298248291015625, -1.1348876953125, 0.12619400024414062, -0.54876708984375, -3.0909042358398438, -0.4595985412597656, -0.36074066162109375, -0.4692344665527344, -0.5601615905761719, 0.008281707763671875, 0.15057754516601562, -0.23151016235351562, -0.16217803955078125, 0.8477325439453125, 0.27448272705078125, 0.6951217651367188, -0.4602508544921875, 0.15318679809570312, -0.7450294494628906, 0.2842903137207031, 3.684154510498047, -0.3621864318847656, 0.4919013977050781, -0.9091224670410156, 37.265567779541016, -0.28153228759765625, -1.4116439819335938, -0.34258270263671875, -0.60809326171875, -0.3963661193847656, -0.4127197265625, 4.8111114501953125, 35.77465057373047, -0.38042449951171875, -0.4875373840332031, -0.8849830627441406, -0.12291336059570312, -0.5751609802246094, -0.23552322387695312, -0.233551025390625, -2.2010154724121094, -0.6670303344726562, 0.022403717041015625, -0.4775352478027344, -0.3088493347167969, -0.46718597412109375, -3.572681427001953, -5.723451614379883, -1.1693687438964844, -3.6545333862304688, 0.1578369140625, 0.021167755126953125], "mean_td_error": 1.4712493419647217}}, "num_steps_sampled": 9000, "num_agent_steps_sampled": 18000, "num_steps_trained": 256256, "num_steps_trained_this_iter": 256, "num_agent_steps_trained": 512512, "last_target_update_ts": 8560, "num_target_updates": 16}, "done": false, "episodes_total": 14, "training_iteration": 9, "trial_id": "e21e6_00000", "experiment_id": "434cd42d33fd4b638f17fc69fa01d74f", "date": "2022-06-14_19-04-55", "timestamp": 1655247895, "time_this_iter_s": 20.938111543655396, "time_total_s": 176.04074382781982, "pid": 2568314, "hostname": "lambda-dual", "node_ip": "10.104.51.19", "config": {"num_workers": 2, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.0005, "train_batch_size": 256, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "cnet", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "1v1env", "observation_space": null, "action_space": null, "env_config": {}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": true, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": null, "evaluation_duration": 10, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"num_workers": 2, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.0005, "train_batch_size": 256, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "cnet", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "1v1env", "observation_space": null, "action_space": null, "env_config": {}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": false, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": null, "evaluation_duration": 10, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"explore": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": true, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": null, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 2, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"policy_01": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]], [[[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]], (3, 64, 64), float32)", "Discrete(7)", {}], "policy_02": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]], [[[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]], (3, 64, 64), float32)", "Discrete(7)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": "<function <lambda> at 0x7f1aa3726ef0>", "policies_to_train": ["policy_01"], "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": 100000, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": false, "hiddens": [], "double_q": false, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": true, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": null, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 2, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"policy_01": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]], [[[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]], (3, 64, 64), float32)", "Discrete(7)", {}], "policy_02": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]], [[[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]], (3, 64, 64), float32)", "Discrete(7)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": "<function <lambda> at 0x7f1aa3726ef0>", "policies_to_train": ["policy_01"], "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": 100000, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": false, "hiddens": [], "double_q": false, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "time_since_restore": 176.04074382781982, "timesteps_since_restore": 2304, "iterations_since_restore": 9, "warmup_time": 45.46659183502197, "perf": {"cpu_util_percent": 24.34666666666666, "ram_util_percent": 9.800000000000002}}
{"episode_reward_max": 441.0, "episode_reward_min": 0.0, "episode_reward_mean": 40.3125, "episode_len_mean": 601.0, "episode_media": {}, "episodes_this_iter": 2, "policy_reward_min": {"policy_01": 0.0, "policy_02": 0.0}, "policy_reward_max": {"policy_01": 204.0, "policy_02": 441.0}, "policy_reward_mean": {"policy_01": 12.75, "policy_02": 27.5625}, "custom_metrics": {}, "hist_stats": {"episode_reward": [0.0, 204.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0], "episode_lengths": [601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601], "policy_policy_01_reward": [0.0, 204.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "policy_policy_02_reward": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.3383169546765703, "mean_inference_ms": 5.881616900596249, "mean_action_processing_ms": 0.0894811751629937, "mean_env_wait_ms": 8.4458031964781, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 2, "timesteps_total": 10000, "timesteps_this_iter": 256, "agent_timesteps_total": 20000, "timers": {"load_time_ms": 1.302, "load_throughput": 196573.206, "learn_time_ms": 11.605, "learn_throughput": 22060.317, "update_time_ms": 2.164}, "info": {"learner": {"policy_01": {"custom_metrics": {}, "learner_stats": {"mean_q": 39.45851516723633, "min_q": 35.397674560546875, "max_q": 81.60482788085938, "cur_lr": 0.0005}, "model": {}, "td_error": [-1.3717079162597656, -0.0085296630859375, 0.2690010070800781, -0.5036048889160156, -0.3009185791015625, 0.3541603088378906, 0.39925384521484375, 0.24749755859375, 0.20664215087890625, 0.5840644836425781, -0.514129638671875, 0.5647964477539062, 0.126678466796875, -0.0155029296875, 0.110321044921875, -0.5393409729003906, 0.09064102172851562, 0.12484359741210938, 0.021129608154296875, -0.033329010009765625, -0.08927536010742188, 0.9780731201171875, 0.2684326171875, 0.6327400207519531, 1.1086654663085938, 5.159156799316406, 0.11706924438476562, -6.502983093261719, -1.0743217468261719, -4.258953094482422, 0.5370674133300781, 0.5763816833496094, -0.42606353759765625, 0.33899688720703125, -0.14854812622070312, 0.28409576416015625, 0.48770904541015625, -0.13982772827148438, 1.0103263854980469, -0.08387374877929688, 0.3365745544433594, -0.03131103515625, -6.502983093261719, -2.0754241943359375, 0.35016632080078125, 0.8904685974121094, 0.3827857971191406, -1.3510551452636719, -0.11473464965820312, 0.4899024963378906, 0.10546112060546875, -4.258953094482422, -0.6419219970703125, 0.11180496215820312, 0.3570098876953125, 0.032268524169921875, 0.9272308349609375, -0.267059326171875, 0.0205841064453125, 0.971099853515625, 0.3652381896972656, 0.12247085571289062, -0.17804336547851562, -0.19145965576171875, 5.159156799316406, 0.6583709716796875, -0.1732025146484375, 0.418701171875, 0.5655555725097656, 0.09862136840820312, 0.5842933654785156, -0.021747589111328125, -0.13237380981445312, 0.4604377746582031, 0.20621871948242188, 0.012561798095703125, -0.0895538330078125, -0.15136337280273438, -12.962516784667969, 1.9003181457519531, -0.23193359375, 0.014003753662109375, -4.91510009765625, -6.250457763671875, 0.3729705810546875, 0.2103118896484375, 0.7145271301269531, 0.7114181518554688, 0.042598724365234375, 0.34407806396484375, 1.7496185302734375, 0.014003753662109375, -1.5022048950195312, 0.5075263977050781, 0.5154800415039062, 0.48201751708984375, 0.3717308044433594, 0.10319137573242188, 0.6765556335449219, 0.22344207763671875, 0.05666351318359375, 0.13960647583007812, 0.3180389404296875, -0.14854812622070312, 2.416088104248047, -0.8221549987792969, -0.08498001098632812, 3.4332275390625e-05, 0.0205841064453125, -0.45388031005859375, 0.340240478515625, 0.13878250122070312, 2.669780731201172, 0.20809555053710938, 0.001617431640625, 0.4961738586425781, -0.013820648193359375, 0.39255523681640625, 0.49152374267578125, 0.3618354797363281, 0.013843536376953125, 0.6009368896484375, 0.8375091552734375, -0.23409652709960938, -0.8975334167480469, 0.54376220703125, -0.7832221984863281, 0.11180496215820312, 37.56903839111328, -0.11133575439453125, 0.5616683959960938, 0.5470390319824219, 0.9252090454101562, 0.13440704345703125, 0.4269218444824219, 0.36285400390625, -0.01488494873046875, 0.0900421142578125, 0.34764862060546875, 0.5026016235351562, 0.22888946533203125, 0.23633575439453125, 0.3441200256347656, -1.8347129821777344, -0.3427886962890625, 3.8964500427246094, 0.4965248107910156, 0.00885009765625, 0.208953857421875, 0.434783935546875, 1.3003273010253906, 0.1261138916015625, 0.4224395751953125, 1.0493202209472656, -0.10408782958984375, -0.870819091796875, -0.12395095825195312, -0.8224334716796875, -4.013496398925781, -5.556243896484375, 0.2709197998046875, 0.340240478515625, 0.12589645385742188, 0.12180328369140625, 0.11742782592773438, 0.9780731201171875, 0.5812568664550781, 0.36785125732421875, -0.18659591674804688, -0.13107681274414062, -0.0505828857421875, 0.39319610595703125, 0.3577690124511719, 0.16736984252929688, -1.0638923645019531, 0.2931671142578125, 0.4178619384765625, 0.013843536376953125, -0.002941131591796875, 2.1128616333007812, -0.88922119140625, -4.171596527099609, -0.3790779113769531, -1.1479110717773438, 0.3269081115722656, 0.3932304382324219, 1.1664199829101562, -0.33155059814453125, -0.16930770874023438, 0.32561492919921875, -4.683204650878906, 0.4011688232421875, 0.0205841064453125, -5.219444274902344, 0.2818870544433594, 0.2446746826171875, 1.0087928771972656, 0.5669593811035156, -0.08051300048828125, -0.07884979248046875, 0.2336273193359375, -0.09526824951171875, -0.561279296875, 0.6059989929199219, 1.3182411193847656, 0.4994850158691406, 0.13735198974609375, -1.9249153137207031, -0.6439437866210938, -0.004444122314453125, -0.558013916015625, 0.3300209045410156, -0.3863372802734375, -0.7558975219726562, 0.2708168029785156, 0.4121246337890625, 0.2745552062988281, -11.82470703125, 0.21080780029296875, 0.4712486267089844, -0.13201904296875, 0.3023109436035156, 0.4726600646972656, 0.051967620849609375, 0.10316085815429688, -0.45359039306640625, 0.647705078125, -0.17926406860351562, 0.3454475402832031, 0.2847480773925781, 0.07677841186523438, -0.24410629272460938, -3.0261611938476562, 0.33930206298828125, 36.36317443847656, 0.8742599487304688, 0.3356208801269531, -0.06826400756835938, -0.045856475830078125, -0.4144706726074219, 0.16322708129882812, -0.055240631103515625, -0.21369171142578125, 0.988189697265625, 0.2545928955078125, 0.45177459716796875, 0.5713157653808594, 0.400390625, 36.36317443847656, 0.549774169921875, 0.34737396240234375, 0.34688568115234375, 37.15485382080078, -0.23112106323242188, 0.6183433532714844, 0.5100669860839844], "mean_td_error": 0.45681262016296387}}, "num_steps_sampled": 10000, "num_agent_steps_sampled": 20000, "num_steps_trained": 288256, "num_steps_trained_this_iter": 256, "num_agent_steps_trained": 576512, "last_target_update_ts": 9568, "num_target_updates": 18}, "done": false, "episodes_total": 16, "training_iteration": 10, "trial_id": "e21e6_00000", "experiment_id": "434cd42d33fd4b638f17fc69fa01d74f", "date": "2022-06-14_19-05-16", "timestamp": 1655247916, "time_this_iter_s": 20.9290452003479, "time_total_s": 196.96978902816772, "pid": 2568314, "hostname": "lambda-dual", "node_ip": "10.104.51.19", "config": {"num_workers": 2, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.0005, "train_batch_size": 256, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "cnet", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "1v1env", "observation_space": null, "action_space": null, "env_config": {}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": true, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": null, "evaluation_duration": 10, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"num_workers": 2, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.0005, "train_batch_size": 256, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "cnet", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "1v1env", "observation_space": null, "action_space": null, "env_config": {}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": false, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": null, "evaluation_duration": 10, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"explore": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": true, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": null, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 2, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"policy_01": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]], [[[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]], (3, 64, 64), float32)", "Discrete(7)", {}], "policy_02": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]], [[[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]], (3, 64, 64), float32)", "Discrete(7)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": "<function <lambda> at 0x7f1aa2ebeb00>", "policies_to_train": ["policy_01"], "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": 100000, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": false, "hiddens": [], "double_q": false, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": true, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": null, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 2, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"policy_01": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]], [[[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]], (3, 64, 64), float32)", "Discrete(7)", {}], "policy_02": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]], [[[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]], (3, 64, 64), float32)", "Discrete(7)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": "<function <lambda> at 0x7f1aa2ebeb00>", "policies_to_train": ["policy_01"], "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": 100000, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": false, "hiddens": [], "double_q": false, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "time_since_restore": 196.96978902816772, "timesteps_since_restore": 2560, "iterations_since_restore": 10, "warmup_time": 45.46659183502197, "perf": {"cpu_util_percent": 24.566666666666663, "ram_util_percent": 9.800000000000002}}
{"episode_reward_max": 441.0, "episode_reward_min": 0.0, "episode_reward_mean": 35.833333333333336, "episode_len_mean": 601.0, "episode_media": {}, "episodes_this_iter": 2, "policy_reward_min": {"policy_01": 0.0, "policy_02": 0.0}, "policy_reward_max": {"policy_01": 204.0, "policy_02": 441.0}, "policy_reward_mean": {"policy_01": 11.333333333333334, "policy_02": 24.5}, "custom_metrics": {}, "hist_stats": {"episode_reward": [0.0, 204.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "episode_lengths": [601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601], "policy_policy_01_reward": [0.0, 204.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "policy_policy_02_reward": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.33848000399618117, "mean_inference_ms": 5.878241182654239, "mean_action_processing_ms": 0.08948987132639402, "mean_env_wait_ms": 8.406990697097303, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 2, "timesteps_total": 11000, "timesteps_this_iter": 256, "agent_timesteps_total": 22000, "timers": {"load_time_ms": 1.317, "load_throughput": 194437.431, "learn_time_ms": 12.028, "learn_throughput": 21284.342, "update_time_ms": 2.234}, "info": {"learner": {"policy_01": {"custom_metrics": {}, "learner_stats": {"mean_q": 39.78450393676758, "min_q": 30.73003578186035, "max_q": 87.87998962402344, "cur_lr": 0.0005}, "model": {}, "td_error": [0.9589309692382812, 0.8446884155273438, -0.2449493408203125, 0.24822235107421875, 0.4329719543457031, 0.28403472900390625, 0.032131195068359375, 1.3481864929199219, 1.3000984191894531, 0.8953399658203125, 0.2803001403808594, 0.12034225463867188, 0.7623863220214844, 0.463470458984375, -3.3521881103515625, 0.8489456176757812, 36.00535583496094, -0.3295478820800781, 0.14510345458984375, 0.8033332824707031, 0.8729515075683594, -13.27143669128418, 0.43407440185546875, 0.057048797607421875, 5.7794189453125, 0.6156578063964844, 0.10289382934570312, 0.8886337280273438, -0.13460159301757812, 0.09759902954101562, 0.17931747436523438, 0.4429168701171875, 0.08152008056640625, 0.6268424987792969, 0.005115509033203125, 0.8281898498535156, 1.2403030395507812, 0.07564544677734375, 0.6984138488769531, -0.39951324462890625, 1.0577011108398438, -0.6028099060058594, -1.0591888427734375, 0.4020843505859375, 0.665771484375, 0.390594482421875, 0.12929153442382812, 0.21049880981445312, -0.2449493408203125, 0.0649566650390625, 1.3909378051757812, 1.1151657104492188, 0.8446884155273438, 31.800495147705078, 0.4437980651855469, 0.7947883605957031, 0.4573631286621094, 0.8493690490722656, -0.15716171264648438, 34.813716888427734, 0.9132308959960938, 0.06859970092773438, 0.3782234191894531, 1.09149169921875, 0.12040328979492188, 0.06385421752929688, 1.0980377197265625, 0.50927734375, 0.0506134033203125, 0.7902755737304688, 34.51427459716797, 0.8156967163085938, 0.25127410888671875, 6.052478790283203, 0.8324851989746094, 0.7409934997558594, 0.05078887939453125, 0.78997802734375, 0.9502067565917969, 0.5068817138671875, 0.44048309326171875, 0.22292709350585938, 0.2978019714355469, 0.07067489624023438, -0.17784500122070312, 0.8440780639648438, 0.619873046875, -0.07794570922851562, 0.8364906311035156, -3.641845703125, 0.23902130126953125, 0.3041725158691406, -0.5704116821289062, 0.13033294677734375, 0.9630546569824219, 0.9555206298828125, 0.8935966491699219, 0.5371551513671875, 0.14336395263671875, 0.3076515197753906, -0.31591796875, 0.515411376953125, 0.11563491821289062, 0.5139427185058594, 0.8781852722167969, 0.056491851806640625, -0.18006515502929688, 2.7465057373046875, 0.5120735168457031, 0.3352203369140625, 0.23331832885742188, 0.9601631164550781, 4.314697265625, 0.7938766479492188, 1.145233154296875, 3.0905227661132812, 0.056270599365234375, 0.5477943420410156, 0.587921142578125, -0.09444046020507812, 0.10289382934570312, 0.6436042785644531, 0.8371200561523438, 0.2655830383300781, 0.55828857421875, 1.3041648864746094, 0.5490379333496094, 0.6511116027832031, 0.2672882080078125, 0.16006851196289062, 0.4628639221191406, -0.3364143371582031, 0.389373779296875, 1.2530784606933594, 0.4329681396484375, 0.5173416137695312, 0.4713020324707031, 0.7072944641113281, 0.08156585693359375, 0.03754425048828125, 0.08421707153320312, 1.139251708984375, 0.08796310424804688, -0.3491630554199219, 0.3212852478027344, 0.8384666442871094, 36.00535583496094, 0.7019004821777344, 1.6228218078613281, 0.4448738098144531, 0.78997802734375, -0.3966178894042969, 2.165966033935547, 34.398136138916016, 0.08493423461914062, 0.8641586303710938, -1.1878242492675781, 0.5356712341308594, 0.7233924865722656, 0.835235595703125, 2.018627166748047, 1.6700325012207031, 0.9657745361328125, 0.7110824584960938, 0.41289520263671875, 0.5848007202148438, 5.7794189453125, -3.353759765625, 0.23317718505859375, -0.41845703125, 35.301387786865234, 0.26514434814453125, 0.3937339782714844, 0.15857315063476562, 0.2503242492675781, 0.5490379333496094, 0.87451171875, 0.09946823120117188, 1.0075569152832031, 1.04132080078125, 1.370880126953125, 0.18205642700195312, 0.5282249450683594, 5.569126129150391, 0.1110687255859375, 1.1507644653320312, 0.7158775329589844, 0.8464622497558594, 0.19815826416015625, -0.9824867248535156, -0.15311050415039062, 0.3120574951171875, 0.7769012451171875, -1.1199455261230469, 0.5757598876953125, 0.9722709655761719, 0.3131217956542969, 0.74981689453125, -0.3197441101074219, 0.4592857360839844, 36.14311981201172, 0.7399406433105469, 0.6553916931152344, 1.3136024475097656, 0.4135169982910156, 0.3696136474609375, -13.270879745483398, 0.3874092102050781, 2.10791015625, 0.0013580322265625, 0.8121871948242188, 0.8407211303710938, 36.14311981201172, 1.3185501098632812, 1.29681396484375, 0.42010498046875, 0.7053642272949219, 0.8986244201660156, 0.9541778564453125, 0.6483688354492188, 0.6154251098632812, 0.10882186889648438, 0.20859146118164062, 0.3789825439453125, 0.21439361572265625, 0.9956817626953125, 0.0691375732421875, 0.13148117065429688, 0.5452194213867188, 1.3481864929199219, 8.627197265625, 0.5117034912109375, -0.26911163330078125, 0.659271240234375, 1.2079925537109375, 1.0828666687011719, 35.301387786865234, 0.5304183959960938, -0.1390533447265625, 1.2744712829589844, 0.43344879150390625, -0.6497535705566406, 0.0556640625, 0.22944259643554688, 0.13657760620117188, 1.8988113403320312, -0.10097122192382812, 0.15313720703125, 0.7124214172363281, 0.24279403686523438, 36.45395278930664, 0.3939476013183594, 0.7167320251464844, 0.4218254089355469, 0.8714866638183594, -11.776237487792969], "mean_td_error": 1.9200701713562012}}, "num_steps_sampled": 11000, "num_agent_steps_sampled": 22000, "num_steps_trained": 320256, "num_steps_trained_this_iter": 256, "num_agent_steps_trained": 640512, "last_target_update_ts": 10576, "num_target_updates": 20}, "done": false, "episodes_total": 18, "training_iteration": 11, "trial_id": "e21e6_00000", "experiment_id": "434cd42d33fd4b638f17fc69fa01d74f", "date": "2022-06-14_19-05-37", "timestamp": 1655247937, "time_this_iter_s": 20.97951912879944, "time_total_s": 217.94930815696716, "pid": 2568314, "hostname": "lambda-dual", "node_ip": "10.104.51.19", "config": {"num_workers": 2, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.0005, "train_batch_size": 256, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "cnet", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "1v1env", "observation_space": null, "action_space": null, "env_config": {}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": true, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": null, "evaluation_duration": 10, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"num_workers": 2, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.0005, "train_batch_size": 256, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "cnet", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "1v1env", "observation_space": null, "action_space": null, "env_config": {}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": false, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": null, "evaluation_duration": 10, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"explore": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": true, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": null, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 2, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"policy_01": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]], [[[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]], (3, 64, 64), float32)", "Discrete(7)", {}], "policy_02": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]], [[[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]], (3, 64, 64), float32)", "Discrete(7)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": "<function <lambda> at 0x7f1aa2ebeef0>", "policies_to_train": ["policy_01"], "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": 100000, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": false, "hiddens": [], "double_q": false, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": true, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": null, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 2, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"policy_01": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]], [[[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]], (3, 64, 64), float32)", "Discrete(7)", {}], "policy_02": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]], [[[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]], (3, 64, 64), float32)", "Discrete(7)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": "<function <lambda> at 0x7f1aa2ebeef0>", "policies_to_train": ["policy_01"], "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": 100000, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": false, "hiddens": [], "double_q": false, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "time_since_restore": 217.94930815696716, "timesteps_since_restore": 2816, "iterations_since_restore": 11, "warmup_time": 45.46659183502197, "perf": {"cpu_util_percent": 24.82666666666666, "ram_util_percent": 9.893333333333333}}
{"episode_reward_max": 441.0, "episode_reward_min": 0.0, "episode_reward_mean": 35.833333333333336, "episode_len_mean": 601.0, "episode_media": {}, "episodes_this_iter": 0, "policy_reward_min": {"policy_01": 0.0, "policy_02": 0.0}, "policy_reward_max": {"policy_01": 204.0, "policy_02": 441.0}, "policy_reward_mean": {"policy_01": 11.333333333333334, "policy_02": 24.5}, "custom_metrics": {}, "hist_stats": {"episode_reward": [0.0, 204.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "episode_lengths": [601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601], "policy_policy_01_reward": [0.0, 204.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "policy_policy_02_reward": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.33848000399618117, "mean_inference_ms": 5.878241182654239, "mean_action_processing_ms": 0.08948987132639402, "mean_env_wait_ms": 8.406990697097303, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 2, "timesteps_total": 12000, "timesteps_this_iter": 256, "agent_timesteps_total": 24000, "timers": {"load_time_ms": 1.35, "load_throughput": 189576.409, "learn_time_ms": 11.68, "learn_throughput": 21918.735, "update_time_ms": 2.266}, "info": {"learner": {"policy_01": {"custom_metrics": {}, "learner_stats": {"mean_q": 37.95404815673828, "min_q": 25.930498123168945, "max_q": 83.99744415283203, "cur_lr": 0.0005}, "model": {}, "td_error": [-0.015674591064453125, 0.4224128723144531, -0.051639556884765625, 1.343780517578125, 0.3103370666503906, 0.36061859130859375, 0.039318084716796875, 0.23694610595703125, 0.127410888671875, 0.021484375, -3.7879867553710938, -2.422718048095703, 0.4473114013671875, -0.137603759765625, 0.46903228759765625, -0.11281585693359375, -0.03174591064453125, -0.3116569519042969, -1.245330810546875, -2.9344863891601562, 0.3736152648925781, 0.13269805908203125, 0.2555389404296875, -5.993110656738281, -0.7234306335449219, 0.879425048828125, -1.9873542785644531, 0.09090042114257812, -1.6261520385742188, 0.45374298095703125, 0.20621871948242188, 0.14379119873046875, 0.36760711669921875, 0.5374069213867188, -0.2665443420410156, 1.1228675842285156, 0.1585235595703125, 0.8459663391113281, 0.14622116088867188, 25.095476150512695, 0.7025871276855469, 0.4357109069824219, 0.000537872314453125, 0.37244415283203125, 0.019168853759765625, 0.14891815185546875, -0.2690925598144531, 0.11147689819335938, 0.24161148071289062, -0.19028472900390625, 0.22777175903320312, 0.37200927734375, 34.26013946533203, 0.15063095092773438, -0.7846107482910156, 0.8026771545410156, 0.15227890014648438, 0.3936500549316406, -0.07865142822265625, -0.00125885009765625, 0.3102302551269531, 0.406524658203125, 0.15782546997070312, 34.93402862548828, 0.45652008056640625, -1.8001861572265625, -0.37920379638671875, 0.15256881713867188, 0.2597694396972656, -0.18531417846679688, -0.41655731201171875, -0.10072708129882812, -0.36670684814453125, -0.13905715942382812, 0.1262054443359375, -0.3685646057128906, 0.3403778076171875, -0.28124237060546875, 0.46903228759765625, 4.286857604980469, -0.4909095764160156, -0.06769180297851562, 0.1340179443359375, -1.0078620910644531, 0.4325904846191406, -0.3662757873535156, 1.3642730712890625, 0.18855667114257812, -0.08659744262695312, 0.4800682067871094, -0.3494453430175781, 0.167449951171875, -1.87213134765625, 34.93402862548828, -0.2626380920410156, 0.13866043090820312, -0.01363372802734375, 0.14891815185546875, -1.7260208129882812, 0.3229827880859375, -1.5460166931152344, 0.007976531982421875, -0.710418701171875, 0.06760787963867188, 0.7406845092773438, 34.75344467163086, 0.40505218505859375, -0.13613510131835938, 0.3936500549316406, 0.20598220825195312, 0.0872955322265625, -0.3617210388183594, 0.13499832153320312, -0.2470703125, 0.3708343505859375, -0.16153335571289062, -0.08526992797851562, -0.3697967529296875, -0.16959762573242188, 0.42145538330078125, 0.326507568359375, -0.6980743408203125, 0.17069625854492188, -0.4793701171875, 0.04613494873046875, -0.09817123413085938, 35.30172348022461, 0.2621574401855469, 0.33725738525390625, 0.12531661987304688, -0.07736587524414062, -2.0680198669433594, 0.12999343872070312, 0.06987380981445312, -0.020992279052734375, -0.002857208251953125, 0.16191482543945312, -0.15215301513671875, -1.2321701049804688, 0.14891815185546875, -0.17906570434570312, -0.029071807861328125, 0.14609527587890625, 0.13699722290039062, -5.159431457519531, 0.19744491577148438, -0.0677642822265625, 0.19787216186523438, 0.14731597900390625, 0.4326972961425781, -3.6596298217773438, 0.23227691650390625, -0.10509490966796875, -0.6239509582519531, -0.3416786193847656, 0.13332748413085938, -1.1765480041503906, 0.3059425354003906, 0.20606613159179688, 0.408599853515625, -0.06503677368164062, 0.14282989501953125, -0.042156219482421875, -0.038631439208984375, 0.188995361328125, 0.44078826904296875, 0.06335067749023438, -0.07131195068359375, 0.19664764404296875, 0.3752250671386719, -0.06433868408203125, 0.20684432983398438, -0.048099517822265625, -1.0176315307617188, 0.3383216857910156, -0.18151092529296875, -0.2602806091308594, -0.018894195556640625, -0.07738494873046875, 0.3372764587402344, -1.7515335083007812, 35.43494415283203, 0.23878097534179688, -0.37819671630859375, 0.4668617248535156, 0.7998847961425781, 0.0255584716796875, 35.30172348022461, 0.343963623046875, 0.234832763671875, -0.13710403442382812, 0.4742317199707031, 0.039592742919921875, 0.1193695068359375, 0.3963165283203125, -0.16252517700195312, 0.29740142822265625, 0.4217109680175781, -0.5484962463378906, 0.4451103210449219, 0.14194869995117188, 0.1973724365234375, 0.17580795288085938, 0.422271728515625, 0.14034652709960938, 0.2318267822265625, 0.153411865234375, -0.2534523010253906, -0.6730537414550781, -1.0913581848144531, 0.1875762939453125, -0.016254425048828125, 1.0439987182617188, 0.16216659545898438, 0.3550567626953125, -0.5752754211425781, 0.14932632446289062, -0.6320114135742188, 0.6031723022460938, 0.08504867553710938, -0.25787353515625, 0.4534645080566406, 0.2738609313964844, -4.736515045166016, 0.2923316955566406, -0.035091400146484375, 0.21419906616210938, 0.13782501220703125, 0.7079582214355469, -0.09193801879882812, 0.14089202880859375, 0.4623985290527344, 0.1838836669921875, 0.2707977294921875, 0.018970489501953125, 0.5168418884277344, 0.23829269409179688, 0.2606697082519531, 0.14277267456054688, 0.10897445678710938, 0.0420989990234375, -0.30657958984375, -0.00241851806640625, 0.08276748657226562, 0.3420600891113281, 0.6632843017578125, 0.4395332336425781, -0.3632926940917969, 34.93402862548828, 0.5758514404296875, 0.4103660583496094, 0.4378204345703125, 0.38698577880859375, 0.17647933959960938, -0.08526992797851562, -0.026500701904296875], "mean_td_error": 1.122965931892395}}, "num_steps_sampled": 12000, "num_agent_steps_sampled": 24000, "num_steps_trained": 352256, "num_steps_trained_this_iter": 256, "num_agent_steps_trained": 704512, "last_target_update_ts": 11584, "num_target_updates": 22}, "done": false, "episodes_total": 18, "training_iteration": 12, "trial_id": "e21e6_00000", "experiment_id": "434cd42d33fd4b638f17fc69fa01d74f", "date": "2022-06-14_19-05-58", "timestamp": 1655247958, "time_this_iter_s": 20.860183000564575, "time_total_s": 238.80949115753174, "pid": 2568314, "hostname": "lambda-dual", "node_ip": "10.104.51.19", "config": {"num_workers": 2, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.0005, "train_batch_size": 256, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "cnet", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "1v1env", "observation_space": null, "action_space": null, "env_config": {}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": true, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": null, "evaluation_duration": 10, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"num_workers": 2, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.0005, "train_batch_size": 256, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "cnet", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "1v1env", "observation_space": null, "action_space": null, "env_config": {}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": false, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": null, "evaluation_duration": 10, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"explore": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": true, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": null, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 2, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"policy_01": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]], [[[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]], (3, 64, 64), float32)", "Discrete(7)", {}], "policy_02": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]], [[[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]], (3, 64, 64), float32)", "Discrete(7)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": "<function <lambda> at 0x7f1aa3747b90>", "policies_to_train": ["policy_01"], "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": 100000, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": false, "hiddens": [], "double_q": false, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": true, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": null, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 2, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"policy_01": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]], [[[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]], (3, 64, 64), float32)", "Discrete(7)", {}], "policy_02": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]], [[[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]], (3, 64, 64), float32)", "Discrete(7)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": "<function <lambda> at 0x7f1aa3747b90>", "policies_to_train": ["policy_01"], "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": 100000, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": false, "hiddens": [], "double_q": false, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "time_since_restore": 238.80949115753174, "timesteps_since_restore": 3072, "iterations_since_restore": 12, "warmup_time": 45.46659183502197, "perf": {"cpu_util_percent": 24.51724137931034, "ram_util_percent": 9.906896551724138}}
{"episode_reward_max": 441.0, "episode_reward_min": 0.0, "episode_reward_mean": 32.25, "episode_len_mean": 601.0, "episode_media": {}, "episodes_this_iter": 2, "policy_reward_min": {"policy_01": 0.0, "policy_02": 0.0}, "policy_reward_max": {"policy_01": 204.0, "policy_02": 441.0}, "policy_reward_mean": {"policy_01": 10.2, "policy_02": 22.05}, "custom_metrics": {}, "hist_stats": {"episode_reward": [0.0, 204.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "episode_lengths": [601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601], "policy_policy_01_reward": [0.0, 204.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "policy_policy_02_reward": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.33808515035987746, "mean_inference_ms": 5.873347224460039, "mean_action_processing_ms": 0.08947285246492767, "mean_env_wait_ms": 8.368548605508655, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 2, "timesteps_total": 13000, "timesteps_this_iter": 256, "agent_timesteps_total": 26000, "timers": {"load_time_ms": 1.329, "load_throughput": 192647.808, "learn_time_ms": 11.528, "learn_throughput": 22205.853, "update_time_ms": 2.106}, "info": {"learner": {"policy_01": {"custom_metrics": {}, "learner_stats": {"mean_q": 39.59019088745117, "min_q": 30.149028778076172, "max_q": 109.67743682861328, "cur_lr": 0.0005}, "model": {}, "td_error": [-0.7753982543945312, -0.7638893127441406, -0.5255775451660156, -0.23791885375976562, -0.20450973510742188, 0.18166732788085938, -0.6249313354492188, -0.66162109375, -0.6685066223144531, -0.8323516845703125, -1.498931884765625, 33.93714904785156, -0.50469970703125, -0.5584754943847656, -0.0006103515625, -0.105743408203125, 1.2047691345214844, -0.8389472961425781, -1.1149330139160156, -0.3711585998535156, 0.11085891723632812, -0.6294326782226562, -0.655853271484375, -0.7198410034179688, -0.08094406127929688, -0.9776382446289062, -0.1330413818359375, -1.0683174133300781, -1.7709426879882812, -0.2662391662597656, -0.36754608154296875, 35.37882614135742, 0.34537506103515625, -0.3224220275878906, -0.14209747314453125, -1.19525146484375, -0.09371566772460938, -0.33443450927734375, 1.34417724609375, 0.003063201904296875, -0.6644439697265625, -0.518707275390625, -0.6972579956054688, -0.8338699340820312, 0.020915985107421875, -0.3440704345703125, -0.6507987976074219, -0.4392967224121094, -1.7785110473632812, -1.0539054870605469, -0.3502197265625, -0.7475471496582031, -0.4951896667480469, -2.2223587036132812, -0.624359130859375, -1.1081390380859375, -0.8529090881347656, -1.206573486328125, 34.423160552978516, -0.053955078125, -1.069549560546875, -1.40081787109375, -1.5784721374511719, -0.7391624450683594, -23.87194061279297, -0.34139251708984375, -0.41475677490234375, -0.3406639099121094, 0.1662750244140625, -1.2383041381835938, -0.7677955627441406, -1.0132293701171875, -1.0191497802734375, -0.77178955078125, -4.085170745849609, -0.7196998596191406, 0.379638671875, -0.6537284851074219, -0.14449691772460938, -0.6749267578125, -0.4199867248535156, 27.24127197265625, 0.2693595886230469, -0.37464141845703125, -0.3555870056152344, -0.6491889953613281, 1.3552513122558594, 0.7131881713867188, -0.3579826354980469, -0.7933158874511719, -0.9210433959960938, -0.5282974243164062, -0.9583702087402344, -0.106536865234375, -1.0161628723144531, 0.8509635925292969, -0.8060226440429688, -0.3226356506347656, 0.555206298828125, -0.529541015625, -0.7645454406738281, -0.5680809020996094, -6.711067199707031, -0.8167533874511719, -0.7312774658203125, -0.045482635498046875, -0.3173828125, -5.709983825683594, -0.87310791015625, -0.8282012939453125, 0.027843475341796875, -1.2822608947753906, -1.0519752502441406, -0.5902252197265625, -0.4093132019042969, -5.289588928222656, -0.5007247924804688, -0.659698486328125, -0.08774185180664062, -0.741912841796875, -0.7050132751464844, -0.6356086730957031, -0.9604034423828125, -0.8161239624023438, -2.1575393676757812, -0.18235015869140625, -0.3026618957519531, -1.2522392272949219, -0.8285560607910156, -0.7706108093261719, -0.3450126647949219, -0.333709716796875, -0.7045249938964844, -0.6015739440917969, -0.6539878845214844, 0.12538909912109375, -0.3503265380859375, -0.5871772766113281, -0.15687179565429688, -0.6525993347167969, -0.11649703979492188, -0.6688957214355469, -0.9790306091308594, -0.4799690246582031, -0.08881759643554688, -0.3057823181152344, -6.038780212402344, -0.6451225280761719, -0.03102874755859375, -0.3384246826171875, -1.0671119689941406, -0.22628402709960938, -0.30722808837890625, -0.07553863525390625, 0.0077667236328125, -0.03455352783203125, -0.023670196533203125, -6.573890686035156, -0.07932281494140625, -0.18390274047851562, -0.34183502197265625, -1.743621826171875, -0.4154548645019531, 0.2801513671875, -0.6506118774414062, -0.18029022216796875, -0.7493324279785156, -0.17375946044921875, -1.9391326904296875, -0.5422630310058594, -0.24810409545898438, -0.7300529479980469, -0.49684906005859375, -1.7589950561523438, 0.1023712158203125, 5.247611999511719, -0.3601799011230469, 0.2959747314453125, -0.4181404113769531, -0.6678123474121094, -0.13329315185546875, -0.3412284851074219, -0.7037277221679688, -0.6578826904296875, -0.19426345825195312, -0.3046455383300781, -0.2574348449707031, -0.6499404907226562, -0.4640083312988281, -0.17691421508789062, -0.6563796997070312, -3.569805145263672, 35.317256927490234, -0.2837982177734375, 35.37882614135742, -0.3897247314453125, -0.605926513671875, -0.3724632263183594, 35.27496337890625, 0.8676643371582031, 35.27496337890625, 33.056785583496094, -1.0920906066894531, -1.1851730346679688, -0.5318222045898438, -0.8374252319335938, -0.8093376159667969, -0.5483741760253906, -0.31966400146484375, -0.3436927795410156, -0.7924613952636719, 0.2959747314453125, -0.3846549987792969, 0.21027755737304688, -0.8135643005371094, -0.3315582275390625, -0.49962615966796875, -0.7286148071289062, -0.5697822570800781, 0.02099609375, -5.913627624511719, -1.0216293334960938, -0.129150390625, -0.6771087646484375, -0.7766189575195312, 0.7298355102539062, -0.6915435791015625, -0.5258102416992188, -0.3528594970703125, 34.326229095458984, -0.4304771423339844, -0.17558670043945312, -1.3531875610351562, -0.6608810424804688, -0.25991058349609375, -0.4487457275390625, -3.0133705139160156, -0.7873573303222656, -0.669342041015625, 1.0263710021972656, -0.2223052978515625, -0.8389472961425781, 0.08208465576171875, -0.05100250244140625, 0.23868942260742188, -0.7731285095214844, -3.6696624755859375, -0.24359512329101562, -1.4244728088378906, -0.06751251220703125, -0.6642875671386719, 0.07834243774414062, -1.1520500183105469, -0.1248779296875, -0.8282890319824219, -0.9166336059570312], "mean_td_error": 0.6054896116256714}}, "num_steps_sampled": 13000, "num_agent_steps_sampled": 26000, "num_steps_trained": 384256, "num_steps_trained_this_iter": 256, "num_agent_steps_trained": 768512, "last_target_update_ts": 12592, "num_target_updates": 24}, "done": false, "episodes_total": 20, "training_iteration": 13, "trial_id": "e21e6_00000", "experiment_id": "434cd42d33fd4b638f17fc69fa01d74f", "date": "2022-06-14_19-06-19", "timestamp": 1655247979, "time_this_iter_s": 21.09358549118042, "time_total_s": 259.90307664871216, "pid": 2568314, "hostname": "lambda-dual", "node_ip": "10.104.51.19", "config": {"num_workers": 2, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.0005, "train_batch_size": 256, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "cnet", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "1v1env", "observation_space": null, "action_space": null, "env_config": {}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": true, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": null, "evaluation_duration": 10, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"num_workers": 2, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.0005, "train_batch_size": 256, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "cnet", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "1v1env", "observation_space": null, "action_space": null, "env_config": {}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": false, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": null, "evaluation_duration": 10, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"explore": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": true, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": null, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 2, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"policy_01": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]], [[[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]], (3, 64, 64), float32)", "Discrete(7)", {}], "policy_02": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]], [[[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]], (3, 64, 64), float32)", "Discrete(7)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": "<function <lambda> at 0x7f1aa375fa70>", "policies_to_train": ["policy_01"], "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": 100000, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": false, "hiddens": [], "double_q": false, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": true, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": null, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 2, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"policy_01": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]], [[[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]], (3, 64, 64), float32)", "Discrete(7)", {}], "policy_02": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]], [[[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]], (3, 64, 64), float32)", "Discrete(7)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": "<function <lambda> at 0x7f1aa375fa70>", "policies_to_train": ["policy_01"], "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": 100000, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": false, "hiddens": [], "double_q": false, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "time_since_restore": 259.90307664871216, "timesteps_since_restore": 3328, "iterations_since_restore": 13, "warmup_time": 45.46659183502197, "perf": {"cpu_util_percent": 25.2, "ram_util_percent": 9.996666666666666}}
{"episode_reward_max": 441.0, "episode_reward_min": 0.0, "episode_reward_mean": 29.318181818181817, "episode_len_mean": 601.0, "episode_media": {}, "episodes_this_iter": 2, "policy_reward_min": {"policy_01": 0.0, "policy_02": 0.0}, "policy_reward_max": {"policy_01": 204.0, "policy_02": 441.0}, "policy_reward_mean": {"policy_01": 9.272727272727273, "policy_02": 20.045454545454547}, "custom_metrics": {}, "hist_stats": {"episode_reward": [0.0, 204.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "episode_lengths": [601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601], "policy_policy_01_reward": [0.0, 204.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "policy_policy_02_reward": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.3378704407583111, "mean_inference_ms": 5.869350904804491, "mean_action_processing_ms": 0.08945190194121415, "mean_env_wait_ms": 8.335061369297732, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 2, "timesteps_total": 14000, "timesteps_this_iter": 256, "agent_timesteps_total": 28000, "timers": {"load_time_ms": 1.308, "load_throughput": 195656.23, "learn_time_ms": 12.324, "learn_throughput": 20772.678, "update_time_ms": 2.369}, "info": {"learner": {"policy_01": {"custom_metrics": {}, "learner_stats": {"mean_q": 40.88214111328125, "min_q": 27.377456665039062, "max_q": 113.41889953613281, "cur_lr": 0.0005}, "model": {}, "td_error": [2.2173843383789062, 0.8848609924316406, 0.5727005004882812, 1.000946044921875, 1.1380767822265625, 1.1762199401855469, 1.4600753784179688, 0.8621788024902344, 2.2794570922851562, 2.01678466796875, 1.2604522705078125, 0.8420143127441406, 2.0417404174804688, 0.6276779174804688, 0.9532470703125, 0.6078948974609375, 0.03387451171875, 1.3250656127929688, -7.1890411376953125, 1.988494873046875, 0.7965278625488281, 0.6758537292480469, 1.1287918090820312, 1.3225173950195312, 1.3360633850097656, 1.7312240600585938, 0.8431434631347656, 1.76708984375, 1.4601860046386719, 1.3931427001953125, 0.9619026184082031, 1.8324203491210938, 1.4558868408203125, 0.44611358642578125, 0.9402961730957031, 3.7548866271972656, 0.9077911376953125, 35.072200775146484, 0.9451446533203125, 0.8055839538574219, 1.37548828125, 1.4369010925292969, -0.08782196044921875, 1.3694038391113281, 36.884857177734375, 0.8852005004882812, -0.045543670654296875, 0.3606109619140625, 1.1578216552734375, 0.8726348876953125, 0.13074111938476562, 0.4788627624511719, 1.3686141967773438, 0.0777587890625, 1.430816650390625, 1.7725715637207031, 2.2098045349121094, -4.062232971191406, 1.3727760314941406, 35.71205139160156, 0.9571456909179688, 1.346893310546875, 0.19580841064453125, 0.8021469116210938, 1.6660919189453125, 1.3225173950195312, 1.1450653076171875, 1.6449470520019531, 0.022518157958984375, 1.1755332946777344, 0.19942855834960938, 1.603912353515625, 0.10928726196289062, 0.9636383056640625, -0.6032981872558594, 0.4885406494140625, 0.10928726196289062, 36.69376754760742, 1.606109619140625, 0.695831298828125, 0.5059852600097656, 0.9978141784667969, 0.2581787109375, 0.6078948974609375, 0.1173553466796875, 1.1751899719238281, 1.0004844665527344, 1.4287109375, 0.39527130126953125, 0.18417739868164062, 1.026153564453125, 1.0685844421386719, -0.8630867004394531, 0.7905769348144531, 0.5255851745605469, 1.3320960998535156, -0.5423011779785156, 0.5549354553222656, 5.2984466552734375, 0.1657257080078125, 28.644718170166016, 1.7743034362792969, 0.24004745483398438, 1.6734161376953125, 0.24658966064453125, 1.4051017761230469, 0.9404258728027344, 1.7345542907714844, -1.5908622741699219, 0.38068389892578125, 1.547943115234375, 1.5855865478515625, 0.9836921691894531, 0.4810600280761719, 1.41278076171875, 0.6856422424316406, 1.3261375427246094, 1.0811271667480469, 1.5109786987304688, 0.8217697143554688, 36.614967346191406, 0.8843421936035156, 1.6066818237304688, 35.36298751831055, 1.3468437194824219, 1.2520980834960938, 0.7689704895019531, 0.7975120544433594, 1.0807037353515625, 0.9849319458007812, 0.5516624450683594, 0.534149169921875, -8.423370361328125, 1.1431198120117188, 0.6256752014160156, 0.3869209289550781, 0.5237770080566406, -0.6560554504394531, -4.200218200683594, -4.200218200683594, 1.2530441284179688, 1.0647125244140625, 1.0631179809570312, 0.201568603515625, 0.1779022216796875, 1.8408775329589844, 2.5020523071289062, 1.1525421142578125, 1.3387832641601562, 1.6850738525390625, 1.1256790161132812, 1.0976333618164062, 1.0820693969726562, 0.5235595703125, 0.13176345825195312, 0.18316268920898438, 2.4561767578125, 0.9134902954101562, 0.9284400939941406, 1.7280426025390625, 1.3199310302734375, 1.5555610656738281, 1.5257949829101562, 1.4086341857910156, 0.5321769714355469, 1.1883277893066406, 1.3231391906738281, 0.1661834716796875, 0.17787933349609375, 0.5384635925292969, 0.7436866760253906, 1.6305809020996094, 0.966033935546875, 1.6385078430175781, 0.5386695861816406, 0.32506561279296875, 0.5126571655273438, 1.4976882934570312, 0.9979095458984375, -0.158416748046875, 1.4035148620605469, 5.585685729980469, 0.4314994812011719, 0.6134223937988281, 0.4228324890136719, 0.47737884521484375, 1.3492012023925781, 1.6066818237304688, 0.989288330078125, 0.5444374084472656, 0.7682876586914062, 1.734283447265625, 0.9904747009277344, 0.290008544921875, 1.4346771240234375, 1.3562660217285156, 0.9984779357910156, 36.00543212890625, 0.5716285705566406, 1.5654411315917969, 0.6438407897949219, 1.1618614196777344, 26.11019515991211, 0.5634727478027344, 1.2474517822265625, 0.5060691833496094, 0.05059814453125, 0.8134574890136719, 0.15260696411132812, 1.9970970153808594, 1.1911277770996094, 0.15344619750976562, 0.8760948181152344, 0.5792274475097656, 0.8645896911621094, 1.2277641296386719, 0.6758537292480469, 0.07134246826171875, 5.585685729980469, 0.7844505310058594, 0.7421455383300781, 1.3703231811523438, 1.362579345703125, 0.4761543273925781, 0.3393898010253906, 0.7330513000488281, 1.3152656555175781, 0.8404655456542969, 1.0419120788574219, 0.8802032470703125, 0.8763961791992188, 1.3210105895996094, 0.5345268249511719, 36.63232421875, 0.3890342712402344, 0.6010665893554688, 1.163482666015625, 0.4765625, 1.8101577758789062, -3.2663497924804688, 1.0767555236816406, 36.44361877441406, 0.15148544311523438, 0.9780654907226562, 1.1171112060546875, 1.4237747192382812, 0.9620628356933594, 1.3698883056640625, 1.0924644470214844, 0.526824951171875, 0.3221244812011719, 1.104095458984375, 0.8315505981445312, 1.026641845703125, 0.032108306884765625, 1.1920356750488281], "mean_td_error": 2.2929139137268066}}, "num_steps_sampled": 14000, "num_agent_steps_sampled": 28000, "num_steps_trained": 416256, "num_steps_trained_this_iter": 256, "num_agent_steps_trained": 832512, "last_target_update_ts": 13600, "num_target_updates": 26}, "done": false, "episodes_total": 22, "training_iteration": 14, "trial_id": "e21e6_00000", "experiment_id": "434cd42d33fd4b638f17fc69fa01d74f", "date": "2022-06-14_19-06-40", "timestamp": 1655248000, "time_this_iter_s": 21.048968076705933, "time_total_s": 280.9520447254181, "pid": 2568314, "hostname": "lambda-dual", "node_ip": "10.104.51.19", "config": {"num_workers": 2, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.0005, "train_batch_size": 256, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "cnet", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "1v1env", "observation_space": null, "action_space": null, "env_config": {}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": true, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": null, "evaluation_duration": 10, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"num_workers": 2, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.0005, "train_batch_size": 256, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "cnet", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "1v1env", "observation_space": null, "action_space": null, "env_config": {}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": false, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": null, "evaluation_duration": 10, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"explore": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": true, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": null, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 2, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"policy_01": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]], [[[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]], (3, 64, 64), float32)", "Discrete(7)", {}], "policy_02": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]], [[[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]], (3, 64, 64), float32)", "Discrete(7)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": "<function <lambda> at 0x7f1aa2ecbdd0>", "policies_to_train": ["policy_01"], "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": 100000, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": false, "hiddens": [], "double_q": false, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": true, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": null, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 2, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"policy_01": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]], [[[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]], (3, 64, 64), float32)", "Discrete(7)", {}], "policy_02": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]], [[[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]], (3, 64, 64), float32)", "Discrete(7)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": "<function <lambda> at 0x7f1aa2ecbdd0>", "policies_to_train": ["policy_01"], "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": 100000, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": false, "hiddens": [], "double_q": false, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "time_since_restore": 280.9520447254181, "timesteps_since_restore": 3584, "iterations_since_restore": 14, "warmup_time": 45.46659183502197, "perf": {"cpu_util_percent": 25.086666666666666, "ram_util_percent": 10.050000000000004}}
{"episode_reward_max": 441.0, "episode_reward_min": 0.0, "episode_reward_mean": 26.875, "episode_len_mean": 601.0, "episode_media": {}, "episodes_this_iter": 2, "policy_reward_min": {"policy_01": 0.0, "policy_02": 0.0}, "policy_reward_max": {"policy_01": 204.0, "policy_02": 441.0}, "policy_reward_mean": {"policy_01": 8.5, "policy_02": 18.375}, "custom_metrics": {}, "hist_stats": {"episode_reward": [0.0, 204.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "episode_lengths": [601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601], "policy_policy_01_reward": [0.0, 204.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "policy_policy_02_reward": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.3377708409280673, "mean_inference_ms": 5.865880268092838, "mean_action_processing_ms": 0.08942900167308543, "mean_env_wait_ms": 8.305635424979615, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 2, "timesteps_total": 15000, "timesteps_this_iter": 256, "agent_timesteps_total": 30000, "timers": {"load_time_ms": 1.337, "load_throughput": 191411.477, "learn_time_ms": 11.474, "learn_throughput": 22310.962, "update_time_ms": 2.224}, "info": {"learner": {"policy_01": {"custom_metrics": {}, "learner_stats": {"mean_q": 39.672584533691406, "min_q": 28.61832618713379, "max_q": 111.36988830566406, "cur_lr": 0.0005}, "model": {}, "td_error": [-0.05173492431640625, 35.366554260253906, -0.07452774047851562, -0.08274459838867188, 0.08709335327148438, -0.5783157348632812, 0.3590660095214844, -0.3522834777832031, -0.3968315124511719, 0.232879638671875, 35.870399475097656, -0.1635284423828125, -0.23438644409179688, 0.3923072814941406, -0.2328643798828125, 0.2072296142578125, -0.16089630126953125, 0.21701812744140625, 0.18112945556640625, 0.7914009094238281, 1.5073928833007812, 0.5526161193847656, 1.6167182922363281, 0.5589179992675781, -0.4644775390625, -0.16739654541015625, 0.20722198486328125, -0.3579063415527344, -0.10554885864257812, -0.6394271850585938, -0.5355072021484375, -0.2280426025390625, -0.031909942626953125, -0.8431930541992188, 0.2819480895996094, -0.20366668701171875, 0.6046257019042969, 0.3712615966796875, -0.8253746032714844, 1.9330863952636719, 0.3613624572753906, -0.07903289794921875, 0.001277923583984375, 0.32147979736328125, -0.037326812744140625, 0.41362762451171875, 0.13467788696289062, -0.587890625, -0.10554885864257812, -0.14101028442382812, -0.1953582763671875, -0.09251022338867188, -0.3145103454589844, -0.10070037841796875, -0.0887908935546875, -0.11450576782226562, 0.4476661682128906, 0.3676948547363281, 0.4874305725097656, 0.3413276672363281, 0.15603256225585938, 0.3257942199707031, 0.00846099853515625, -0.5082244873046875, -0.7102279663085938, -0.17786788940429688, 0.601226806640625, 0.5754470825195312, 0.3423881530761719, -5.3795013427734375, 0.6220855712890625, 0.27069854736328125, 0.31574249267578125, 0.21583938598632812, 0.6239166259765625, -0.134674072265625, 0.27278900146484375, 0.46628570556640625, -0.05809783935546875, 0.3674468994140625, -0.3682289123535156, 0.5506744384765625, 0.6375846862792969, 0.4618873596191406, 0.6351470947265625, -0.06296920776367188, 0.20760345458984375, 0.7838134765625, -0.3478546142578125, -0.8294525146484375, -0.30837249755859375, 0.3181800842285156, 0.5799942016601562, 11.192184448242188, -1.0156021118164062, -0.10719680786132812, -0.2997856140136719, -0.07490921020507812, -0.5449485778808594, 0.13354873657226562, 0.2485198974609375, -0.7264671325683594, 1.0703887939453125, -3.3225746154785156, 0.21113204956054688, -0.04032135009765625, 0.01165771484375, -0.5512580871582031, 36.193817138671875, 0.058746337890625, -0.00858306884765625, 26.6495304107666, -0.559661865234375, 35.79327392578125, 0.6752510070800781, -0.5275344848632812, 0.10245895385742188, 0.4136199951171875, 1.0355491638183594, 0.5377845764160156, -0.2676124572753906, -0.12138748168945312, -0.034221649169921875, 0.16168975830078125, -5.760772705078125, -0.032989501953125, 0.42032623291015625, 0.4790191650390625, 0.22590255737304688, 36.05730056762695, 0.5015640258789062, 0.6779823303222656, 0.35056304931640625, -0.139312744140625, -0.16338348388671875, -0.18762969970703125, -0.56732177734375, -0.3243560791015625, 0.5980491638183594, 0.044536590576171875, 0.6691093444824219, 0.4575538635253906, 0.9859390258789062, -0.5453147888183594, 4.258644104003906, -0.14243316650390625, 0.6598281860351562, -0.08896255493164062, 0.3429450988769531, 35.9300537109375, 0.1495361328125, 0.06475830078125, -0.3311920166015625, 0.476287841796875, 0.40174102783203125, -0.4070281982421875, 0.5775260925292969, 0.5860786437988281, 0.538909912109375, 0.3949928283691406, -0.3981132507324219, 0.5710411071777344, -0.09987640380859375, -2.4562606811523438, 5.719291687011719, -0.10265350341796875, -0.13437271118164062, 0.6157417297363281, 0.32045745849609375, -0.25746917724609375, 0.4721412658691406, 0.2349700927734375, -0.5095252990722656, 0.3536720275878906, 0.4616127014160156, 0.3548469543457031, -0.27396392822265625, -8.204132080078125, 1.4255752563476562, 0.49282073974609375, -1.0567779541015625, -0.2607383728027344, -0.2323150634765625, 0.08331680297851562, 0.4321784973144531, 0.5557327270507812, -0.2331390380859375, -0.27257537841796875, -0.005008697509765625, -0.4217720031738281, 0.2203216552734375, 0.3441619873046875, 0.5887222290039062, 35.870399475097656, 0.6684837341308594, 0.39363861083984375, 1.5509071350097656, -0.11634063720703125, 0.254119873046875, -0.0887908935546875, -0.15899658203125, -0.10255813598632812, 0.5218429565429688, 0.16944122314453125, -0.04449462890625, 0.49251556396484375, 0.5527305603027344, 0.5631370544433594, -0.10338973999023438, 0.21766281127929688, -1.7224655151367188, 3.0233116149902344, 0.3506660461425781, -0.07452774047851562, 0.3764915466308594, 0.2874870300292969, -0.5867538452148438, -0.10454940795898438, 0.3167152404785156, 0.3625602722167969, -0.3323631286621094, 0.7070999145507812, 0.23641204833984375, -15.498970031738281, 0.44455718994140625, 0.5606803894042969, 0.14262008666992188, 0.2745704650878906, -0.0069122314453125, 0.46294403076171875, -0.7448692321777344, 0.513031005859375, 0.946929931640625, 0.37676239013671875, 0.20391845703125, -0.172027587890625, 0.04443359375, 0.5367774963378906, -0.6134986877441406, -0.1879730224609375, -0.1058197021484375, -0.09299087524414062, -0.8157844543457031, -3.343902587890625, 0.16102218627929688, -1.8169097900390625, -5.831363677978516, -0.13045501708984375, -0.1821441650390625, 0.3447837829589844, 0.10145187377929688, 0.31385040283203125, -0.034423828125, -0.22278594970703125, 0.5227279663085938, 0.6101112365722656], "mean_td_error": 1.0863101482391357}}, "num_steps_sampled": 15000, "num_agent_steps_sampled": 30000, "num_steps_trained": 448256, "num_steps_trained_this_iter": 256, "num_agent_steps_trained": 896512, "last_target_update_ts": 14608, "num_target_updates": 28}, "done": false, "episodes_total": 24, "training_iteration": 15, "trial_id": "e21e6_00000", "experiment_id": "434cd42d33fd4b638f17fc69fa01d74f", "date": "2022-06-14_19-07-01", "timestamp": 1655248021, "time_this_iter_s": 21.033972024917603, "time_total_s": 301.9860167503357, "pid": 2568314, "hostname": "lambda-dual", "node_ip": "10.104.51.19", "config": {"num_workers": 2, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.0005, "train_batch_size": 256, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "cnet", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "1v1env", "observation_space": null, "action_space": null, "env_config": {}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": true, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": null, "evaluation_duration": 10, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"num_workers": 2, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.0005, "train_batch_size": 256, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "cnet", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "1v1env", "observation_space": null, "action_space": null, "env_config": {}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": false, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": null, "evaluation_duration": 10, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"explore": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": true, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": null, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 2, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"policy_01": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]], [[[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]], (3, 64, 64), float32)", "Discrete(7)", {}], "policy_02": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]], [[[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]], (3, 64, 64), float32)", "Discrete(7)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": "<function <lambda> at 0x7f1aa2ecbef0>", "policies_to_train": ["policy_01"], "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": 100000, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": false, "hiddens": [], "double_q": false, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": true, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": null, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 2, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"policy_01": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]], [[[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]], (3, 64, 64), float32)", "Discrete(7)", {}], "policy_02": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]], [[[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]], (3, 64, 64), float32)", "Discrete(7)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": "<function <lambda> at 0x7f1aa2ecbef0>", "policies_to_train": ["policy_01"], "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": 100000, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": false, "hiddens": [], "double_q": false, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "time_since_restore": 301.9860167503357, "timesteps_since_restore": 3840, "iterations_since_restore": 15, "warmup_time": 45.46659183502197, "perf": {"cpu_util_percent": 25.63, "ram_util_percent": 10.100000000000001}}
{"episode_reward_max": 441.0, "episode_reward_min": 0.0, "episode_reward_mean": 24.807692307692307, "episode_len_mean": 601.0, "episode_media": {}, "episodes_this_iter": 2, "policy_reward_min": {"policy_01": 0.0, "policy_02": 0.0}, "policy_reward_max": {"policy_01": 204.0, "policy_02": 441.0}, "policy_reward_mean": {"policy_01": 7.846153846153846, "policy_02": 16.96153846153846}, "custom_metrics": {}, "hist_stats": {"episode_reward": [0.0, 204.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "episode_lengths": [601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601], "policy_policy_01_reward": [0.0, 204.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "policy_policy_02_reward": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.3377771228687714, "mean_inference_ms": 5.862449787386357, "mean_action_processing_ms": 0.08940534972877347, "mean_env_wait_ms": 8.280188247980753, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 2, "timesteps_total": 16000, "timesteps_this_iter": 256, "agent_timesteps_total": 32000, "timers": {"load_time_ms": 1.35, "load_throughput": 189589.799, "learn_time_ms": 11.839, "learn_throughput": 21623.346, "update_time_ms": 2.056}, "info": {"learner": {"policy_01": {"custom_metrics": {}, "learner_stats": {"mean_q": 39.51054000854492, "min_q": 31.522947311401367, "max_q": 117.8072280883789, "cur_lr": 0.0005}, "model": {}, "td_error": [-0.5654296875, -0.32880401611328125, -0.7060813903808594, 35.368717193603516, -1.0202407836914062, -0.6861419677734375, -0.8840751647949219, -0.5153579711914062, 37.515228271484375, -0.180023193359375, -0.47652435302734375, -0.5469589233398438, -0.2505149841308594, 0.02355194091796875, -0.6548385620117188, 0.03986358642578125, -0.9458694458007812, -0.06552505493164062, -0.4084358215332031, -0.24616622924804688, -1.9250335693359375, -0.5519485473632812, 0.5098075866699219, -2.06988525390625, -0.18636703491210938, -1.0905838012695312, 0.1840972900390625, -0.4773674011230469, 0.2833213806152344, -0.5597381591796875, -0.20114898681640625, -0.6444320678710938, -0.7215957641601562, -0.4365997314453125, -0.2699089050292969, -0.5991935729980469, 0.16833114624023438, -0.6954727172851562, -0.20319747924804688, -0.32704925537109375, -0.3748817443847656, -0.5939216613769531, -0.3679084777832031, -0.8246307373046875, -0.09711456298828125, -0.5366859436035156, -0.5313148498535156, -0.5452232360839844, -0.20319747924804688, -0.5503692626953125, -0.7088165283203125, -0.4630393981933594, -0.4763832092285156, 1.0215606689453125, -0.1911163330078125, -0.5220794677734375, -0.3166999816894531, -0.038562774658203125, 0.23415374755859375, -0.08884429931640625, -0.5658111572265625, -0.22840499877929688, -0.4757881164550781, -0.5497016906738281, -0.6119956970214844, -0.6205291748046875, -3.0074234008789062, -5.1984405517578125, -0.49413299560546875, -0.5496635437011719, -1.9431381225585938, -0.5648918151855469, -0.12145233154296875, 0.035736083984375, -0.2890777587890625, -0.6081619262695312, 8.337127685546875, 0.3111457824707031, -0.8114471435546875, -0.5497245788574219, -0.27486419677734375, -0.6346893310546875, -0.9316368103027344, -0.5438995361328125, 0.38639068603515625, -0.6157798767089844, -0.6204490661621094, -0.6155242919921875, -0.17840194702148438, 0.6307945251464844, -0.38448333740234375, -0.6324615478515625, -0.3431739807128906, -1.1745796203613281, -0.15935516357421875, 0.7920074462890625, -1.1775398254394531, -0.5597381591796875, -0.5699119567871094, -0.4118614196777344, -0.2699089050292969, -0.4553871154785156, -0.4289970397949219, -4.451393127441406, 0.18366622924804688, -0.5257453918457031, -0.32614898681640625, -0.468719482421875, -0.4006538391113281, -0.47891998291015625, -0.1753387451171875, -1.0915107727050781, 0.18366622924804688, -0.3772544860839844, -0.4882011413574219, -1.3896484375, -0.48540496826171875, 0.43715667724609375, -0.35012054443359375, -0.6134452819824219, 0.2926826477050781, -0.3886070251464844, -0.5853309631347656, -0.5294837951660156, -0.5624275207519531, -0.932098388671875, -0.3142814636230469, -0.6287689208984375, -0.9265594482421875, 0.008441925048828125, -0.16989898681640625, -0.12391281127929688, -0.6638450622558594, -0.4965019226074219, -13.369731903076172, -0.6027870178222656, -0.5322303771972656, 0.027286529541015625, -0.11709976196289062, -0.5065574645996094, -0.7947540283203125, 28.25058364868164, -0.4769935607910156, -0.23146820068359375, -0.7015533447265625, 35.9281005859375, -1.0921669006347656, -0.10774993896484375, -0.6104011535644531, -1.4044151306152344, -0.19390487670898438, -0.31899261474609375, -0.36304473876953125, -0.6400909423828125, -1.270721435546875, -0.6173934936523438, -0.09291839599609375, 0.23686599731445312, -0.2952728271484375, -0.5837059020996094, -0.9997901916503906, -0.5090293884277344, -0.6167831420898438, 35.83611297607422, -0.12311935424804688, -0.19077682495117188, -0.980072021484375, -1.0485305786132812, -0.6393585205078125, -0.5502128601074219, -0.17536163330078125, -0.5417594909667969, -0.6809043884277344, -0.6230506896972656, -0.13970565795898438, -0.9696846008300781, -0.1168670654296875, 0.4839630126953125, -0.4056510925292969, -0.5500297546386719, -0.20703125, -1.2621650695800781, -0.41886138916015625, -0.47891998291015625, -0.5989227294921875, -0.33428192138671875, -0.7187271118164062, -0.4084892272949219, -0.8200607299804688, -0.4792594909667969, -0.43849945068359375, -0.566375732421875, 0.46515655517578125, -0.19350051879882812, -0.6240768432617188, -0.6510848999023438, -0.5007400512695312, -0.3770751953125, -0.6641731262207031, -0.35843658447265625, -0.5053977966308594, -0.058734893798828125, -0.30379486083984375, -0.28368377685546875, 12.960342407226562, -0.53265380859375, 0.2173004150390625, 0.08764266967773438, 2.7994308471679688, -0.489349365234375, 0.1028289794921875, -0.5720100402832031, -0.8200607299804688, -0.5117073059082031, -0.6287689208984375, -0.3438606262207031, -0.756195068359375, 1.7354278564453125, -0.5440711975097656, -0.5905685424804688, 0.792694091796875, -0.4710273742675781, -0.3815460205078125, -0.06298828125, -0.0251617431640625, -0.6106643676757812, -0.6598587036132812, -0.6956939697265625, -0.4573631286621094, -1.0759429931640625, -0.5441436767578125, -3.154327392578125, -0.7381401062011719, -0.2872314453125, -0.12694168090820312, -1.4514007568359375, 0.006000518798828125, -0.14684677124023438, -0.8717765808105469, -0.19356918334960938, -0.6157302856445312, 4.044635772705078, -0.5765914916992188, -0.455169677734375, -0.8444175720214844, -0.5490379333496094, -0.5964012145996094, -0.18199920654296875, -0.5184822082519531, 0.21086883544921875, -0.07091522216796875, -0.6959648132324219, 0.13573455810546875, -0.5178909301757812, -1.19171142578125, -0.4972877502441406], "mean_td_error": 0.2624534070491791}}, "num_steps_sampled": 16000, "num_agent_steps_sampled": 32000, "num_steps_trained": 480256, "num_steps_trained_this_iter": 256, "num_agent_steps_trained": 960512, "last_target_update_ts": 15616, "num_target_updates": 30}, "done": false, "episodes_total": 26, "training_iteration": 16, "trial_id": "e21e6_00000", "experiment_id": "434cd42d33fd4b638f17fc69fa01d74f", "date": "2022-06-14_19-07-22", "timestamp": 1655248042, "time_this_iter_s": 21.221609592437744, "time_total_s": 323.20762634277344, "pid": 2568314, "hostname": "lambda-dual", "node_ip": "10.104.51.19", "config": {"num_workers": 2, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.0005, "train_batch_size": 256, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "cnet", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "1v1env", "observation_space": null, "action_space": null, "env_config": {}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": true, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": null, "evaluation_duration": 10, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"num_workers": 2, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.0005, "train_batch_size": 256, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "cnet", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "1v1env", "observation_space": null, "action_space": null, "env_config": {}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": false, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": null, "evaluation_duration": 10, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"explore": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": true, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": null, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 2, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"policy_01": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]], [[[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]], (3, 64, 64), float32)", "Discrete(7)", {}], "policy_02": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]], [[[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]], (3, 64, 64), float32)", "Discrete(7)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": "<function <lambda> at 0x7f1aa372e680>", "policies_to_train": ["policy_01"], "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": 100000, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": false, "hiddens": [], "double_q": false, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": true, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": null, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 2, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"policy_01": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]], [[[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]], (3, 64, 64), float32)", "Discrete(7)", {}], "policy_02": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]], [[[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]], (3, 64, 64), float32)", "Discrete(7)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": "<function <lambda> at 0x7f1aa372e680>", "policies_to_train": ["policy_01"], "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": 100000, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": false, "hiddens": [], "double_q": false, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "time_since_restore": 323.20762634277344, "timesteps_since_restore": 4096, "iterations_since_restore": 16, "warmup_time": 45.46659183502197, "perf": {"cpu_util_percent": 25.87741935483871, "ram_util_percent": 10.196774193548384}}
{"episode_reward_max": 441.0, "episode_reward_min": 0.0, "episode_reward_mean": 23.035714285714285, "episode_len_mean": 601.0, "episode_media": {}, "episodes_this_iter": 2, "policy_reward_min": {"policy_01": 0.0, "policy_02": 0.0}, "policy_reward_max": {"policy_01": 204.0, "policy_02": 441.0}, "policy_reward_mean": {"policy_01": 7.285714285714286, "policy_02": 15.75}, "custom_metrics": {}, "hist_stats": {"episode_reward": [0.0, 204.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "episode_lengths": [601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601], "policy_policy_01_reward": [0.0, 204.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "policy_policy_02_reward": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.33787206899187744, "mean_inference_ms": 5.859167038688098, "mean_action_processing_ms": 0.089380034599097, "mean_env_wait_ms": 8.258418028874022, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 2, "timesteps_total": 17000, "timesteps_this_iter": 256, "agent_timesteps_total": 34000, "timers": {"load_time_ms": 1.381, "load_throughput": 185332.405, "learn_time_ms": 12.034, "learn_throughput": 21273.63, "update_time_ms": 2.37}, "info": {"learner": {"policy_01": {"custom_metrics": {}, "learner_stats": {"mean_q": 41.3675537109375, "min_q": 35.07673645019531, "max_q": 122.30068969726562, "cur_lr": 0.0005}, "model": {}, "td_error": [-1.0026473999023438, 2.6424331665039062, 0.27264404296875, -0.4278907775878906, 1.4406204223632812, -0.5367164611816406, -0.045543670654296875, 0.3578300476074219, -0.6042060852050781, -0.66339111328125, -0.20025253295898438, 0.005390167236328125, -3.619823455810547, -0.2764167785644531, 0.06548309326171875, -0.62994384765625, -0.030689239501953125, 0.15420913696289062, 0.10857391357421875, -0.35821533203125, -0.4743537902832031, -0.7072410583496094, -0.5473403930664062, -0.151397705078125, -0.7236480712890625, 0.36358642578125, 0.10280990600585938, -3.0803146362304688, 0.2936553955078125, -0.27138519287109375, -0.2985725402832031, -0.8540420532226562, -0.48229217529296875, -0.9741439819335938, -0.6095314025878906, 0.034393310546875, -15.757774353027344, -0.5466957092285156, 0.0935516357421875, 0.05597686767578125, -0.00577545166015625, 0.006744384765625, -0.153533935546875, -0.7592658996582031, -0.3969688415527344, -0.7712593078613281, -0.27080535888671875, 2.540599822998047, -16.331336975097656, -1.1241798400878906, 0.2984771728515625, -1.1119041442871094, 0.6190185546875, -2.6587600708007812, -0.6209068298339844, -0.3781166076660156, 0.16169357299804688, 0.4763641357421875, 0.12300491333007812, 0.04831695556640625, -0.41473388671875, -0.8580207824707031, -1.9536666870117188, -0.5351715087890625, -0.22352218627929688, 2.2841415405273438, -10.292816162109375, -0.5267906188964844, -0.14120101928710938, -0.3961639404296875, -0.5429306030273438, -0.5034027099609375, 0.9560356140136719, -0.406646728515625, -0.8332328796386719, -0.5515022277832031, -1.6609764099121094, -0.3138275146484375, -1.3444557189941406, -0.3127403259277344, -1.1268272399902344, -0.1083526611328125, 0.7625350952148438, -2.7310638427734375, 0.7472801208496094, -0.7002182006835938, 0.17200088500976562, 0.015045166015625, -9.350364685058594, -1.2943000793457031, -0.6272659301757812, 0.09739303588867188, -1.2384719848632812, -1.1782646179199219, -5.791004180908203, 0.3033256530761719, 0.3637542724609375, -0.5175819396972656, -0.5055961608886719, -0.7808380126953125, 0.36521148681640625, 0.2750282287597656, 0.059062957763671875, -1.3655281066894531, -1.0629539489746094, -1.8234977722167969, -0.5613861083984375, -0.10453414916992188, -1.4995536804199219, 0.2052764892578125, -1.0331344604492188, -0.398468017578125, -0.5082473754882812, -8.625038146972656, -0.5408058166503906, -0.841888427734375, -1.1026878356933594, 0.3710060119628906, -0.170989990234375, -1.1782646179199219, -0.24004364013671875, -0.6920051574707031, -0.3280754089355469, -0.13775253295898438, -0.037334442138671875, -0.6683311462402344, -0.3693389892578125, -0.178466796875, -0.1601715087890625, -4.9314727783203125, -1.2504692077636719, -0.9762687683105469, 0.40335845947265625, -0.9469757080078125, 0.4662284851074219, -0.3669281005859375, -2.1108016967773438, -0.5766143798828125, -1.0453224182128906, 0.18177032470703125, 35.622589111328125, -0.4200248718261719, 0.2094573974609375, 0.21084213256835938, -0.7899093627929688, 0.17331314086914062, -0.07459640502929688, 0.3229827880859375, 2.1529464721679688, 0.4895973205566406, -0.29052734375, -4.326194763183594, -0.4878692626953125, -0.6947212219238281, -0.2767333984375, 0.14595794677734375, -1.9567184448242188, -0.7813720703125, -0.3379058837890625, -0.070709228515625, -0.4539756774902344, -0.05118560791015625, -1.5029525756835938, -0.2925872802734375, -0.0711212158203125, -0.6503334045410156, -8.46820068359375, -0.3182525634765625, -0.5467605590820312, -0.5982589721679688, -0.5386924743652344, 0.20209884643554688, 0.046703338623046875, -0.19514083862304688, 0.9551620483398438, -0.4639015197753906, -1.1452522277832031, -12.875160217285156, 35.44430923461914, -5.753425598144531, -0.7073440551757812, -0.3312492370605469, 0.2781181335449219, -0.13373565673828125, 0.14350128173828125, -0.6675262451171875, -0.35784912109375, -0.7977371215820312, -0.5600738525390625, -0.3332061767578125, -0.33658599853515625, -0.6411895751953125, 0.14339828491210938, -0.45662689208984375, -6.232215881347656, -0.5393028259277344, 37.17491912841797, 0.3957023620605469, -0.3565711975097656, 0.38228607177734375, -0.2366180419921875, -0.3737449645996094, -1.7728805541992188, -0.5562057495117188, -0.8796653747558594, -0.5863838195800781, -0.6966781616210938, 0.3573760986328125, -1.1827964782714844, -1.00140380859375, -0.553253173828125, -0.263641357421875, 0.18177032470703125, -0.6470565795898438, -5.8173980712890625, -0.5031318664550781, 0.4844207763671875, -1.0195121765136719, -0.6143226623535156, -0.72216796875, -0.36293792724609375, -0.3465690612792969, 0.20900726318359375, 0.19019699096679688, 0.09790420532226562, -0.6444587707519531, -7.3916168212890625, -0.7919960021972656, -0.3667945861816406, 0.20179367065429688, 0.9846839904785156, 0.13856887817382812, -0.8479995727539062, 0.08941650390625, 0.3518486022949219, -0.11376953125, -0.7995033264160156, -0.3842353820800781, 0.2949333190917969, -2.0431137084960938, 0.14830398559570312, -2.438793182373047, -1.1583976745605469, -0.7668533325195312, -0.5647087097167969, -0.056316375732421875, -0.6324653625488281, -0.07991790771484375, -0.7016754150390625, -0.6066665649414062, -0.318603515625, -1.7939834594726562, -0.13994598388671875, -0.8512535095214844, -0.3702201843261719, -0.8434295654296875], "mean_td_error": -0.41117745637893677}}, "num_steps_sampled": 17000, "num_agent_steps_sampled": 34000, "num_steps_trained": 512256, "num_steps_trained_this_iter": 256, "num_agent_steps_trained": 1024512, "last_target_update_ts": 16624, "num_target_updates": 32}, "done": false, "episodes_total": 28, "training_iteration": 17, "trial_id": "e21e6_00000", "experiment_id": "434cd42d33fd4b638f17fc69fa01d74f", "date": "2022-06-14_19-07-44", "timestamp": 1655248064, "time_this_iter_s": 21.178813219070435, "time_total_s": 344.3864395618439, "pid": 2568314, "hostname": "lambda-dual", "node_ip": "10.104.51.19", "config": {"num_workers": 2, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.0005, "train_batch_size": 256, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "cnet", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "1v1env", "observation_space": null, "action_space": null, "env_config": {}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": true, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": null, "evaluation_duration": 10, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"num_workers": 2, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.0005, "train_batch_size": 256, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "cnet", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "1v1env", "observation_space": null, "action_space": null, "env_config": {}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": false, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": null, "evaluation_duration": 10, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"explore": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": true, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": null, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 2, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"policy_01": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]], [[[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]], (3, 64, 64), float32)", "Discrete(7)", {}], "policy_02": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]], [[[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]], (3, 64, 64), float32)", "Discrete(7)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": "<function <lambda> at 0x7f1aa3747b90>", "policies_to_train": ["policy_01"], "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": 100000, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": false, "hiddens": [], "double_q": false, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": true, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": null, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 2, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"policy_01": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]], [[[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]], (3, 64, 64), float32)", "Discrete(7)", {}], "policy_02": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]], [[[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]], (3, 64, 64), float32)", "Discrete(7)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": "<function <lambda> at 0x7f1aa3747b90>", "policies_to_train": ["policy_01"], "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": 100000, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": false, "hiddens": [], "double_q": false, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "time_since_restore": 344.3864395618439, "timesteps_since_restore": 4352, "iterations_since_restore": 17, "warmup_time": 45.46659183502197, "perf": {"cpu_util_percent": 26.88, "ram_util_percent": 10.309999999999999}}
{"episode_reward_max": 441.0, "episode_reward_min": 0.0, "episode_reward_mean": 23.035714285714285, "episode_len_mean": 601.0, "episode_media": {}, "episodes_this_iter": 0, "policy_reward_min": {"policy_01": 0.0, "policy_02": 0.0}, "policy_reward_max": {"policy_01": 204.0, "policy_02": 441.0}, "policy_reward_mean": {"policy_01": 7.285714285714286, "policy_02": 15.75}, "custom_metrics": {}, "hist_stats": {"episode_reward": [0.0, 204.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "episode_lengths": [601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601], "policy_policy_01_reward": [0.0, 204.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "policy_policy_02_reward": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.33787206899187744, "mean_inference_ms": 5.859167038688098, "mean_action_processing_ms": 0.089380034599097, "mean_env_wait_ms": 8.258418028874022, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 2, "timesteps_total": 18000, "timesteps_this_iter": 256, "agent_timesteps_total": 36000, "timers": {"load_time_ms": 1.336, "load_throughput": 191674.579, "learn_time_ms": 12.186, "learn_throughput": 21007.175, "update_time_ms": 2.382}, "info": {"learner": {"policy_01": {"custom_metrics": {}, "learner_stats": {"mean_q": 44.63896942138672, "min_q": 24.11124038696289, "max_q": 165.64111328125, "cur_lr": 0.0005}, "model": {}, "td_error": [12.516786575317383, -0.7495994567871094, -7.068756103515625, 9.882308959960938, -0.55633544921875, 0.180999755859375, 0.4349250793457031, 0.11738967895507812, 0.053012847900390625, -2.474384307861328, 0.9307670593261719, 0.1807861328125, 0.5490303039550781, -0.3531074523925781, 0.7245674133300781, 0.4982566833496094, 0.39208221435546875, -0.06829452514648438, 0.8067436218261719, -0.07862091064453125, 36.9744758605957, -4.239917755126953, 0.6479225158691406, 2.9711761474609375, 0.6935806274414062, 1.1308403015136719, 9.882308959960938, 1.4717521667480469, 0.34133148193359375, 0.27130889892578125, 0.42378997802734375, -0.044727325439453125, 0.5369186401367188, 0.6400527954101562, 0.3322906494140625, 0.28926849365234375, -3.951873779296875, -0.2627716064453125, -1.2538566589355469, 37.56703567504883, 0.5347824096679688, -3.4390792846679688, 0.16489028930664062, 1.139617919921875, 0.49269866943359375, -2.4017257690429688, 0.38744354248046875, 0.0825653076171875, 0.5120925903320312, -0.4106330871582031, -6.272758483886719, -0.12584304809570312, -0.47271728515625, 0.4846458435058594, 37.16197204589844, 0.6674690246582031, -0.13055419921875, 0.2893333435058594, 0.10655975341796875, 8.618606567382812, 36.13859558105469, -7.481315612792969, -0.1296539306640625, -0.076507568359375, -0.0153045654296875, 0.13530349731445312, 0.5120353698730469, 0.26299285888671875, 0.5051078796386719, 0.34418487548828125, 0.6046180725097656, 0.4434471130371094, 0.1149749755859375, 0.5901718139648438, 0.006866455078125, 0.4688377380371094, -0.2752685546875, -0.210693359375, 0.38707733154296875, 0.34152984619140625, 0.18085098266601562, -0.6378898620605469, 0.3852195739746094, -0.3531074523925781, 0.3904914855957031, 1.8284378051757812, 0.5664558410644531, -0.5449180603027344, 0.6074752807617188, 0.4067573547363281, 0.7158164978027344, 0.2901611328125, 14.792205810546875, 0.4970054626464844, 0.7597084045410156, 0.4016227722167969, 0.097686767578125, 0.263885498046875, 0.7922897338867188, 0.3276824951171875, -0.050380706787109375, 0.6541290283203125, 0.49224853515625, 0.46895599365234375, 0.4266777038574219, 0.6062774658203125, 0.10953521728515625, 0.44583892822265625, -0.833953857421875, 0.26299285888671875, 0.7444839477539062, -0.26209259033203125, 0.05219268798828125, 0.45798492431640625, 36.039302825927734, 0.7619400024414062, 0.3782196044921875, 11.020965576171875, 0.5255088806152344, 0.3284797668457031, 0.6189842224121094, 0.2657432556152344, 0.39171600341796875, 0.5579566955566406, 0.7547798156738281, 0.4856414794921875, 0.6798820495605469, -0.29206085205078125, 0.16414642333984375, 0.17097091674804688, 0.5175132751464844, 0.40106201171875, -0.3246040344238281, -0.8928070068359375, 0.5712890625, 0.314117431640625, -0.040485382080078125, -9.287010192871094, 36.039302825927734, -0.7637481689453125, 0.17633438110351562, -0.23311996459960938, 0.5470123291015625, 0.5000381469726562, -0.639678955078125, -0.6753768920898438, -0.6238746643066406, 0.50286865234375, 1.5741081237792969, 0.17246246337890625, 36.04347229003906, -0.3850555419921875, 0.2787361145019531, -0.4089469909667969, 0.5832481384277344, 0.5641403198242188, 0.3410987854003906, 0.6888351440429688, 2.8184242248535156, -0.08351516723632812, 1.4109954833984375, 1.7760391235351562, -0.3801231384277344, -0.14965438842773438, 0.5754852294921875, -3.334716796875, 0.033145904541015625, 12.001068115234375, 6.0860748291015625, 0.18372726440429688, 0.6455841064453125, 0.5769004821777344, 0.1725006103515625, 0.5742416381835938, 1.3426856994628906, 20.704322814941406, 0.29409027099609375, -0.3790740966796875, -2.1041908264160156, 0.29416656494140625, 0.3824577331542969, 0.3761405944824219, 0.5919342041015625, 1.4116477966308594, 1.0752182006835938, 0.20853805541992188, 0.44207000732421875, 0.2737388610839844, 0.11024093627929688, 0.7097969055175781, 0.5562705993652344, 1.22454833984375, 0.4587974548339844, 7.044109344482422, 0.7196884155273438, -0.3590354919433594, 0.617218017578125, 0.6357078552246094, -0.1584320068359375, -0.2805824279785156, -3.1561241149902344, 0.12903213500976562, 0.1296234130859375, -0.11977767944335938, 0.4208221435546875, 0.7749862670898438, 0.6181411743164062, -2.3971328735351562, 0.3398284912109375, 0.29839324951171875, 0.25272369384765625, 0.18202590942382812, 0.09110260009765625, 5.660030364990234, 25.15502166748047, 37.56703567504883, -2.6034202575683594, 0.2695808410644531, 0.5938682556152344, -0.4094429016113281, -0.14942550659179688, 0.4414253234863281, 0.4926338195800781, 36.98344039916992, 0.6885414123535156, 1.9448661804199219, -0.20177841186523438, 14.548324584960938, -0.41825103759765625, 0.5379066467285156, 1.0317649841308594, 0.625885009765625, 0.3567848205566406, 0.5049591064453125, 0.4727592468261719, 0.6402854919433594, 0.4761543273925781, 0.6423416137695312, 0.20853805541992188, 0.1723480224609375, 0.39093017578125, 0.8376312255859375, 0.11067581176757812, 0.3046722412109375, 0.3990135192871094, -0.1014862060546875, 1.2384223937988281, 6.453605651855469, -5.994346618652344, 2.6444473266601562, -0.13634872436523438, 0.19481277465820312, 0.3134422302246094, 37.68653869628906, 0.42647552490234375, 1.2257728576660156], "mean_td_error": 2.1211719512939453}}, "num_steps_sampled": 18000, "num_agent_steps_sampled": 36000, "num_steps_trained": 544256, "num_steps_trained_this_iter": 256, "num_agent_steps_trained": 1088512, "last_target_update_ts": 17632, "num_target_updates": 34}, "done": false, "episodes_total": 28, "training_iteration": 18, "trial_id": "e21e6_00000", "experiment_id": "434cd42d33fd4b638f17fc69fa01d74f", "date": "2022-06-14_19-08-05", "timestamp": 1655248085, "time_this_iter_s": 21.33636164665222, "time_total_s": 365.7228012084961, "pid": 2568314, "hostname": "lambda-dual", "node_ip": "10.104.51.19", "config": {"num_workers": 2, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.0005, "train_batch_size": 256, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "cnet", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "1v1env", "observation_space": null, "action_space": null, "env_config": {}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": true, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": null, "evaluation_duration": 10, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"num_workers": 2, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.0005, "train_batch_size": 256, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "cnet", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "1v1env", "observation_space": null, "action_space": null, "env_config": {}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": false, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": null, "evaluation_duration": 10, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"explore": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": true, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": null, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 2, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"policy_01": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]], [[[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]], (3, 64, 64), float32)", "Discrete(7)", {}], "policy_02": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]], [[[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]], (3, 64, 64), float32)", "Discrete(7)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": "<function <lambda> at 0x7f1aa2ecbb00>", "policies_to_train": ["policy_01"], "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": 100000, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": false, "hiddens": [], "double_q": false, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": true, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": null, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 2, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"policy_01": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]], [[[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]], (3, 64, 64), float32)", "Discrete(7)", {}], "policy_02": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]], [[[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]], (3, 64, 64), float32)", "Discrete(7)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": "<function <lambda> at 0x7f1aa2ecbb00>", "policies_to_train": ["policy_01"], "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": 100000, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": false, "hiddens": [], "double_q": false, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "time_since_restore": 365.7228012084961, "timesteps_since_restore": 4608, "iterations_since_restore": 18, "warmup_time": 45.46659183502197, "perf": {"cpu_util_percent": 26.879999999999995, "ram_util_percent": 10.5}}
{"episode_reward_max": 441.0, "episode_reward_min": 0.0, "episode_reward_mean": 36.2, "episode_len_mean": 601.0, "episode_media": {}, "episodes_this_iter": 2, "policy_reward_min": {"policy_01": 0.0, "policy_02": 0.0}, "policy_reward_max": {"policy_01": 441.0, "policy_02": 441.0}, "policy_reward_mean": {"policy_01": 21.5, "policy_02": 14.7}, "custom_metrics": {}, "hist_stats": {"episode_reward": [0.0, 204.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0], "episode_lengths": [601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601], "policy_policy_01_reward": [0.0, 204.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0], "policy_policy_02_reward": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.3377345339224441, "mean_inference_ms": 5.855885326980638, "mean_action_processing_ms": 0.08935107148851283, "mean_env_wait_ms": 8.238347653510768, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 2, "timesteps_total": 19000, "timesteps_this_iter": 256, "agent_timesteps_total": 38000, "timers": {"load_time_ms": 1.305, "load_throughput": 196142.306, "learn_time_ms": 12.025, "learn_throughput": 21288.941, "update_time_ms": 2.235}, "info": {"learner": {"policy_01": {"custom_metrics": {}, "learner_stats": {"mean_q": 42.767608642578125, "min_q": 33.475120544433594, "max_q": 183.82186889648438, "cur_lr": 0.0005}, "model": {}, "td_error": [-1.0420188903808594, -10.593002319335938, -0.7619972229003906, -0.28253936767578125, -1.743194580078125, -0.3884239196777344, -0.767364501953125, -2.0389442443847656, -0.5482864379882812, -0.3676490783691406, -1.3791847229003906, -1.7194023132324219, -0.5547752380371094, -0.6788177490234375, -0.8788528442382812, -1.3116645812988281, -6.2844390869140625, -0.9743309020996094, -0.3975830078125, -0.7094802856445312, -0.7722816467285156, -1.5711898803710938, -0.1558685302734375, -1.5226631164550781, 5.8990631103515625, -0.7058448791503906, -0.9454765319824219, -0.6363258361816406, -0.5288047790527344, 0.42407989501953125, -32.244667053222656, -0.32396697998046875, -0.5690231323242188, -0.9836540222167969, -0.8902778625488281, -0.620880126953125, -1.2195167541503906, -1.4899978637695312, -1.5621528625488281, 36.43440628051758, -1.8130111694335938, -1.0778121948242188, -6.337528228759766, -0.9018096923828125, -0.5119781494140625, -1.1161537170410156, -0.5094261169433594, -1.1504707336425781, -1.2886543273925781, -0.7340049743652344, -1.0168037414550781, -0.5573692321777344, -0.604736328125, -0.7148056030273438, -0.5445404052734375, -1.675811767578125, -1.8967399597167969, 35.785404205322266, -0.3424263000488281, -0.9049758911132812, -11.57318115234375, 4.223846435546875, -1.7107582092285156, -1.689605712890625, -1.0185050964355469, 1.2010993957519531, -0.22628021240234375, -1.2409934997558594, -0.7162055969238281, -0.8144950866699219, -0.5353851318359375, -0.56982421875, -0.5572090148925781, -32.244667053222656, -0.4993095397949219, -1.4765434265136719, -1.4736557006835938, -1.1703109741210938, -0.946258544921875, 1.0232200622558594, -0.9504432678222656, -1.0874900817871094, -1.6225090026855469, -0.680572509765625, -0.50408935546875, -0.21413803100585938, -0.8042449951171875, -0.6891326904296875, -1.2627220153808594, -0.42119598388671875, -2.2170867919921875, -0.7465858459472656, -1.4883651733398438, -0.43944549560546875, -0.46852874755859375, -1.8386154174804688, -0.4659385681152344, -0.98046875, -1.71319580078125, -0.5480499267578125, -0.3899040222167969, -5.229591369628906, -0.3537940979003906, -0.5772171020507812, -0.5374412536621094, -0.6764259338378906, -1.6656532287597656, -6.167610168457031, 0.35131072998046875, -0.4737358093261719, -2.1558837890625, -1.4942131042480469, 2.5286941528320312, -0.9902687072753906, -0.69512939453125, -0.4133033752441406, -0.7689208984375, -1.2686119079589844, -0.9686355590820312, -0.5590934753417969, -0.6926155090332031, -0.6745758056640625, -0.3702888488769531, -0.1910552978515625, -1.4150466918945312, -0.6206321716308594, -1.8393745422363281, -0.4189605712890625, -0.4627685546875, -0.7774505615234375, -1.5824928283691406, -0.5291671752929688, -0.5245132446289062, -0.4668312072753906, -1.2244644165039062, -1.7990760803222656, -0.4159278869628906, -1.3366050720214844, -0.5146102905273438, -2.5602455139160156, -0.6979713439941406, -0.8266029357910156, -1.2886543273925781, -1.4497222900390625, -5.4862518310546875, -1.1824264526367188, -2.0389442443847656, -0.4694328308105469, -0.5885772705078125, -1.1314659118652344, -0.9923858642578125, -0.7986488342285156, -0.6421585083007812, -0.884918212890625, -0.9818572998046875, -0.6763839721679688, -1.1653709411621094, -2.669544219970703, -1.0995140075683594, -0.9198570251464844, -0.6805419921875, -1.0203590393066406, -0.620361328125, -0.8273582458496094, -0.4064140319824219, -0.6320991516113281, 37.78547668457031, -1.7815017700195312, -0.5791053771972656, -0.3627128601074219, -1.01239013671875, -1.7507286071777344, -0.9432868957519531, -0.4983863830566406, -0.6328239440917969, -0.44652557373046875, -0.9280624389648438, -1.6012535095214844, -0.43944549560546875, 36.22069549560547, -0.2436981201171875, -1.0608978271484375, -1.833648681640625, -1.4356765747070312, -0.12646484375, -0.4432334899902344, -0.5642433166503906, -1.3082237243652344, -1.2856063842773438, -3.0899505615234375, -0.8199501037597656, -0.09360885620117188, -0.3473625183105469, -2.1268577575683594, -0.9686355590820312, -0.8401870727539062, 0.20500946044921875, 36.43440628051758, -1.7438468933105469, -1.9329566955566406, -0.9251136779785156, -0.7607498168945312, -1.5866813659667969, -0.6206321716308594, 0.07143402099609375, 0.7979545593261719, 36.243553161621094, -0.39093780517578125, -0.3575325012207031, -2.9679527282714844, -0.40966796875, -1.5491218566894531, -1.6032752990722656, -0.4220428466796875, -0.9919967651367188, -0.4884681701660156, -0.2615547180175781, 0.023693084716796875, -0.3880615234375, -0.2973518371582031, -0.35578155517578125, -0.5972251892089844, -1.2582435607910156, -0.9703559875488281, 0.7352790832519531, -1.6823387145996094, -0.73150634765625, 2.527313232421875, -0.666351318359375, -0.6813774108886719, -1.8667335510253906, -0.8882560729980469, 2.0361328125, -0.65863037109375, -0.3360023498535156, -0.7237739562988281, -1.695587158203125, -8.114608764648438, -1.9395828247070312, 37.83005905151367, -0.617401123046875, -1.7723846435546875, -5.4633331298828125, -0.3667411804199219, -0.6258468627929688, -0.5867156982421875, -0.5945205688476562, 1.59100341796875, -0.5070419311523438, -1.9500198364257812, -0.38257598876953125, -0.6226997375488281, 35.454139709472656, -1.1681480407714844, 23.284942626953125, -0.17091751098632812], "mean_td_error": 0.0015841573476791382}}, "num_steps_sampled": 19000, "num_agent_steps_sampled": 38000, "num_steps_trained": 576256, "num_steps_trained_this_iter": 256, "num_agent_steps_trained": 1152512, "last_target_update_ts": 18640, "num_target_updates": 36}, "done": false, "episodes_total": 30, "training_iteration": 19, "trial_id": "e21e6_00000", "experiment_id": "434cd42d33fd4b638f17fc69fa01d74f", "date": "2022-06-14_19-08-26", "timestamp": 1655248106, "time_this_iter_s": 21.18567991256714, "time_total_s": 386.90848112106323, "pid": 2568314, "hostname": "lambda-dual", "node_ip": "10.104.51.19", "config": {"num_workers": 2, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.0005, "train_batch_size": 256, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "cnet", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "1v1env", "observation_space": null, "action_space": null, "env_config": {}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": true, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": null, "evaluation_duration": 10, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"num_workers": 2, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.0005, "train_batch_size": 256, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "cnet", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "1v1env", "observation_space": null, "action_space": null, "env_config": {}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": false, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": null, "evaluation_duration": 10, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"explore": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": true, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": null, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 2, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"policy_01": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]], [[[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]], (3, 64, 64), float32)", "Discrete(7)", {}], "policy_02": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]], [[[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]], (3, 64, 64), float32)", "Discrete(7)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": "<function <lambda> at 0x7f1aa3f83560>", "policies_to_train": ["policy_01"], "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": 100000, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": false, "hiddens": [], "double_q": false, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": true, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": null, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 2, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"policy_01": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]], [[[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]], (3, 64, 64), float32)", "Discrete(7)", {}], "policy_02": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]], [[[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]], (3, 64, 64), float32)", "Discrete(7)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": "<function <lambda> at 0x7f1aa3f83560>", "policies_to_train": ["policy_01"], "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": 100000, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": false, "hiddens": [], "double_q": false, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "time_since_restore": 386.90848112106323, "timesteps_since_restore": 4864, "iterations_since_restore": 19, "warmup_time": 45.46659183502197, "perf": {"cpu_util_percent": 25.403225806451612, "ram_util_percent": 10.59354838709678}}
{"episode_reward_max": 441.0, "episode_reward_min": 0.0, "episode_reward_mean": 33.9375, "episode_len_mean": 601.0, "episode_media": {}, "episodes_this_iter": 2, "policy_reward_min": {"policy_01": 0.0, "policy_02": 0.0}, "policy_reward_max": {"policy_01": 441.0, "policy_02": 441.0}, "policy_reward_mean": {"policy_01": 20.15625, "policy_02": 13.78125}, "custom_metrics": {}, "hist_stats": {"episode_reward": [0.0, 204.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0], "episode_lengths": [601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601], "policy_policy_01_reward": [0.0, 204.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0], "policy_policy_02_reward": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.3376582146372652, "mean_inference_ms": 5.852817684527008, "mean_action_processing_ms": 0.08932217755282812, "mean_env_wait_ms": 8.22035055846976, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 2, "timesteps_total": 20000, "timesteps_this_iter": 256, "agent_timesteps_total": 40000, "timers": {"load_time_ms": 1.307, "load_throughput": 195841.798, "learn_time_ms": 11.848, "learn_throughput": 21606.593, "update_time_ms": 2.224}, "info": {"learner": {"policy_01": {"custom_metrics": {}, "learner_stats": {"mean_q": 42.66335678100586, "min_q": 35.679107666015625, "max_q": 168.9644012451172, "cur_lr": 0.0005}, "model": {}, "td_error": [-0.1305084228515625, 2.0687179565429688, -0.5618324279785156, -0.13349151611328125, -0.17890167236328125, 39.15718460083008, -0.7588920593261719, -0.45859527587890625, -0.5742568969726562, 0.4159049987792969, -0.23764801025390625, -0.08337783813476562, -1.8739776611328125, -0.26535797119140625, -0.028171539306640625, -0.13075637817382812, -0.1414337158203125, 0.3376655578613281, 15.953071594238281, -0.3157844543457031, 0.44318389892578125, -0.41201019287109375, -0.3355712890625, -0.3523406982421875, -0.5014991760253906, 0.009410858154296875, -0.67352294921875, -0.5529060363769531, 5.282260894775391, -0.20796966552734375, 0.17458724975585938, 0.2213287353515625, -0.4216880798339844, -0.30367279052734375, -0.1128997802734375, -0.3282470703125, -0.4759559631347656, 36.3597412109375, -0.320068359375, -0.0096588134765625, -0.49687957763671875, -0.29123687744140625, 0.04471588134765625, -0.5057106018066406, -0.5417556762695312, -0.0999298095703125, -0.19573974609375, -0.5202445983886719, -0.18703460693359375, -0.3655242919921875, 0.672698974609375, 0.37464141845703125, -0.4275856018066406, -0.07772064208984375, -0.7536468505859375, -0.14322662353515625, 0.8393669128417969, 0.04694366455078125, -0.2939796447753906, 0.017425537109375, -0.3628387451171875, -0.14811325073242188, -2.4118804931640625, -0.12260818481445312, 0.290557861328125, -0.2664146423339844, 0.2508811950683594, -0.13648223876953125, 0.7415199279785156, -0.131591796875, -0.2519683837890625, -0.3919944763183594, -0.13467788696289062, -0.1138458251953125, -0.5643692016601562, -0.13286972045898438, 0.4135551452636719, -0.15865707397460938, 0.21946334838867188, 0.6451187133789062, -0.20506668090820312, 0.11330413818359375, -0.0999298095703125, -0.3157463073730469, 0.23496246337890625, 0.10891342163085938, -0.5222549438476562, -0.5112800598144531, 0.16100311279296875, 0.4585227966308594, -0.20085906982421875, -0.5342979431152344, -0.0775299072265625, 0.2670555114746094, -0.0355377197265625, -0.5236015319824219, -0.3245582580566406, 0.4578742980957031, 0.224609375, 0.12436676025390625, 0.25009918212890625, -0.48935699462890625, -1.0729103088378906, -0.3419837951660156, -0.00365447998046875, -0.13885498046875, 37.789730072021484, 8.350265502929688, 0.60296630859375, -0.08607864379882812, -0.022735595703125, 0.242706298828125, 0.07644271850585938, -0.4095191955566406, -0.5299949645996094, -0.2469940185546875, -0.5788383483886719, -0.10094070434570312, 6.115203857421875, -0.1976318359375, -0.18927383422851562, -0.6629295349121094, -1.1626853942871094, -0.17941665649414062, -3.164897918701172, -0.2867927551269531, -3.0089149475097656, -0.1606597900390625, -0.23373794555664062, 0.20344924926757812, 0.4544181823730469, -0.024898529052734375, -2.0375518798828125, -0.21375274658203125, 0.43144989013671875, -0.1340484619140625, -0.217437744140625, -0.5652275085449219, -0.4952392578125, -0.11402130126953125, -0.24490737915039062, -0.06680679321289062, -0.7808151245117188, 0.15418624877929688, 0.044403076171875, -0.16252899169921875, -0.5528831481933594, -5.421356201171875, -0.23848342895507812, -0.1544189453125, -0.273284912109375, -0.0927886962890625, 0.005462646484375, -0.24490737915039062, 37.27269744873047, 0.0198211669921875, 0.37865447998046875, -0.3923912048339844, 0.13219833374023438, -0.4147911071777344, 38.22271728515625, -0.2939796447753906, -0.5952110290527344, -0.4628868103027344, 36.94799041748047, 0.4570465087890625, -0.7473411560058594, -0.15287399291992188, -4.230308532714844, -0.20579147338867188, -0.022098541259765625, -0.14390182495117188, -0.43122100830078125, 0.8261756896972656, 0.4752769470214844, -0.4069099426269531, 4.6046295166015625, 39.15718460083008, 0.13385009765625, 0.1219635009765625, -0.37078094482421875, -0.2987480163574219, -0.2755393981933594, -0.14176559448242188, -0.2942771911621094, 38.22271728515625, -0.0975341796875, 0.482177734375, 1.559234619140625, 0.6410026550292969, -0.1087188720703125, -0.4960441589355469, -0.373016357421875, 0.13938522338867188, 0.3673248291015625, -0.5323448181152344, -0.09653854370117188, 0.10944747924804688, 0.23666000366210938, -0.752593994140625, -0.4254264831542969, 0.12429046630859375, 2.2238731384277344, -0.5846748352050781, 0.5821647644042969, -0.3428153991699219, 0.47881317138671875, -0.4239463806152344, -0.3534126281738281, -0.047637939453125, -2.1485328674316406, -2.3469314575195312, -3.0051956176757812, -5.967155456542969, 1.3241004943847656, -0.3611335754394531, -0.3377189636230469, -0.8233108520507812, -2.4725341796875, -12.935661315917969, -0.3240776062011719, -0.2133331298828125, -6.459102630615234, -0.5529060363769531, -11.360702514648438, -0.33168792724609375, -0.3047065734863281, -0.7159461975097656, -0.6209869384765625, -3.95458984375, -0.8457069396972656, 0.15970993041992188, 43.40335464477539, -0.5288352966308594, -0.14515304565429688, -0.5153617858886719, -0.391326904296875, 3.0861282348632812, -0.00649261474609375, -0.4821319580078125, -0.3116607666015625, -0.7282485961914062, -0.54974365234375, -0.24203109741210938, 0.09702301025390625, -0.32952880859375, -0.24784469604492188, -0.4078559875488281, -0.4147911071777344, -0.4199104309082031, -0.8190498352050781, -0.3039283752441406, 0.4570465087890625, -0.09644699096679688, -0.20369338989257812, -0.27825164794921875], "mean_td_error": 1.119187355041504}}, "num_steps_sampled": 20000, "num_agent_steps_sampled": 40000, "num_steps_trained": 608256, "num_steps_trained_this_iter": 256, "num_agent_steps_trained": 1216512, "last_target_update_ts": 19648, "num_target_updates": 38}, "done": false, "episodes_total": 32, "training_iteration": 20, "trial_id": "e21e6_00000", "experiment_id": "434cd42d33fd4b638f17fc69fa01d74f", "date": "2022-06-14_19-08-48", "timestamp": 1655248128, "time_this_iter_s": 21.23567843437195, "time_total_s": 408.1441595554352, "pid": 2568314, "hostname": "lambda-dual", "node_ip": "10.104.51.19", "config": {"num_workers": 2, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.0005, "train_batch_size": 256, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "cnet", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "1v1env", "observation_space": null, "action_space": null, "env_config": {}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": true, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": null, "evaluation_duration": 10, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"num_workers": 2, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.0005, "train_batch_size": 256, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "cnet", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "1v1env", "observation_space": null, "action_space": null, "env_config": {}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": false, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": null, "evaluation_duration": 10, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"explore": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": true, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": null, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 2, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"policy_01": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]], [[[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]], (3, 64, 64), float32)", "Discrete(7)", {}], "policy_02": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]], [[[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]], (3, 64, 64), float32)", "Discrete(7)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": "<function <lambda> at 0x7f1aa2e9acb0>", "policies_to_train": ["policy_01"], "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": 100000, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": false, "hiddens": [], "double_q": false, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": true, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": null, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 2, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"policy_01": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]], [[[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]], (3, 64, 64), float32)", "Discrete(7)", {}], "policy_02": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]], [[[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]], (3, 64, 64), float32)", "Discrete(7)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": "<function <lambda> at 0x7f1aa2e9acb0>", "policies_to_train": ["policy_01"], "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": 100000, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": false, "hiddens": [], "double_q": false, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "time_since_restore": 408.1441595554352, "timesteps_since_restore": 5120, "iterations_since_restore": 20, "warmup_time": 45.46659183502197, "perf": {"cpu_util_percent": 26.07666666666667, "ram_util_percent": 10.656666666666665}}
{"episode_reward_max": 441.0, "episode_reward_min": 0.0, "episode_reward_mean": 31.941176470588236, "episode_len_mean": 601.0, "episode_media": {}, "episodes_this_iter": 2, "policy_reward_min": {"policy_01": 0.0, "policy_02": 0.0}, "policy_reward_max": {"policy_01": 441.0, "policy_02": 441.0}, "policy_reward_mean": {"policy_01": 18.970588235294116, "policy_02": 12.970588235294118}, "custom_metrics": {}, "hist_stats": {"episode_reward": [0.0, 204.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0], "episode_lengths": [601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601], "policy_policy_01_reward": [0.0, 204.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0], "policy_policy_02_reward": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.3376065577313862, "mean_inference_ms": 5.849781573411468, "mean_action_processing_ms": 0.08929342514904291, "mean_env_wait_ms": 8.203978666946066, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 2, "timesteps_total": 21000, "timesteps_this_iter": 256, "agent_timesteps_total": 42000, "timers": {"load_time_ms": 1.336, "load_throughput": 191606.171, "learn_time_ms": 11.582, "learn_throughput": 22103.595, "update_time_ms": 2.028}, "info": {"learner": {"policy_01": {"custom_metrics": {}, "learner_stats": {"mean_q": 45.220436096191406, "min_q": 35.21018981933594, "max_q": 208.37420654296875, "cur_lr": 0.0005}, "model": {}, "td_error": [-0.058330535888671875, -0.12908554077148438, -2.3066444396972656, 0.12189483642578125, -0.023166656494140625, -0.0523529052734375, 0.228973388671875, 0.250152587890625, 4.497158050537109, 0.07776260375976562, 0.20281219482421875, 0.5300941467285156, 0.07565689086914062, 0.4482841491699219, 0.25897216796875, 39.10979461669922, -0.17330169677734375, 0.37796783447265625, -0.11681365966796875, -0.07234573364257812, 0.12620925903320312, 0.16158676147460938, -0.14637374877929688, -0.11704635620117188, -0.12525177001953125, 0.5491676330566406, 0.3447074890136719, 0.7343902587890625, 0.4018440246582031, 0.2765388488769531, -0.001682281494140625, 0.3889961242675781, 1.1028976440429688, 0.4733390808105469, 0.14213180541992188, 0.4030303955078125, 0.1888885498046875, 1.1484603881835938, 0.14638900756835938, 0.7495880126953125, 0.08034896850585938, -0.1739501953125, -0.14393997192382812, -0.18204116821289062, 0.37131500244140625, 0.6447982788085938, 0.2804985046386719, 2.2686309814453125, 0.13359832763671875, 0.2542381286621094, 2.404552459716797, 0.6305160522460938, 0.3991279602050781, 0.6955490112304688, 0.006687164306640625, 0.5604057312011719, 2.8494949340820312, 0.2030792236328125, 0.23378372192382812, -0.4452018737792969, -0.7159233093261719, 0.310516357421875, 20.54656982421875, 0.15232467651367188, 1.1484603881835938, 9.080284118652344, -0.3108711242675781, 0.30916595458984375, 0.3330802917480469, -2.266143798828125, -0.07304763793945312, 0.11397552490234375, -0.3311920166015625, -1.6463050842285156, 0.023326873779296875, 0.17987442016601562, 0.16191864013671875, -0.243896484375, 0.16231155395507812, 0.5041580200195312, 0.4089393615722656, -0.7559890747070312, 1.0066146850585938, 0.304168701171875, -0.024700164794921875, 1.1081390380859375, 0.32994842529296875, 0.6895942687988281, 0.09039306640625, 0.46155548095703125, -55.034912109375, 0.545318603515625, 0.23093795776367188, 0.22269821166992188, 10.939895629882812, 0.12927627563476562, -1.2017898559570312, 0.5299911499023438, 0.08829498291015625, -1.1354141235351562, 0.4138069152832031, 0.2584266662597656, 1.1262397766113281, 0.6744003295898438, -0.6116065979003906, -3.782501220703125, 0.7645721435546875, -0.3024482727050781, 0.2969627380371094, 0.1929779052734375, -0.11974716186523438, 0.6067886352539062, 0.946929931640625, 0.13671493530273438, 0.8415336608886719, 1.3673515319824219, 0.10336685180664062, 0.3854713439941406, 0.14664459228515625, 0.011379241943359375, 0.20731353759765625, 0.9500923156738281, 0.13458633422851562, -4.141777038574219, -0.22593307495117188, 0.22000503540039062, 0.22259140014648438, 1.4840774536132812, 0.5079116821289062, 0.027027130126953125, 0.17751312255859375, 0.07877731323242188, -0.6284675598144531, 1.3501129150390625, 0.3377838134765625, 42.65228271484375, 0.12176513671875, 0.8683052062988281, -0.18557357788085938, 0.6115913391113281, 0.3787651062011719, -0.5522117614746094, 0.16330718994140625, 0.415435791015625, -0.44379425048828125, -0.16602706909179688, 1.0425682067871094, -0.6692085266113281, 3.43524169921875, 0.06663894653320312, -0.13704299926757812, 0.02570343017578125, 0.2579498291015625, -0.032825469970703125, 6.723716735839844, -0.040859222412109375, 0.2476806640625, 0.3861083984375, -0.6562271118164062, 0.3808555603027344, 0.6221656799316406, -0.16640853881835938, 0.3016929626464844, 0.033000946044921875, -0.17583084106445312, -0.32790374755859375, 0.056797027587890625, -0.00685882568359375, 0.33347320556640625, 0.214935302734375, 0.170867919921875, 0.2678375244140625, -3.5971450805664062, 0.81982421875, 0.18550491333007812, -0.09776687622070312, 0.5506744384765625, 0.4496498107910156, 5.108795166015625, 37.27927780151367, -0.012256622314453125, -0.24034500122070312, 0.21346282958984375, -0.01287078857421875, 0.5936203002929688, -9.001754760742188, -0.07175445556640625, -0.173309326171875, -0.7995796203613281, -0.9602432250976562, 0.5442924499511719, -0.108245849609375, 0.080108642578125, -3.1351776123046875, -0.20915603637695312, 0.9068527221679688, 0.44033050537109375, 1.0791854858398438, 0.6818656921386719, 6.629402160644531, 1.0183486938476562, 0.10833740234375, 0.1526947021484375, 0.6688461303710938, 10.939895629882812, 0.13149261474609375, -0.0481719970703125, 36.99241638183594, 0.5329322814941406, 0.5663185119628906, 0.3035697937011719, 0.3839759826660156, -0.4382476806640625, 1.1498489379882812, 0.3245429992675781, 37.2375373840332, 0.1526336669921875, 0.0408172607421875, 0.28002166748046875, 0.6403388977050781, 0.571533203125, 0.4523735046386719, -0.10445404052734375, 0.20456695556640625, 0.015422821044921875, 8.189323425292969, 0.23290634155273438, 0.5493545532226562, 0.061923980712890625, 0.3596305847167969, 0.683563232421875, 0.17333221435546875, -1.04986572265625, 0.218536376953125, 0.5393180847167969, 1.1837310791015625, -3.5666351318359375, 1.036285400390625, 0.11998748779296875, 0.3707313537597656, -0.3411750793457031, 0.24410247802734375, 0.2507972717285156, 0.336761474609375, -0.3622169494628906, 0.3686370849609375, -0.042724609375, 0.4713249206542969, -0.0395660400390625, -0.3152427673339844, 37.72574234008789, 0.6080551147460938, 0.5917243957519531, 0.7598876953125, 0.3311119079589844, -0.2660484313964844], "mean_td_error": 1.1205439567565918}}, "num_steps_sampled": 21000, "num_agent_steps_sampled": 42000, "num_steps_trained": 640256, "num_steps_trained_this_iter": 256, "num_agent_steps_trained": 1280512, "last_target_update_ts": 20656, "num_target_updates": 40}, "done": false, "episodes_total": 34, "training_iteration": 21, "trial_id": "e21e6_00000", "experiment_id": "434cd42d33fd4b638f17fc69fa01d74f", "date": "2022-06-14_19-09-09", "timestamp": 1655248149, "time_this_iter_s": 21.177890062332153, "time_total_s": 429.32204961776733, "pid": 2568314, "hostname": "lambda-dual", "node_ip": "10.104.51.19", "config": {"num_workers": 2, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.0005, "train_batch_size": 256, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "cnet", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "1v1env", "observation_space": null, "action_space": null, "env_config": {}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": true, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": null, "evaluation_duration": 10, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"num_workers": 2, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.0005, "train_batch_size": 256, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "cnet", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "1v1env", "observation_space": null, "action_space": null, "env_config": {}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": false, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": null, "evaluation_duration": 10, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"explore": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": true, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": null, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 2, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"policy_01": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]], [[[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]], (3, 64, 64), float32)", "Discrete(7)", {}], "policy_02": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]], [[[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]], (3, 64, 64), float32)", "Discrete(7)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": "<function <lambda> at 0x7f1aa2e9aef0>", "policies_to_train": ["policy_01"], "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": 100000, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": false, "hiddens": [], "double_q": false, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": true, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": null, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 2, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"policy_01": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]], [[[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]], (3, 64, 64), float32)", "Discrete(7)", {}], "policy_02": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]], [[[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]], (3, 64, 64), float32)", "Discrete(7)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": "<function <lambda> at 0x7f1aa2e9aef0>", "policies_to_train": ["policy_01"], "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": 100000, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": false, "hiddens": [], "double_q": false, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "time_since_restore": 429.32204961776733, "timesteps_since_restore": 5376, "iterations_since_restore": 21, "warmup_time": 45.46659183502197, "perf": {"cpu_util_percent": 25.833333333333336, "ram_util_percent": 10.733333333333333}}
{"episode_reward_max": 441.0, "episode_reward_min": 0.0, "episode_reward_mean": 42.416666666666664, "episode_len_mean": 601.0, "episode_media": {}, "episodes_this_iter": 2, "policy_reward_min": {"policy_01": 0.0, "policy_02": 0.0}, "policy_reward_max": {"policy_01": 441.0, "policy_02": 441.0}, "policy_reward_mean": {"policy_01": 30.166666666666668, "policy_02": 12.25}, "custom_metrics": {}, "hist_stats": {"episode_reward": [0.0, 204.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0], "episode_lengths": [601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601], "policy_policy_01_reward": [0.0, 204.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0], "policy_policy_02_reward": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.3375590127008661, "mean_inference_ms": 5.846732768903674, "mean_action_processing_ms": 0.08926548696186348, "mean_env_wait_ms": 8.188482763978381, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 2, "timesteps_total": 22000, "timesteps_this_iter": 256, "agent_timesteps_total": 44000, "timers": {"load_time_ms": 1.352, "load_throughput": 189288.995, "learn_time_ms": 12.04, "learn_throughput": 21262.762, "update_time_ms": 2.324}, "info": {"learner": {"policy_01": {"custom_metrics": {}, "learner_stats": {"mean_q": 45.782676696777344, "min_q": 36.418540954589844, "max_q": 205.71710205078125, "cur_lr": 0.0005}, "model": {}, "td_error": [1.6958160400390625, 1.0814132690429688, -2.7460250854492188, 0.8337440490722656, 1.3978157043457031, 0.8878326416015625, 1.5361099243164062, -0.17658233642578125, 1.285369873046875, 1.0100440979003906, -5.178478240966797, 1.1617431640625, 1.0499153137207031, 1.8765411376953125, 1.4966545104980469, -0.4008522033691406, 1.1903419494628906, 1.2083740234375, 1.4355888366699219, 0.9133262634277344, -0.14489364624023438, 1.2038002014160156, 1.0775527954101562, 2.09466552734375, 1.0935935974121094, 0.5106201171875, 1.5150032043457031, 2.455913543701172, 0.6184883117675781, 0.9519119262695312, -2.8333206176757812, 2.3565711975097656, -4.1457061767578125, 1.4184761047363281, -45.9378662109375, 1.8461227416992188, 1.1198768615722656, 0.9179534912109375, 1.0806999206542969, -6.012351989746094, 1.13751220703125, 1.6992721557617188, 3.4529037475585938, 0.7351417541503906, 1.1744003295898438, -0.1709747314453125, 1.33917236328125, 1.3437957763671875, 1.2525062561035156, 1.0453376770019531, 4.536415100097656, 0.7845649719238281, 0.7394828796386719, -0.5050048828125, 37.82490921020508, 1.3574409484863281, 0.9155120849609375, 1.2490921020507812, 36.60173797607422, 0.9033393859863281, 1.2566337585449219, 0.6790962219238281, 1.1893501281738281, 1.5912399291992188, 1.3727607727050781, 0.8878326416015625, 5.585849761962891, 1.5947647094726562, 0.9476661682128906, 1.2632637023925781, 0.7966804504394531, -1.3543243408203125, 0.528106689453125, 1.6830635070800781, 1.1551094055175781, 1.846405029296875, 1.5076370239257812, 1.1964378356933594, 6.5059356689453125, 0.6153411865234375, 0.6329193115234375, 1.0497245788574219, 1.1781082153320312, 1.2075386047363281, 2.5078392028808594, 3.600322723388672, 1.0565071105957031, 0.8040618896484375, 1.4310264587402344, 2.58001708984375, 1.5011177062988281, 0.9037818908691406, -1.476898193359375, 1.1505317687988281, 1.1685409545898438, 36.883968353271484, 1.2188796997070312, 1.4137802124023438, 0.9280319213867188, 0.5689849853515625, 1.5862045288085938, 1.3802375793457031, 1.0716361999511719, 1.0565071105957031, 1.2499427795410156, 1.1412315368652344, 1.1228675842285156, 2.0337486267089844, 0.9041824340820312, 0.7308082580566406, 0.017910003662109375, 36.94474792480469, 1.2501602172851562, 0.5440216064453125, 0.6394767761230469, 1.1153640747070312, 1.2770767211914062, 0.6320762634277344, 1.7043228149414062, 1.0551643371582031, -3.8161392211914062, 1.844146728515625, 1.2846107482910156, 1.6922378540039062, 0.3981819152832031, 0.37752532958984375, 1.8302040100097656, 0.809722900390625, 0.7702255249023438, 1.0348777770996094, 1.5785140991210938, 1.082855224609375, -1.1792449951171875, 1.0277290344238281, 0.8165740966796875, 1.0313262939453125, 0.8304519653320312, 1.4922332763671875, 1.2423782348632812, 1.3190460205078125, 1.8681602478027344, 0.6035385131835938, -1.4189949035644531, 1.310211181640625, 0.9273185729980469, 1.154510498046875, 1.3282203674316406, 1.0450096130371094, 1.1165275573730469, 0.8802680969238281, 1.0136528015136719, -4.8532867431640625, 1.846405029296875, 1.1153640747070312, 1.377288818359375, 0.28572845458984375, 0.9581832885742188, 1.221893310546875, 0.7577857971191406, 0.88104248046875, 0.24897003173828125, 0.69219970703125, 1.6075515747070312, 1.0630035400390625, 0.7581138610839844, 0.5117912292480469, 1.9882469177246094, 0.9393196105957031, 0.7616462707519531, 1.5965194702148438, 1.9349136352539062, 1.3837776184082031, 1.5913314819335938, 1.4718132019042969, 1.3452491760253906, 3.228626251220703, 19.902915954589844, 1.5677490234375, 1.2839927673339844, 1.055145263671875, 0.5384674072265625, 1.37860107421875, 1.1621246337890625, 8.667221069335938, 0.608062744140625, 0.4190940856933594, 1.3095283508300781, 0.9909820556640625, 1.2000541687011719, 2.4378395080566406, -4.620349884033203, 0.9047050476074219, 1.4706039428710938, 1.1563377380371094, 1.6137580871582031, 0.8457984924316406, 0.8885650634765625, 0.8287124633789062, 1.0416603088378906, 0.8121452331542969, 0.3931846618652344, -0.1319732666015625, -1.2530555725097656, 1.3756675720214844, 1.2867317199707031, -0.890899658203125, 1.446685791015625, 1.4643363952636719, 1.2963294982910156, 1.0365524291992188, 0.7099266052246094, 1.2583084106445312, 38.843875885009766, 1.9972267150878906, 0.7115478515625, 4.521080017089844, 1.1309432983398438, 2.148937225341797, 1.36151123046875, 0.4839363098144531, 1.1719894409179688, 0.9684600830078125, 0.9555206298828125, 1.5748100280761719, 0.8547286987304688, 0.693084716796875, 1.0165824890136719, 1.2478561401367188, 1.4462928771972656, 1.6785964965820312, 1.261016845703125, 0.9861068725585938, 1.8528327941894531, 0.9969673156738281, -1.5513267517089844, 2.0276870727539062, 1.8696441650390625, 1.8466949462890625, 0.8877067565917969, 0.9659996032714844, 1.4563026428222656, 2.8388671875, 1.3905105590820312, -0.5743904113769531, 1.0328941345214844, 1.2122993469238281, 1.0974845886230469, 0.5434303283691406, 1.6127243041992188, 19.902915954589844, 0.5790519714355469, 1.7798919677734375, 1.1009597778320312, 0.9632644653320312, 1.2382926940917969, 1.9150924682617188], "mean_td_error": 1.704228401184082}}, "num_steps_sampled": 22000, "num_agent_steps_sampled": 44000, "num_steps_trained": 672256, "num_steps_trained_this_iter": 256, "num_agent_steps_trained": 1344512, "last_target_update_ts": 21664, "num_target_updates": 42}, "done": false, "episodes_total": 36, "training_iteration": 22, "trial_id": "e21e6_00000", "experiment_id": "434cd42d33fd4b638f17fc69fa01d74f", "date": "2022-06-14_19-09-30", "timestamp": 1655248170, "time_this_iter_s": 20.95299196243286, "time_total_s": 450.2750415802002, "pid": 2568314, "hostname": "lambda-dual", "node_ip": "10.104.51.19", "config": {"num_workers": 2, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.0005, "train_batch_size": 256, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "cnet", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "1v1env", "observation_space": null, "action_space": null, "env_config": {}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": true, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": null, "evaluation_duration": 10, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"num_workers": 2, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.0005, "train_batch_size": 256, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "cnet", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "1v1env", "observation_space": null, "action_space": null, "env_config": {}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": false, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": null, "evaluation_duration": 10, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"explore": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": true, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": null, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 2, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"policy_01": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]], [[[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]], (3, 64, 64), float32)", "Discrete(7)", {}], "policy_02": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]], [[[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]], (3, 64, 64), float32)", "Discrete(7)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": "<function <lambda> at 0x7f1aa3747f80>", "policies_to_train": ["policy_01"], "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": 100000, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": false, "hiddens": [], "double_q": false, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": true, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": null, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 2, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"policy_01": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]], [[[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]], (3, 64, 64), float32)", "Discrete(7)", {}], "policy_02": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]], [[[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]], (3, 64, 64), float32)", "Discrete(7)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": "<function <lambda> at 0x7f1aa3747f80>", "policies_to_train": ["policy_01"], "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": 100000, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": false, "hiddens": [], "double_q": false, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "time_since_restore": 450.2750415802002, "timesteps_since_restore": 5632, "iterations_since_restore": 22, "warmup_time": 45.46659183502197, "perf": {"cpu_util_percent": 24.62, "ram_util_percent": 10.879999999999995}}
{"episode_reward_max": 441.0, "episode_reward_min": 0.0, "episode_reward_mean": 40.18421052631579, "episode_len_mean": 601.0, "episode_media": {}, "episodes_this_iter": 2, "policy_reward_min": {"policy_01": 0.0, "policy_02": 0.0}, "policy_reward_max": {"policy_01": 441.0, "policy_02": 441.0}, "policy_reward_mean": {"policy_01": 28.57894736842105, "policy_02": 11.605263157894736}, "custom_metrics": {}, "hist_stats": {"episode_reward": [0.0, 204.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0], "episode_lengths": [601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601], "policy_policy_01_reward": [0.0, 204.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0], "policy_policy_02_reward": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.3375446142639989, "mean_inference_ms": 5.843941747277377, "mean_action_processing_ms": 0.08923951394677727, "mean_env_wait_ms": 8.174101504183403, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 2, "timesteps_total": 23000, "timesteps_this_iter": 256, "agent_timesteps_total": 46000, "timers": {"load_time_ms": 1.308, "load_throughput": 195788.232, "learn_time_ms": 11.85, "learn_throughput": 21603.203, "update_time_ms": 2.296}, "info": {"learner": {"policy_01": {"custom_metrics": {}, "learner_stats": {"mean_q": 45.12354278564453, "min_q": 26.751476287841797, "max_q": 188.19229125976562, "cur_lr": 0.0005}, "model": {}, "td_error": [-0.4733390808105469, -0.6898651123046875, -0.11510467529296875, -0.10665512084960938, -0.4338111877441406, -0.3982276916503906, 0.061695098876953125, -0.6549453735351562, 0.029438018798828125, 0.054126739501953125, 0.8755912780761719, 0.00089263916015625, -0.18136215209960938, -0.27857208251953125, 0.18307876586914062, -0.6731414794921875, 2.7355422973632812, 0.0044097900390625, 1.1725959777832031, -0.116668701171875, -0.35681915283203125, -0.3282318115234375, -0.33948516845703125, -1.1429710388183594, -0.5118522644042969, 0.24074172973632812, -0.4373512268066406, -0.3114509582519531, -0.01227569580078125, -0.4501609802246094, -0.5115509033203125, 0.138092041015625, -0.792205810546875, -0.015087127685546875, 0.6824798583984375, -6.050323486328125, -0.5249862670898438, -0.26434326171875, -0.6434135437011719, -0.34912109375, 37.37192916870117, -0.4633941650390625, -0.7818489074707031, -0.6566581726074219, -0.4919853210449219, -22.141223907470703, -0.8107833862304688, -0.28113555908203125, 0.17975616455078125, -0.3373565673828125, -0.06017303466796875, 2.780139923095703, -0.5034294128417969, -0.0442962646484375, -0.018894195556640625, -0.8624000549316406, -0.5480918884277344, -2.97088623046875, -0.2192840576171875, -0.4987373352050781, 0.43512725830078125, 0.6085624694824219, 34.84312438964844, -0.46488189697265625, 0.37784576416015625, 4.556358337402344, -0.14745712280273438, -0.114776611328125, 37.99758529663086, -6.140106201171875, 0.188140869140625, 2.671905517578125, -0.044185638427734375, -0.23043441772460938, -0.4913139343261719, -0.657623291015625, -1.00836181640625, -0.039581298828125, -1.5421638488769531, -0.8129005432128906, -0.038562774658203125, -0.5430412292480469, -0.18198394775390625, 0.11338043212890625, -0.7536811828613281, 0.1701202392578125, 19.456483840942383, -0.5009803771972656, -0.4283561706542969, -0.3984642028808594, 36.821739196777344, -0.4457435607910156, -0.381134033203125, 35.97776412963867, -0.526275634765625, 0.0419769287109375, 0.016551971435546875, -0.4022865295410156, 0.048717498779296875, 3.093505859375, -0.6313934326171875, -1.3435211181640625, -0.4022865295410156, -0.7959365844726562, -7.8284912109375, -0.3079948425292969, -0.4763145446777344, -0.24094009399414062, -0.4064903259277344, 54.878822326660156, 35.64429473876953, 0.1770477294921875, 0.5399360656738281, 0.13491439819335938, -0.3990135192871094, -0.47245025634765625, 0.12127304077148438, 11.201408386230469, -0.61114501953125, 0.37551116943359375, -0.4105720520019531, -0.07738876342773438, -0.38907623291015625, 0.02065277099609375, -1.3962440490722656, -0.8481063842773438, -0.536865234375, -0.2306976318359375, -0.3284645080566406, 0.17422866821289062, -0.3530845642089844, -0.5575027465820312, -0.6175956726074219, -1.4749374389648438, -0.26076507568359375, 35.01718521118164, -0.005626678466796875, -0.6645088195800781, 2.111480712890625, -0.4283561706542969, -0.6747970581054688, -0.12269210815429688, -0.5137748718261719, 5.072132110595703, -3.896728515625, -10.269401550292969, -0.18135452270507812, -0.5238609313964844, -0.042236328125, -0.43799591064453125, -0.5567092895507812, 9.14923095703125, -0.3923492431640625, -0.374725341796875, 0.3139190673828125, -0.4919319152832031, -60.17755889892578, 0.5511360168457031, -0.06098175048828125, -0.2822303771972656, 0.9415702819824219, 0.001773834228515625, 4.128376007080078, -0.4566612243652344, -0.3337554931640625, 0.4628105163574219, 0.2614860534667969, -1.2436294555664062, -0.32614898681640625, -0.6284523010253906, -0.31000518798828125, -0.12474441528320312, 0.19577789306640625, -0.52880859375, -3.5216293334960938, -0.334625244140625, -6.9720611572265625, -0.5490760803222656, -0.5217094421386719, 0.14363479614257812, -0.708709716796875, -0.1089019775390625, 0.481903076171875, -0.11152267456054688, -0.08940505981445312, 0.11890411376953125, -0.3135719299316406, 2.4082183837890625, 0.17422103881835938, 0.0584259033203125, 0.21308135986328125, 0.24123382568359375, -0.39182281494140625, -0.3789520263671875, -0.4998359680175781, -1.8563194274902344, -4.7640380859375, -0.13209915161132812, -0.0833587646484375, -0.47223663330078125, 0.19864273071289062, -0.18210601806640625, -0.0110931396484375, -0.5370025634765625, -0.7713356018066406, -0.46051788330078125, -0.2711944580078125, -0.010463714599609375, -0.24188232421875, -0.43740081787109375, -1.5231742858886719, -0.02417755126953125, 0.7773361206054688, -0.7908134460449219, -0.33382415771484375, -0.07379150390625, -0.13106918334960938, -0.6794090270996094, 1.0125198364257812, 0.21308135986328125, -15.398017883300781, -0.28052520751953125, -0.30948638916015625, -0.32033538818359375, -0.4171714782714844, 0.333740234375, 35.30772018432617, -0.35681915283203125, -0.0821380615234375, -0.10667800903320312, -0.02618408203125, -0.1556549072265625, 0.0062255859375, -0.4362640380859375, -0.6189727783203125, -0.6677665710449219, 3.1258392333984375, 36.343101501464844, 35.198646545410156, -0.13797378540039062, 0.23537063598632812, -0.05475616455078125, -0.38287353515625, 0.4163932800292969, -0.708709716796875, -0.04750823974609375, -0.548919677734375, -0.5196456909179688, -0.4534149169921875, 0.4573402404785156, 0.2551307678222656, -0.35794830322265625, -0.4892768859863281, 0.0146942138671875, -15.398017883300781, -0.003940582275390625], "mean_td_error": 1.0389693975448608}}, "num_steps_sampled": 23000, "num_agent_steps_sampled": 46000, "num_steps_trained": 704256, "num_steps_trained_this_iter": 256, "num_agent_steps_trained": 1408512, "last_target_update_ts": 22672, "num_target_updates": 44}, "done": false, "episodes_total": 38, "training_iteration": 23, "trial_id": "e21e6_00000", "experiment_id": "434cd42d33fd4b638f17fc69fa01d74f", "date": "2022-06-14_19-09-51", "timestamp": 1655248191, "time_this_iter_s": 21.198199033737183, "time_total_s": 471.4732406139374, "pid": 2568314, "hostname": "lambda-dual", "node_ip": "10.104.51.19", "config": {"num_workers": 2, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.0005, "train_batch_size": 256, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "cnet", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "1v1env", "observation_space": null, "action_space": null, "env_config": {}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": true, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": null, "evaluation_duration": 10, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"num_workers": 2, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.0005, "train_batch_size": 256, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "cnet", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "1v1env", "observation_space": null, "action_space": null, "env_config": {}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": false, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": null, "evaluation_duration": 10, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"explore": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": true, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": null, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 2, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"policy_01": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]], [[[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]], (3, 64, 64), float32)", "Discrete(7)", {}], "policy_02": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]], [[[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]], (3, 64, 64), float32)", "Discrete(7)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": "<function <lambda> at 0x7f1aa2ecb5f0>", "policies_to_train": ["policy_01"], "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": 100000, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": false, "hiddens": [], "double_q": false, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": true, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": null, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 2, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"policy_01": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]], [[[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]], (3, 64, 64), float32)", "Discrete(7)", {}], "policy_02": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]], [[[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]], (3, 64, 64), float32)", "Discrete(7)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": "<function <lambda> at 0x7f1aa2ecb5f0>", "policies_to_train": ["policy_01"], "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": 100000, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": false, "hiddens": [], "double_q": false, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "time_since_restore": 471.4732406139374, "timesteps_since_restore": 5888, "iterations_since_restore": 23, "warmup_time": 45.46659183502197, "perf": {"cpu_util_percent": 25.5, "ram_util_percent": 10.913333333333329}}
{"episode_reward_max": 441.0, "episode_reward_min": 0.0, "episode_reward_mean": 40.18421052631579, "episode_len_mean": 601.0, "episode_media": {}, "episodes_this_iter": 0, "policy_reward_min": {"policy_01": 0.0, "policy_02": 0.0}, "policy_reward_max": {"policy_01": 441.0, "policy_02": 441.0}, "policy_reward_mean": {"policy_01": 28.57894736842105, "policy_02": 11.605263157894736}, "custom_metrics": {}, "hist_stats": {"episode_reward": [0.0, 204.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0], "episode_lengths": [601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601], "policy_policy_01_reward": [0.0, 204.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0], "policy_policy_02_reward": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.3375446142639989, "mean_inference_ms": 5.843941747277377, "mean_action_processing_ms": 0.08923951394677727, "mean_env_wait_ms": 8.174101504183403, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 2, "timesteps_total": 24000, "timesteps_this_iter": 256, "agent_timesteps_total": 48000, "timers": {"load_time_ms": 1.305, "load_throughput": 196242.68, "learn_time_ms": 11.167, "learn_throughput": 22924.294, "update_time_ms": 2.172}, "info": {"learner": {"policy_01": {"custom_metrics": {}, "learner_stats": {"mean_q": 44.583316802978516, "min_q": 27.914310455322266, "max_q": 210.9166717529297, "cur_lr": 0.0005}, "model": {}, "td_error": [-0.42292022705078125, -1.11505126953125, -1.1993446350097656, -0.5600090026855469, -0.8434562683105469, -0.5077934265136719, -0.011196136474609375, -0.3159217834472656, 2.5524864196777344, -0.36808013916015625, -1.3989448547363281, -0.22512054443359375, -0.3291168212890625, -0.5600090026855469, -0.24309158325195312, -0.7451362609863281, -0.7322006225585938, -7.290336608886719, 0.37319183349609375, -1.1078758239746094, -0.01494598388671875, -0.331329345703125, -0.2919197082519531, 0.15364837646484375, -0.296356201171875, 33.70797348022461, 1.2135505676269531, -1.2000770568847656, 0.15063095092773438, -1.4689559936523438, 0.18863296508789062, -0.21414947509765625, -0.9964675903320312, -22.098236083984375, -0.7916603088378906, -0.524566650390625, 0.1816864013671875, 0.08472824096679688, -0.45478057861328125, -0.49968719482421875, 0.240264892578125, -0.5695266723632812, -0.38385009765625, -0.8260002136230469, -0.6580581665039062, -0.24195480346679688, 0.22335052490234375, 2.6702880859375e-05, -0.0486907958984375, -0.286865234375, -0.6239166259765625, -0.8522834777832031, -1.4016227722167969, 0.7839927673339844, 0.09177017211914062, 0.00542449951171875, -0.12916183471679688, -0.500244140625, -0.6277618408203125, -0.9750938415527344, -0.07988739013671875, -0.8973922729492188, -0.4721832275390625, -12.364517211914062, 0.4066886901855469, -3.054168701171875, -0.8947601318359375, 36.844932556152344, -0.4951324462890625, 1.6934852600097656, -0.9931602478027344, 0.08382034301757812, 0.012279510498046875, -0.5221405029296875, -0.3888893127441406, -0.036693572998046875, -4.436737060546875, -0.42779541015625, -0.9967269897460938, -0.3897285461425781, -1.8359260559082031, -1.1648101806640625, -0.9704971313476562, -1.8024139404296875, -2.661632537841797, -1.1794815063476562, 36.844932556152344, -1.0190048217773438, 0.1856231689453125, -0.05849456787109375, -0.89996337890625, -0.18039703369140625, -0.11248779296875, -0.5637321472167969, -0.7033538818359375, -1.3565597534179688, -0.18349838256835938, 1.1956939697265625, -0.7974777221679688, -0.9764404296875, -7.290336608886719, -0.01854705810546875, 0.14727401733398438, -1.0344696044921875, -0.3913993835449219, 0.5704383850097656, -7.1248626708984375, -0.14551925659179688, 0.030780792236328125, -0.3964576721191406, -0.5253677368164062, -0.2199859619140625, -2.507598876953125, -5.283771514892578, -0.4151878356933594, 2.2392845153808594, -0.7681121826171875, -0.5217170715332031, -0.5091209411621094, -0.527130126953125, -0.7812690734863281, -0.25186920166015625, -0.015773773193359375, 0.6404151916503906, 3.8927993774414062, -0.5638084411621094, 0.013225555419921875, 0.0088958740234375, 2.83636474609375, -0.5877456665039062, -3.3308639526367188, -3.323974609375, -0.3934326171875, 2.6710433959960938, 0.10003662109375, -0.20597076416015625, 2.782268524169922, -10.365203857421875, -0.3121337890625, 0.8449897766113281, -0.5132026672363281, -0.049106597900390625, -0.8418159484863281, 0.2629661560058594, -0.6246795654296875, -0.06616592407226562, -2.015533447265625, -0.3314208984375, -0.6282615661621094, -0.5773200988769531, -0.14704513549804688, -0.7159347534179688, 0.000133514404296875, -0.76983642578125, -0.4574432373046875, -2.5963897705078125, -0.5499458312988281, -0.4258232116699219, -1.0331916809082031, -2.3301773071289062, 35.14371109008789, 22.120647430419922, -0.34853363037109375, -0.003631591796875, -0.4377708435058594, -1.0164871215820312, -8.3795166015625, 35.407962799072266, -0.5637321472167969, -0.8830413818359375, -1.0579986572265625, 0.3903160095214844, -0.11007308959960938, -0.228546142578125, -0.011165618896484375, 11.539291381835938, -0.08815383911132812, -0.2905120849609375, -0.2686614990234375, -18.277511596679688, -0.4755287170410156, -0.21799850463867188, -1.1532363891601562, -1.1392745971679688, -2.159801483154297, -0.7997016906738281, -0.3443336486816406, -0.45642852783203125, 1.0617446899414062, 0.5268096923828125, -0.6423377990722656, 0.7119827270507812, -0.5532722473144531, -0.17342376708984375, -1.1182327270507812, -0.13039398193359375, -0.5331611633300781, -0.7199974060058594, -0.654296875, -0.4011116027832031, 0.396942138671875, -1.0164871215820312, -0.22573089599609375, -8.414787292480469, -0.48809051513671875, -0.2663307189941406, -0.13735198974609375, -1.3401527404785156, -0.5331611633300781, 0.7882347106933594, -0.031711578369140625, -0.21530914306640625, -0.620330810546875, -0.8116416931152344, -0.18700790405273438, -7.372901916503906, -0.6852989196777344, 0.4049644470214844, -1.2497329711914062, -0.7762184143066406, -0.15913772583007812, -0.447540283203125, 0.06621932983398438, 0.19738006591796875, -1.0821037292480469, 0.6698570251464844, -0.28897857666015625, 0.04288482666015625, -0.036594390869140625, -0.47490692138671875, -1.2608985900878906, 0.10369873046875, 0.14930343627929688, 0.11282730102539062, -0.5631294250488281, -0.8320121765136719, -2.1721267700195312, -1.5879859924316406, -7.475589752197266, -0.7446479797363281, -0.16252517700195312, -0.8084526062011719, 0.136260986328125, -0.2468719482421875, 0.731781005859375, -0.251617431640625, -0.5759010314941406, -0.026340484619140625, -1.2686691284179688, 1.9322776794433594, 0.36325836181640625, 0.6897048950195312, 0.07596206665039062, -0.40659332275390625, -0.12902069091796875, -0.13895797729492188], "mean_td_error": -0.007077082991600037}}, "num_steps_sampled": 24000, "num_agent_steps_sampled": 48000, "num_steps_trained": 736256, "num_steps_trained_this_iter": 256, "num_agent_steps_trained": 1472512, "last_target_update_ts": 23680, "num_target_updates": 46}, "done": false, "episodes_total": 38, "training_iteration": 24, "trial_id": "e21e6_00000", "experiment_id": "434cd42d33fd4b638f17fc69fa01d74f", "date": "2022-06-14_19-10-12", "timestamp": 1655248212, "time_this_iter_s": 20.921568393707275, "time_total_s": 492.39480900764465, "pid": 2568314, "hostname": "lambda-dual", "node_ip": "10.104.51.19", "config": {"num_workers": 2, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.0005, "train_batch_size": 256, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "cnet", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "1v1env", "observation_space": null, "action_space": null, "env_config": {}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": true, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": null, "evaluation_duration": 10, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"num_workers": 2, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.0005, "train_batch_size": 256, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "cnet", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "1v1env", "observation_space": null, "action_space": null, "env_config": {}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": false, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": null, "evaluation_duration": 10, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"explore": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": true, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": null, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 2, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"policy_01": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]], [[[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]], (3, 64, 64), float32)", "Discrete(7)", {}], "policy_02": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]], [[[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]], (3, 64, 64), float32)", "Discrete(7)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": "<function <lambda> at 0x7f1aa2e9a3b0>", "policies_to_train": ["policy_01"], "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": 100000, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": false, "hiddens": [], "double_q": false, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": true, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": null, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 2, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"policy_01": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]], [[[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]], (3, 64, 64), float32)", "Discrete(7)", {}], "policy_02": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]], [[[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]], (3, 64, 64), float32)", "Discrete(7)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": "<function <lambda> at 0x7f1aa2e9a3b0>", "policies_to_train": ["policy_01"], "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": 100000, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": false, "hiddens": [], "double_q": false, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "time_since_restore": 492.39480900764465, "timesteps_since_restore": 6144, "iterations_since_restore": 24, "warmup_time": 45.46659183502197, "perf": {"cpu_util_percent": 24.583333333333336, "ram_util_percent": 10.913333333333332}}
{"episode_reward_max": 441.0, "episode_reward_min": 0.0, "episode_reward_mean": 38.175, "episode_len_mean": 601.0, "episode_media": {}, "episodes_this_iter": 2, "policy_reward_min": {"policy_01": 0.0, "policy_02": 0.0}, "policy_reward_max": {"policy_01": 441.0, "policy_02": 441.0}, "policy_reward_mean": {"policy_01": 27.15, "policy_02": 11.025}, "custom_metrics": {}, "hist_stats": {"episode_reward": [0.0, 204.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0], "episode_lengths": [601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601], "policy_policy_01_reward": [0.0, 204.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0], "policy_policy_02_reward": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.3373654624345603, "mean_inference_ms": 5.841218074534845, "mean_action_processing_ms": 0.08921215147507029, "mean_env_wait_ms": 8.159777004592485, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 2, "timesteps_total": 25000, "timesteps_this_iter": 256, "agent_timesteps_total": 50000, "timers": {"load_time_ms": 1.322, "load_throughput": 193627.479, "learn_time_ms": 11.791, "learn_throughput": 21711.667, "update_time_ms": 2.351}, "info": {"learner": {"policy_01": {"custom_metrics": {}, "learner_stats": {"mean_q": 47.41615295410156, "min_q": 35.40033721923828, "max_q": 207.92910766601562, "cur_lr": 0.0005}, "model": {}, "td_error": [-0.33526611328125, -0.09839248657226562, -4.003059387207031, -0.16817474365234375, 0.4541130065917969, 37.34923553466797, 0.47618865966796875, -1.0111846923828125, 0.18626022338867188, -0.6833114624023438, -0.164825439453125, -0.14531326293945312, 0.9771575927734375, -0.65277099609375, -0.8531761169433594, -0.315338134765625, 0.14789581298828125, 39.80065155029297, -0.06813430786132812, 5.7000732421875, 0.3083610534667969, -0.3420448303222656, -0.27510833740234375, -0.7557449340820312, 1.108428955078125, -0.6340293884277344, 1.4260711669921875, 1.5688209533691406, -0.12002182006835938, 0.5939788818359375, -0.4232521057128906, 0.060577392578125, -0.09160232543945312, -29.684539794921875, 0.7276611328125, -9.988723754882812, -0.5870513916015625, 0.8997611999511719, -0.024494171142578125, -0.09642410278320312, -0.22023391723632812, -0.76544189453125, -1.7835807800292969, -0.7492904663085938, -28.758445739746094, 1.1461639404296875, -1.2046470642089844, 0.2937736511230469, 10.221572875976562, 0.22027587890625, 0.0733642578125, -12.105636596679688, 3.0696449279785156, -0.20653152465820312, 36.666160583496094, 0.4305839538574219, 2.3875961303710938, -0.18564987182617188, 0.24710845947265625, -7.155670166015625, -1.0784950256347656, -0.03692626953125, 5.347484588623047, -0.5565567016601562, -3.80047607421875, -0.8235435485839844, 0.12862014770507812, 0.6955757141113281, -1.0034408569335938, 0.03436279296875, -1.2226638793945312, 0.5081672668457031, -6.564579010009766, 0.36542510986328125, 0.6764869689941406, 0.784210205078125, -0.319000244140625, 36.70787048339844, 1.392425537109375, 0.7216072082519531, 0.01230621337890625, -0.43871307373046875, -0.18457794189453125, -0.5910453796386719, 0.5128631591796875, -1.0908660888671875, -0.3692436218261719, 0.19218826293945312, 0.020709991455078125, -0.7457160949707031, -0.1285247802734375, -0.6051368713378906, 0.8494720458984375, 0.72186279296875, 0.5830345153808594, 0.575653076171875, -1.3069801330566406, -0.6945953369140625, -0.28624725341796875, 0.4452705383300781, -0.4542655944824219, -0.14944076538085938, 0.1543426513671875, -0.2664375305175781, 0.7028579711914062, -0.13505935668945312, 0.4233741760253906, -0.8979606628417969, 0.15015792846679688, -0.8498916625976562, 0.138946533203125, -0.07570266723632812, 7.515899658203125, 0.5374259948730469, -0.18991851806640625, 0.5217399597167969, 0.5640792846679688, -0.25336456298828125, -0.006633758544921875, -1.0171432495117188, -0.390045166015625, -0.4801063537597656, 0.3394584655761719, -0.397430419921875, 1.5944595336914062, 0.1278076171875, 0.8120956420898438, 0.35717010498046875, -0.1957855224609375, -1.0552406311035156, -0.1602783203125, 0.2410430908203125, -1.2547988891601562, -0.06959915161132812, 0.5027198791503906, -0.017208099365234375, -0.8154373168945312, 0.8409309387207031, -0.15798187255859375, -0.140533447265625, 0.41033935546875, -1.2729072570800781, 0.15679550170898438, 0.6878280639648438, 0.15761566162109375, 1.0735359191894531, -0.12920761108398438, 6.453132629394531, 0.47713470458984375, 0.014492034912109375, 0.3126411437988281, 1.403472900390625, 0.17226791381835938, 0.6032829284667969, 0.7490768432617188, 0.1473846435546875, 1.5018653869628906, 0.5932464599609375, 6.6867523193359375, 0.47126007080078125, -0.502960205078125, -0.3203392028808594, -0.046009063720703125, 0.3565521240234375, -0.8872756958007812, 0.14186859130859375, 1.169403076171875, -0.15869140625, -0.20125579833984375, -0.18816375732421875, -0.139801025390625, -0.6511154174804688, -0.3614463806152344, -3.1771621704101562, 37.27268981933594, 0.02228546142578125, 0.33806610107421875, -0.002567291259765625, 0.5273056030273438, -0.3664283752441406, 1.3353462219238281, 0.08135986328125, 0.196868896484375, 0.07364273071289062, 0.10892105102539062, -0.6443557739257812, -0.08154296875, -0.8749618530273438, -0.0993499755859375, -1.3144073486328125, 0.7572212219238281, -0.6270408630371094, -0.5274887084960938, -0.6671791076660156, -0.8833427429199219, -0.03589630126953125, -1.0818252563476562, 5.3617095947265625, 0.2538719177246094, 0.152618408203125, -0.057949066162109375, 0.12823867797851562, 39.42396926879883, 0.4228706359863281, 2.6996307373046875, -7.0721435546875, 0.22251129150390625, -0.6306533813476562, -0.24298477172851562, 2.6996307373046875, 0.4277915954589844, -0.3490562438964844, 0.8107490539550781, 0.02286529541015625, 1.3532485961914062, 0.32244110107421875, -8.577987670898438, 0.31182861328125, -1.5423622131347656, 0.20143890380859375, 2.3374252319335938, -0.5952491760253906, 0.3802299499511719, 0.9703178405761719, -0.9266548156738281, 0.0253753662109375, 0.6940116882324219, -0.4812431335449219, 0.19525146484375, -0.8942375183105469, 0.4522666931152344, 0.2626991271972656, -0.04839324951171875, 0.29396820068359375, 0.5183067321777344, 0.5367240905761719, -0.29114532470703125, 0.3990364074707031, 0.03931427001953125, 0.4111061096191406, 5.7000732421875, 0.2205963134765625, 0.4735374450683594, 0.7056732177734375, 0.3092193603515625, 0.6995010375976562, 0.4154777526855469, -0.6017532348632812, -1.0086669921875, -0.3781089782714844, 0.2011260986328125, -0.656005859375, 0.7603416442871094, -0.5379600524902344, -0.3165855407714844, 0.18459320068359375], "mean_td_error": 0.6773838400840759}}, "num_steps_sampled": 25000, "num_agent_steps_sampled": 50000, "num_steps_trained": 768256, "num_steps_trained_this_iter": 256, "num_agent_steps_trained": 1536512, "last_target_update_ts": 24688, "num_target_updates": 48}, "done": false, "episodes_total": 40, "training_iteration": 25, "trial_id": "e21e6_00000", "experiment_id": "434cd42d33fd4b638f17fc69fa01d74f", "date": "2022-06-14_19-10-33", "timestamp": 1655248233, "time_this_iter_s": 21.178491353988647, "time_total_s": 513.5733003616333, "pid": 2568314, "hostname": "lambda-dual", "node_ip": "10.104.51.19", "config": {"num_workers": 2, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.0005, "train_batch_size": 256, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "cnet", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "1v1env", "observation_space": null, "action_space": null, "env_config": {}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": true, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": null, "evaluation_duration": 10, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"num_workers": 2, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.0005, "train_batch_size": 256, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "cnet", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "1v1env", "observation_space": null, "action_space": null, "env_config": {}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": false, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": null, "evaluation_duration": 10, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"explore": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": true, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": null, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 2, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"policy_01": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]], [[[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]], (3, 64, 64), float32)", "Discrete(7)", {}], "policy_02": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]], [[[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]], (3, 64, 64), float32)", "Discrete(7)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": "<function <lambda> at 0x7f1aa2e9a050>", "policies_to_train": ["policy_01"], "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": 100000, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": false, "hiddens": [], "double_q": false, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": true, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": null, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 2, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"policy_01": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]], [[[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]], (3, 64, 64), float32)", "Discrete(7)", {}], "policy_02": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]], [[[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]], (3, 64, 64), float32)", "Discrete(7)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": "<function <lambda> at 0x7f1aa2e9a050>", "policies_to_train": ["policy_01"], "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": 100000, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": false, "hiddens": [], "double_q": false, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "time_since_restore": 513.5733003616333, "timesteps_since_restore": 6400, "iterations_since_restore": 25, "warmup_time": 45.46659183502197, "perf": {"cpu_util_percent": 24.270967741935483, "ram_util_percent": 11.0}}
{"episode_reward_max": 441.0, "episode_reward_min": 0.0, "episode_reward_mean": 36.357142857142854, "episode_len_mean": 601.0, "episode_media": {}, "episodes_this_iter": 2, "policy_reward_min": {"policy_01": 0.0, "policy_02": 0.0}, "policy_reward_max": {"policy_01": 441.0, "policy_02": 441.0}, "policy_reward_mean": {"policy_01": 25.857142857142858, "policy_02": 10.5}, "custom_metrics": {}, "hist_stats": {"episode_reward": [0.0, 204.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "episode_lengths": [601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601], "policy_policy_01_reward": [0.0, 204.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "policy_policy_02_reward": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.3372147441218721, "mean_inference_ms": 5.8385780423229905, "mean_action_processing_ms": 0.08918501517730754, "mean_env_wait_ms": 8.146296888839382, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 2, "timesteps_total": 26000, "timesteps_this_iter": 256, "agent_timesteps_total": 52000, "timers": {"load_time_ms": 1.321, "load_throughput": 193760.254, "learn_time_ms": 11.466, "learn_throughput": 22326.921, "update_time_ms": 2.143}, "info": {"learner": {"policy_01": {"custom_metrics": {}, "learner_stats": {"mean_q": 48.004798889160156, "min_q": 36.33031463623047, "max_q": 185.87103271484375, "cur_lr": 0.0005}, "model": {}, "td_error": [0.39408111572265625, 0.6671714782714844, 0.4607696533203125, 1.102264404296875, 0.7675819396972656, 1.302459716796875, 0.5581932067871094, -1.5660781860351562, 0.48995208740234375, 0.6271018981933594, 1.0635452270507812, 0.6772003173828125, 39.362308502197266, 0.5896415710449219, 1.0928230285644531, 0.8031578063964844, 0.430908203125, 0.71868896484375, -0.0008697509765625, 0.5720710754394531, 0.52349853515625, 0.5153732299804688, -13.489517211914062, 1.270263671875, 0.3546867370605469, 0.8732032775878906, -4.248748779296875, 0.7783241271972656, 0.9997520446777344, 1.6091423034667969, 0.8750991821289062, 0.4746284484863281, 0.7404747009277344, 39.68888854980469, 0.8360671997070312, 0.5500106811523438, 0.180023193359375, 0.5515327453613281, 0.901275634765625, 0.9607772827148438, 0.3954734802246094, 0.5217819213867188, 3.5105056762695312, -0.3742561340332031, 2.8504714965820312, 0.1890106201171875, -5.353900909423828, 0.8333206176757812, 0.6871337890625, -3.5981216430664062, 0.19332504272460938, 0.5834236145019531, -33.56227493286133, 0.3348121643066406, 0.9244766235351562, 0.3505096435546875, 4.780773162841797, 0.8425102233886719, 4.306545257568359, 25.054580688476562, 0.7287330627441406, 1.1713981628417969, 0.8223724365234375, 0.9609527587890625, -0.5563926696777344, 0.44266510009765625, 0.941864013671875, -0.9638557434082031, 0.9514236450195312, 0.22440338134765625, 1.4490623474121094, 1.2354621887207031, 0.2557640075683594, 0.37129974365234375, 0.8378486633300781, -0.3065605163574219, 0.3759803771972656, 1.7146568298339844, 1.4887237548828125, 0.8385353088378906, 0.5084724426269531, 4.636058807373047, 0.264984130859375, 0.7958755493164062, 1.033355712890625, 0.9890823364257812, 5.399272918701172, 0.260833740234375, -1.9565963745117188, 1.7530593872070312, -1.7303848266601562, 0.48564910888671875, 0.9084587097167969, 38.158111572265625, 1.4878692626953125, 0.5883026123046875, -2.96331787109375, 0.7834053039550781, 0.48432159423828125, 0.13677215576171875, 0.6944007873535156, 1.8290023803710938, 1.1683235168457031, -0.39849090576171875, -3.20721435546875, 3.111358642578125, -13.891403198242188, 0.9301109313964844, -7.315696716308594, 0.5934257507324219, 0.6064071655273438, 0.21268081665039062, 73.92230987548828, 0.4348297119140625, 1.3449440002441406, 0.6047859191894531, 0.09450912475585938, 0.5740242004394531, 0.954315185546875, 10.170608520507812, 1.019317626953125, 0.4607696533203125, 0.4482688903808594, 3.5288314819335938, -1.3293609619140625, -1.8777313232421875, 1.2876091003417969, 0.6549797058105469, -0.15678787231445312, 0.6563911437988281, 0.13895034790039062, 1.6692466735839844, 0.3761787414550781, 0.5665817260742188, 0.9272041320800781, 0.9664421081542969, 0.9258995056152344, -3.3293724060058594, 0.2242584228515625, 0.5494499206542969, 0.5543060302734375, 0.2395477294921875, 1.00897216796875, 0.4009284973144531, 1.4878883361816406, 0.599853515625, 1.1762619018554688, 0.9390335083007812, 0.604949951171875, 0.3461265563964844, 0.7695960998535156, 4.2238006591796875, 1.4738121032714844, 0.3512458801269531, 0.55633544921875, 0.81365966796875, 1.670623779296875, 2.637889862060547, 1.9726600646972656, -4.2449798583984375, -1.1415557861328125, 2.68359375, 36.303001403808594, 39.684810638427734, 0.9323921203613281, 0.9028053283691406, 0.779693603515625, 0.9912528991699219, 0.6327438354492188, 0.3265228271484375, 0.6121063232421875, 0.24604034423828125, 0.6419906616210938, 1.3250694274902344, 1.3004608154296875, 0.6097297668457031, 1.3655853271484375, 0.7240829467773438, -1.2392425537109375, -1.29827880859375, 0.20123672485351562, 0.2975273132324219, 1.0838699340820312, -3.5091705322265625, 1.2113113403320312, 0.8230705261230469, 0.2975273132324219, 39.684810638427734, 0.21664810180664062, 0.9378204345703125, 0.37163543701171875, 3.381153106689453, 0.9481697082519531, 0.27437591552734375, 0.8177871704101562, 0.5499153137207031, 0.8967018127441406, 0.7588577270507812, 39.69207763671875, -2.3458404541015625, 0.2918281555175781, 0.393829345703125, 0.4398155212402344, 0.9728546142578125, 0.7953643798828125, 0.8979988098144531, 0.20826339721679688, 1.2480354309082031, 0.5896415710449219, 0.04195404052734375, 0.4752616882324219, -0.9633445739746094, 0.3647918701171875, 0.771026611328125, -35.06095886230469, 0.5066070556640625, 3.1013336181640625, 3.0538101196289062, 0.854827880859375, 0.3786964416503906, 0.29613494873046875, 0.10631179809570312, 0.9503822326660156, 0.75262451171875, 0.25348663330078125, 0.9467010498046875, 0.4517669677734375, 0.6485595703125, -0.09338760375976562, 1.1626052856445312, 0.039051055908203125, -0.9368896484375, 0.6218757629394531, 0.19843292236328125, 0.6763191223144531, 73.92230987548828, 0.9408683776855469, 0.4710273742675781, 0.26407623291015625, 2.1357345581054688, 0.7692718505859375, 0.9617233276367188, 0.8047027587890625, 2.1212997436523438, 0.34722137451171875, 0.532440185546875, -7.011688232421875, 0.5697288513183594, 0.6254043579101562, 1.4303550720214844, 0.24245452880859375, 1.0037651062011719, 1.7106895446777344, 0.8900375366210938, 0.30396270751953125, 0.3837547302246094], "mean_td_error": 1.9261159896850586}}, "num_steps_sampled": 26000, "num_agent_steps_sampled": 52000, "num_steps_trained": 800256, "num_steps_trained_this_iter": 256, "num_agent_steps_trained": 1600512, "last_target_update_ts": 25696, "num_target_updates": 50}, "done": false, "episodes_total": 42, "training_iteration": 26, "trial_id": "e21e6_00000", "experiment_id": "434cd42d33fd4b638f17fc69fa01d74f", "date": "2022-06-14_19-10-55", "timestamp": 1655248255, "time_this_iter_s": 21.21492886543274, "time_total_s": 534.788229227066, "pid": 2568314, "hostname": "lambda-dual", "node_ip": "10.104.51.19", "config": {"num_workers": 2, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.0005, "train_batch_size": 256, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "cnet", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "1v1env", "observation_space": null, "action_space": null, "env_config": {}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": true, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": null, "evaluation_duration": 10, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"num_workers": 2, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.0005, "train_batch_size": 256, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "cnet", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "1v1env", "observation_space": null, "action_space": null, "env_config": {}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": false, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": null, "evaluation_duration": 10, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"explore": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": true, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": null, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 2, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"policy_01": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]], [[[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]], (3, 64, 64), float32)", "Discrete(7)", {}], "policy_02": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]], [[[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]], (3, 64, 64), float32)", "Discrete(7)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": "<function <lambda> at 0x7f1aa2ed3c20>", "policies_to_train": ["policy_01"], "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": 100000, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": false, "hiddens": [], "double_q": false, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": true, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": null, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 2, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"policy_01": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]], [[[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]], (3, 64, 64), float32)", "Discrete(7)", {}], "policy_02": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]], [[[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]], (3, 64, 64), float32)", "Discrete(7)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": "<function <lambda> at 0x7f1aa2ed3c20>", "policies_to_train": ["policy_01"], "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": 100000, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": false, "hiddens": [], "double_q": false, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "time_since_restore": 534.788229227066, "timesteps_since_restore": 6656, "iterations_since_restore": 26, "warmup_time": 45.46659183502197, "perf": {"cpu_util_percent": 24.493333333333336, "ram_util_percent": 11.083333333333337}}
{"episode_reward_max": 441.0, "episode_reward_min": 0.0, "episode_reward_mean": 34.70454545454545, "episode_len_mean": 601.0, "episode_media": {}, "episodes_this_iter": 2, "policy_reward_min": {"policy_01": 0.0, "policy_02": 0.0}, "policy_reward_max": {"policy_01": 441.0, "policy_02": 441.0}, "policy_reward_mean": {"policy_01": 24.681818181818183, "policy_02": 10.022727272727273}, "custom_metrics": {}, "hist_stats": {"episode_reward": [0.0, 204.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "episode_lengths": [601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601], "policy_policy_01_reward": [0.0, 204.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "policy_policy_02_reward": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.33708316294202206, "mean_inference_ms": 5.836201471335238, "mean_action_processing_ms": 0.08915713594869551, "mean_env_wait_ms": 8.133533538529965, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 2, "timesteps_total": 27000, "timesteps_this_iter": 256, "agent_timesteps_total": 54000, "timers": {"load_time_ms": 1.309, "load_throughput": 195513.725, "learn_time_ms": 11.351, "learn_throughput": 22553.242, "update_time_ms": 2.193}, "info": {"learner": {"policy_01": {"custom_metrics": {}, "learner_stats": {"mean_q": 50.93347930908203, "min_q": 35.755218505859375, "max_q": 239.764892578125, "cur_lr": 0.0005}, "model": {}, "td_error": [0.23748397827148438, 0.10535049438476562, -0.08202362060546875, -0.4625091552734375, -0.2689361572265625, -0.4546852111816406, -0.2644309997558594, 1.3049545288085938, -0.4680671691894531, -0.4235115051269531, 0.2176513671875, 0.11740493774414062, 13.276214599609375, 0.5337257385253906, -0.16510009765625, -0.3545570373535156, 0.5611534118652344, -0.14704513549804688, -1.5482101440429688, 0.4174346923828125, -0.4601707458496094, 0.5651588439941406, 0.52294921875, -4.183963775634766, 0.48050689697265625, -0.9137420654296875, 0.3459587097167969, 0.09813308715820312, -0.4043617248535156, -0.6829185485839844, 0.39000701904296875, -0.884857177734375, -0.2872276306152344, -0.2583160400390625, -0.8625297546386719, 5.4019927978515625, -0.4698677062988281, 0.09162139892578125, 0.4328956604003906, -0.0915069580078125, 0.09674072265625, 0.4636497497558594, -0.1716461181640625, 8.196456909179688, 0.8341140747070312, -0.11525344848632812, 0.019443511962890625, -0.15631103515625, -0.09862518310546875, -15.50836181640625, -88.35685729980469, -0.14714813232421875, 0.3967857360839844, 1.7285537719726562, -0.1291351318359375, 0.084075927734375, -0.0945587158203125, 1.606964111328125, 0.07251358032226562, 2.6349334716796875, 0.26639556884765625, 0.14868927001953125, -0.19163131713867188, -0.3801231384277344, -0.4698677062988281, -0.5038833618164062, 2.8970603942871094, 0.4638938903808594, -1.4534034729003906, -0.15904998779296875, 0.11861038208007812, -0.6905288696289062, 0.7434425354003906, -3.1437530517578125, -0.2906990051269531, -0.8785133361816406, 0.07787704467773438, -0.5685348510742188, -1.245361328125, -0.6654548645019531, -3.00677490234375, 0.0810546875, -0.3198280334472656, 38.81560134887695, 0.7662353515625, 0.10715866088867188, 13.40081787109375, 0.11754989624023438, 0.45868682861328125, 0.454681396484375, 38.516536712646484, 0.046001434326171875, -0.23697280883789062, -0.36266326904296875, -0.10363388061523438, 3.4487152099609375, 0.2084197998046875, 2.1954078674316406, -0.3812904357910156, -0.5039520263671875, 0.7827873229980469, -0.5536117553710938, -0.4766807556152344, -0.5982208251953125, -0.4779510498046875, -1.1976776123046875, 0.6636581420898438, 2.2021255493164062, 0.13151931762695312, -0.5264396667480469, -0.6075210571289062, -0.037151336669921875, 5.003986358642578, -15.50836181640625, -0.3317413330078125, -0.8603401184082031, 11.09039306640625, 1.0139617919921875, -0.3599853515625, -1.0710983276367188, -0.20986175537109375, 18.812164306640625, 0.2733497619628906, 7.714824676513672, 0.0887908935546875, 0.9020233154296875, 38.516536712646484, -0.175750732421875, -0.24796676635742188, -0.24355316162109375, 37.869483947753906, -0.04749298095703125, -4.875740051269531, -0.2428741455078125, 15.817901611328125, 0.5768394470214844, -0.45339202880859375, -0.48795318603515625, -0.17346572875976562, -0.2981529235839844, 0.127197265625, 0.22833633422851562, 1.2442665100097656, -0.26300811767578125, 0.8941879272460938, -1.7103500366210938, -0.2689018249511719, 0.4283409118652344, -0.9020347595214844, 3.8304901123046875, 0.3122673034667969, -0.7626152038574219, 0.48540496826171875, 0.45369720458984375, -0.33896636962890625, -0.7996635437011719, 1.0978355407714844, 0.15139007568359375, -0.164825439453125, -0.19099044799804688, -0.7178268432617188, -0.14092254638671875, -0.884857177734375, 0.5881233215332031, -0.6676445007324219, -0.717529296875, -0.16356658935546875, -1.0110206604003906, 4.263057708740234, -0.5536651611328125, -0.2567481994628906, -0.0429840087890625, -0.3828277587890625, -1.4650955200195312, -0.19712066650390625, 0.24988937377929688, -0.4918556213378906, -0.5510635375976562, -0.6716728210449219, -1.3452568054199219, 0.41146087646484375, -5.9679718017578125, -0.05297088623046875, -0.08945465087890625, -0.048870086669921875, -0.2912864685058594, 0.0294952392578125, -0.046108245849609375, -6.438133239746094, -0.37186431884765625, -0.1992645263671875, -0.4261283874511719, -0.02916717529296875, -0.2495269775390625, 0.09023284912109375, 0.43698883056640625, -0.38587188720703125, 18.812164306640625, 0.0394439697265625, -0.037151336669921875, -0.9124755859375, 0.437042236328125, -0.84747314453125, -0.4036293029785156, -15.50836181640625, 0.2589530944824219, -0.4406585693359375, 0.14103317260742188, -0.2269134521484375, -0.4679756164550781, -0.07692718505859375, -2.9400291442871094, 0.024929046630859375, -0.5194053649902344, -1.34149169921875, 2.7404747009277344, -0.47913360595703125, 0.037265777587890625, -1.3407745361328125, 2.658458709716797, 0.140350341796875, 13.319046020507812, -0.12644195556640625, -24.2939453125, 0.4476966857910156, -9.222885131835938, 0.07347488403320312, -0.9511489868164062, -0.09337997436523438, -0.5268669128417969, 0.0697174072265625, -0.11664199829101562, -0.8790016174316406, 0.3816375732421875, 1.2135047912597656, -0.18315887451171875, -0.02037811279296875, -0.0770721435546875, -0.14105224609375, -1.9904327392578125, -1.0547027587890625, 0.9015579223632812, -0.24260711669921875, -0.21080398559570312, -0.5667724609375, -0.3552284240722656, -0.279815673828125, -0.06258392333984375, -0.8720779418945312, -0.22990036010742188, 36.402259826660156, -4.761562347412109, 8.494598388671875, -0.09087753295898438, -0.14206695556640625, 0.347625732421875], "mean_td_error": 0.47055912017822266}}, "num_steps_sampled": 27000, "num_agent_steps_sampled": 54000, "num_steps_trained": 832256, "num_steps_trained_this_iter": 256, "num_agent_steps_trained": 1664512, "last_target_update_ts": 26704, "num_target_updates": 52}, "done": false, "episodes_total": 44, "training_iteration": 27, "trial_id": "e21e6_00000", "experiment_id": "434cd42d33fd4b638f17fc69fa01d74f", "date": "2022-06-14_19-11-16", "timestamp": 1655248276, "time_this_iter_s": 21.287437915802002, "time_total_s": 556.075667142868, "pid": 2568314, "hostname": "lambda-dual", "node_ip": "10.104.51.19", "config": {"num_workers": 2, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.0005, "train_batch_size": 256, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "cnet", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "1v1env", "observation_space": null, "action_space": null, "env_config": {}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": true, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": null, "evaluation_duration": 10, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"num_workers": 2, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.0005, "train_batch_size": 256, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "cnet", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "1v1env", "observation_space": null, "action_space": null, "env_config": {}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": false, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": null, "evaluation_duration": 10, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"explore": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": true, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": null, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 2, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"policy_01": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]], [[[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]], (3, 64, 64), float32)", "Discrete(7)", {}], "policy_02": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]], [[[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]], (3, 64, 64), float32)", "Discrete(7)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": "<function <lambda> at 0x7f1aa3747f80>", "policies_to_train": ["policy_01"], "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": 100000, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": false, "hiddens": [], "double_q": false, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": true, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": null, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 2, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"policy_01": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]], [[[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]], (3, 64, 64), float32)", "Discrete(7)", {}], "policy_02": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]], [[[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]], (3, 64, 64), float32)", "Discrete(7)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": "<function <lambda> at 0x7f1aa3747f80>", "policies_to_train": ["policy_01"], "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": 100000, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": false, "hiddens": [], "double_q": false, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "time_since_restore": 556.075667142868, "timesteps_since_restore": 6912, "iterations_since_restore": 27, "warmup_time": 45.46659183502197, "perf": {"cpu_util_percent": 24.593333333333337, "ram_util_percent": 11.166666666666664}}
{"episode_reward_max": 441.0, "episode_reward_min": 0.0, "episode_reward_mean": 33.19565217391305, "episode_len_mean": 601.0, "episode_media": {}, "episodes_this_iter": 2, "policy_reward_min": {"policy_01": 0.0, "policy_02": 0.0}, "policy_reward_max": {"policy_01": 441.0, "policy_02": 441.0}, "policy_reward_mean": {"policy_01": 23.608695652173914, "policy_02": 9.58695652173913}, "custom_metrics": {}, "hist_stats": {"episode_reward": [0.0, 204.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "episode_lengths": [601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601], "policy_policy_01_reward": [0.0, 204.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "policy_policy_02_reward": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.3369862194314355, "mean_inference_ms": 5.833872028487238, "mean_action_processing_ms": 0.08913090193167214, "mean_env_wait_ms": 8.121580969854495, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 2, "timesteps_total": 28000, "timesteps_this_iter": 256, "agent_timesteps_total": 56000, "timers": {"load_time_ms": 1.359, "load_throughput": 188316.29, "learn_time_ms": 11.86, "learn_throughput": 21584.703, "update_time_ms": 2.343}, "info": {"learner": {"policy_01": {"custom_metrics": {}, "learner_stats": {"mean_q": 52.48565673828125, "min_q": 35.42405700683594, "max_q": 246.52536010742188, "cur_lr": 0.0005}, "model": {}, "td_error": [-5.59906005859375, -0.2783546447753906, 0.22358322143554688, 0.5687484741210938, 2.4133377075195312, 1.4744071960449219, -0.4404945373535156, -3.739501953125, 0.8215141296386719, -2.1915130615234375, 1.7969512939453125, 1.4127311706542969, 0.11049270629882812, -0.36347198486328125, 0.21990203857421875, 2.180713653564453, -0.3616180419921875, -0.7067718505859375, -0.8548049926757812, -0.6148414611816406, -0.5387802124023438, 0.376617431640625, -0.6426277160644531, 0.37134552001953125, -0.23052597045898438, 0.6573905944824219, -0.008617401123046875, -0.10012054443359375, 0.8987922668457031, 0.7336196899414062, 0.2594261169433594, -1.4694404602050781, 0.3382110595703125, 0.6097145080566406, -0.5669937133789062, -7.489204406738281, 0.32294464111328125, -13.484546661376953, 0.2505683898925781, -0.3451576232910156, 0.05838775634765625, 0.3331108093261719, -0.19808578491210938, -0.021541595458984375, -5.137870788574219, 1.8362312316894531, 0.11487197875976562, -0.0932769775390625, 0.19026565551757812, 2.860065460205078, -0.10809326171875, -0.7797203063964844, 0.030078887939453125, 5.211822509765625, 5.227123260498047, -0.3314170837402344, -0.017009735107421875, 0.6288795471191406, -0.03905487060546875, -0.20086669921875, -0.6546783447265625, -0.7514762878417969, -0.5522918701171875, -0.030628204345703125, 1.5104103088378906, -0.012348175048828125, 4.545490264892578, 1.7114639282226562, 0.1575775146484375, 1.4315719604492188, -0.3696136474609375, -0.08393478393554688, 0.9216842651367188, 0.1649169921875, 0.4099464416503906, -0.3485755920410156, 1.306671142578125, 2.1860809326171875, 0.15988922119140625, -0.1790618896484375, -0.026531219482421875, -0.010677337646484375, -0.7006187438964844, -0.6222572326660156, 2.1055145263671875, -0.3210182189941406, 1.6367568969726562, -0.27323150634765625, 0.08129501342773438, -0.6370582580566406, 0.10947036743164062, 14.209346771240234, 1.3031463623046875, 1.0570297241210938, -0.7139663696289062, -6.282890319824219, -0.5801353454589844, -0.05294036865234375, 37.405879974365234, -0.30808258056640625, 0.7937431335449219, 0.376617431640625, -0.9873199462890625, 0.6531906127929688, -0.17479324340820312, 0.7596626281738281, -0.15064239501953125, -0.08974075317382812, 0.3491020202636719, 0.6484947204589844, -0.02533721923828125, -0.04822540283203125, 2.3513641357421875, 37.333900451660156, 0.3841552734375, 0.302490234375, 2.7057571411132812, 0.4709053039550781, 0.55499267578125, -0.306884765625, 0.0487518310546875, -7.564888000488281, 0.8267745971679688, -1.9604911804199219, 37.44135284423828, 1.1681060791015625, 2.6940841674804688, 0.3029594421386719, 0.8226814270019531, 0.18235015869140625, 38.195899963378906, 0.11461257934570312, -6.413105010986328, 14.401519775390625, -0.4218788146972656, -0.7771987915039062, 0.2119903564453125, 1.1468696594238281, -0.016803741455078125, -15.584457397460938, 1.6095390319824219, -0.33734893798828125, -2.524242401123047, 1.0750732421875, -0.4949607849121094, -0.16562652587890625, -18.701553344726562, 0.4429893493652344, 0.5528373718261719, -0.7408103942871094, 0.17317962646484375, 0.7646217346191406, 0.3581733703613281, 0.3636322021484375, 0.7540664672851562, -0.15325164794921875, 0.60723876953125, 0.17462539672851562, -2.6293563842773438, -4.67626953125, -8.694610595703125, -0.17198944091796875, 1.1748695373535156, -0.6950149536132812, -0.022495269775390625, 2.162586212158203, 0.4174003601074219, -0.2820587158203125, -0.27440643310546875, -0.030788421630859375, -4.7762451171875, 0.6262016296386719, 0.8206138610839844, 1.5597267150878906, 0.368927001953125, 0.16828536987304688, 7.4061126708984375, 0.7826309204101562, -0.0924072265625, -1.4430389404296875, 0.7724380493164062, -3.3305206298828125, -0.14796829223632812, 0.8115158081054688, 0.8931655883789062, -0.5421791076660156, 13.62823486328125, 3.5090789794921875, -0.5149383544921875, 0.24776077270507812, 0.1654205322265625, 0.899139404296875, 0.5434799194335938, -0.0794830322265625, 0.20108795166015625, 1.2289657592773438, 1.1772727966308594, 0.2804679870605469, 40.97903060913086, 1.0167312622070312, 1.5632438659667969, -0.2573356628417969, 0.6159515380859375, -0.085235595703125, -54.542022705078125, 1.2927322387695312, 0.09608078002929688, -0.039752960205078125, -8.373046875, 6.029441833496094, -5.234867095947266, 0.3592643737792969, -2.8102188110351562, 1.816192626953125, 0.2653350830078125, 2.9157485961914062, 0.675201416015625, -0.7777175903320312, -0.548370361328125, 0.4894256591796875, -0.030788421630859375, -0.3826484680175781, 1.0482177734375, 8.225639343261719, 4.089992523193359, 0.8901596069335938, 1.0442657470703125, -0.006977081298828125, -14.839675903320312, -0.19393157958984375, 0.8933372497558594, -0.3059730529785156, -0.3005256652832031, 36.4891242980957, 1.2202796936035156, 0.10378265380859375, 0.12863540649414062, 0.8002586364746094, 0.6171836853027344, -4.0774688720703125, 0.18256759643554688, 11.659923553466797, 0.6447296142578125, 0.09822463989257812, 0.1349945068359375, -0.37618255615234375, -0.3340034484863281, -0.3182411193847656, 1.2487335205078125, 0.6056022644042969, -0.3908882141113281, 2.235565185546875, 0.222900390625, 1.2665290832519531, 0.6216773986816406, -5.5811767578125], "mean_td_error": 0.7155492305755615}}, "num_steps_sampled": 28000, "num_agent_steps_sampled": 56000, "num_steps_trained": 864256, "num_steps_trained_this_iter": 256, "num_agent_steps_trained": 1728512, "last_target_update_ts": 27712, "num_target_updates": 54}, "done": false, "episodes_total": 46, "training_iteration": 28, "trial_id": "e21e6_00000", "experiment_id": "434cd42d33fd4b638f17fc69fa01d74f", "date": "2022-06-14_19-11-37", "timestamp": 1655248297, "time_this_iter_s": 21.316645860671997, "time_total_s": 577.39231300354, "pid": 2568314, "hostname": "lambda-dual", "node_ip": "10.104.51.19", "config": {"num_workers": 2, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.0005, "train_batch_size": 256, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "cnet", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "1v1env", "observation_space": null, "action_space": null, "env_config": {}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": true, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": null, "evaluation_duration": 10, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"num_workers": 2, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.0005, "train_batch_size": 256, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "cnet", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "1v1env", "observation_space": null, "action_space": null, "env_config": {}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": false, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": null, "evaluation_duration": 10, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"explore": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": true, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": null, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 2, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"policy_01": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]], [[[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]], (3, 64, 64), float32)", "Discrete(7)", {}], "policy_02": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]], [[[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]], (3, 64, 64), float32)", "Discrete(7)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": "<function <lambda> at 0x7f1aa2ebe560>", "policies_to_train": ["policy_01"], "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": 100000, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": false, "hiddens": [], "double_q": false, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": true, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": null, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 2, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"policy_01": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]], [[[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]], (3, 64, 64), float32)", "Discrete(7)", {}], "policy_02": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]], [[[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]], (3, 64, 64), float32)", "Discrete(7)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": "<function <lambda> at 0x7f1aa2ebe560>", "policies_to_train": ["policy_01"], "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": 100000, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": false, "hiddens": [], "double_q": false, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "time_since_restore": 577.39231300354, "timesteps_since_restore": 7168, "iterations_since_restore": 28, "warmup_time": 45.46659183502197, "perf": {"cpu_util_percent": 24.473333333333336, "ram_util_percent": 11.240000000000002}}
{"episode_reward_max": 441.0, "episode_reward_min": 0.0, "episode_reward_mean": 31.8125, "episode_len_mean": 601.0, "episode_media": {}, "episodes_this_iter": 2, "policy_reward_min": {"policy_01": 0.0, "policy_02": 0.0}, "policy_reward_max": {"policy_01": 441.0, "policy_02": 441.0}, "policy_reward_mean": {"policy_01": 22.625, "policy_02": 9.1875}, "custom_metrics": {}, "hist_stats": {"episode_reward": [0.0, 204.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "episode_lengths": [601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601], "policy_policy_01_reward": [0.0, 204.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "policy_policy_02_reward": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.33691132771530313, "mean_inference_ms": 5.831664038379704, "mean_action_processing_ms": 0.08910747805142942, "mean_env_wait_ms": 8.110393282414515, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 2, "timesteps_total": 29000, "timesteps_this_iter": 256, "agent_timesteps_total": 58000, "timers": {"load_time_ms": 1.339, "load_throughput": 191210.368, "learn_time_ms": 11.663, "learn_throughput": 21949.651, "update_time_ms": 2.238}, "info": {"learner": {"policy_01": {"custom_metrics": {}, "learner_stats": {"mean_q": 55.63523864746094, "min_q": 33.70696258544922, "max_q": 321.4642028808594, "cur_lr": 0.0005}, "model": {}, "td_error": [0.42075347900390625, -0.2909889221191406, -1.1272773742675781, -1.7065544128417969, -0.4092674255371094, -0.27593994140625, -0.8120880126953125, 37.07558059692383, -0.11991500854492188, 0.5023689270019531, -1.0302886962890625, 0.30422210693359375, -3.3979415893554688, -1.2491836547851562, 37.81916427612305, -0.31986236572265625, -0.5332107543945312, 4.544185638427734, -0.4905586242675781, -0.6081504821777344, -3.5389862060546875, -0.266387939453125, -0.6082344055175781, -0.45891571044921875, -1.6661338806152344, -0.2442626953125, -0.5936164855957031, -0.2562446594238281, -0.14304351806640625, -0.5991058349609375, -0.21362686157226562, -0.46743011474609375, -1.6978836059570312, 0.0200042724609375, -0.5971488952636719, -0.7543411254882812, -4.494167327880859, 0.16998291015625, 17.016342163085938, -0.4539527893066406, -0.4349555969238281, -7.034328460693359, 29.339202880859375, -1.7451820373535156, -3.0228958129882812, 0.23868179321289062, -0.5813026428222656, -1.45703125, -0.5199089050292969, 0.13967514038085938, -0.9240989685058594, 0.03211212158203125, -0.8311691284179688, 37.98863220214844, -3.646465301513672, -0.9745445251464844, -1.0372390747070312, -1.1390609741210938, -8.76126480102539, 0.088134765625, -1.4048271179199219, -0.8178329467773438, -3.891082763671875, -1.74334716796875, -1.1457481384277344, 0.3505287170410156, -1.5719223022460938, 0.39380645751953125, -8.703155517578125, -0.73980712890625, -0.12379074096679688, 0.6309547424316406, -1.0726852416992188, -1.7962303161621094, -0.6505966186523438, 2.718189239501953, 0.3704566955566406, -0.02155303955078125, -1.4180755615234375, 0.03095245361328125, 0.05915069580078125, -4.013744354248047, -1.2306289672851562, -0.013416290283203125, 1.5378913879394531, -0.000202178955078125, -0.91583251953125, 0.4858360290527344, -1.0429801940917969, -0.6889152526855469, 0.028717041015625, -0.3854408264160156, -0.6923980712890625, -1.0593452453613281, -3.1672210693359375, -0.9359092712402344, -1.259674072265625, -1.1517677307128906, -0.25487518310546875, -3.168487548828125, 2.4284515380859375, -0.7390365600585938, -1.3568878173828125, -7.1260223388671875, -0.3010101318359375, -8.4256591796875, -0.44915008544921875, -0.7996635437011719, -0.20262527465820312, 1.2979354858398438, 36.38493728637695, -1.0279693603515625, 0.042217254638671875, 31.637664794921875, -0.9465789794921875, 0.07823562622070312, -0.6002845764160156, 2.884960174560547, -1.4413414001464844, -0.5848121643066406, -0.5577430725097656, 0.061344146728515625, 6.9145050048828125, 0.23047256469726562, -1.2372589111328125, -1.5138206481933594, 11.366943359375, -0.78167724609375, -13.67510986328125, -0.41229248046875, -0.15909576416015625, -3.0343055725097656, -0.020648956298828125, 35.48455047607422, -0.24642181396484375, -1.4047698974609375, -19.879348754882812, -2.0944976806640625, -0.2507591247558594, 0.05231475830078125, -0.6404495239257812, 0.18354034423828125, -0.9402008056640625, -0.8889579772949219, 5.043170928955078, -0.2360382080078125, -0.7572174072265625, 2.9988479614257812, -1.2320022583007812, -0.19412612915039062, -2.104949951171875, -0.4862861633300781, -0.7754249572753906, -0.6527099609375, 10.205924987792969, -0.28963470458984375, -0.7482452392578125, -0.10656356811523438, -1.3771247863769531, -0.4363441467285156, -7.91680908203125, 59.82366943359375, -0.7751045227050781, -2.8419418334960938, -0.12178421020507812, -1.3635139465332031, -0.8780174255371094, -1.7226066589355469, -0.37722015380859375, -0.3952484130859375, -2.10760498046875, -4.241069793701172, -0.7435989379882812, 0.47176361083984375, -1.9432144165039062, 0.4229316711425781, -8.413864135742188, 0.7591056823730469, -0.06061553955078125, -0.06508255004882812, 3.196514129638672, -2.835651397705078, -0.47779083251953125, 0.034580230712890625, -0.9297943115234375, 1.0772171020507812, -0.5456047058105469, -0.37384796142578125, 1.6503448486328125, -4.569858551025391, 0.3620719909667969, 0.11885452270507812, 38.90171813964844, -0.4960479736328125, -2.5539321899414062, -0.6997871398925781, 45.44203186035156, -0.46164703369140625, -0.83099365234375, -1.7990150451660156, -1.0074234008789062, -8.33642578125, 3.196514129638672, 11.91290283203125, -0.3531455993652344, -0.058185577392578125, -0.0582275390625, -0.6828155517578125, -0.3805656433105469, -1.8078384399414062, -0.18785858154296875, -0.10024642944335938, 8.034194946289062, 0.024913787841796875, -2.575183868408203, 0.018184661865234375, 17.616683959960938, -4.218818664550781, -0.4087715148925781, -0.3412322998046875, -1.6361541748046875, -0.5681610107421875, -0.24033737182617188, -0.0936431884765625, 31.637664794921875, -0.22065353393554688, -1.0895156860351562, -0.7238197326660156, -0.7321014404296875, -1.2250633239746094, -1.1046600341796875, -0.4741973876953125, -0.7320671081542969, -2.0442161560058594, 0.66033935546875, -0.12178421020507812, -0.4886932373046875, -1.3994941711425781, -0.16759490966796875, -0.9420356750488281, -0.7597312927246094, -1.5841484069824219, -0.3412322998046875, -0.6688575744628906, -0.4820137023925781, -0.6967926025390625, 1.6525001525878906, -0.6051025390625, 4.4207916259765625, -3.670166015625, -1.9362373352050781, -9.426797866821289, -0.472137451171875, -0.15458297729492188, 7.787681579589844, 0.052398681640625], "mean_td_error": 1.0361969470977783}}, "num_steps_sampled": 29000, "num_agent_steps_sampled": 58000, "num_steps_trained": 896256, "num_steps_trained_this_iter": 256, "num_agent_steps_trained": 1792512, "last_target_update_ts": 28720, "num_target_updates": 56}, "done": false, "episodes_total": 48, "training_iteration": 29, "trial_id": "e21e6_00000", "experiment_id": "434cd42d33fd4b638f17fc69fa01d74f", "date": "2022-06-14_19-11-59", "timestamp": 1655248319, "time_this_iter_s": 21.291513919830322, "time_total_s": 598.6838269233704, "pid": 2568314, "hostname": "lambda-dual", "node_ip": "10.104.51.19", "config": {"num_workers": 2, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.0005, "train_batch_size": 256, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "cnet", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "1v1env", "observation_space": null, "action_space": null, "env_config": {}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": true, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": null, "evaluation_duration": 10, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"num_workers": 2, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.0005, "train_batch_size": 256, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "cnet", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "1v1env", "observation_space": null, "action_space": null, "env_config": {}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": false, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": null, "evaluation_duration": 10, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"explore": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": true, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": null, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 2, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"policy_01": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]], [[[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]], (3, 64, 64), float32)", "Discrete(7)", {}], "policy_02": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]], [[[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]], (3, 64, 64), float32)", "Discrete(7)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": "<function <lambda> at 0x7f1aa2e9a200>", "policies_to_train": ["policy_01"], "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": 100000, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": false, "hiddens": [], "double_q": false, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": true, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": null, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 2, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"policy_01": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]], [[[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]], (3, 64, 64), float32)", "Discrete(7)", {}], "policy_02": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]], [[[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]], (3, 64, 64), float32)", "Discrete(7)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": "<function <lambda> at 0x7f1aa2e9a200>", "policies_to_train": ["policy_01"], "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": 100000, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": false, "hiddens": [], "double_q": false, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "time_since_restore": 598.6838269233704, "timesteps_since_restore": 7424, "iterations_since_restore": 29, "warmup_time": 45.46659183502197, "perf": {"cpu_util_percent": 24.603225806451615, "ram_util_percent": 11.335483870967737}}
{"episode_reward_max": 441.0, "episode_reward_min": 0.0, "episode_reward_mean": 31.8125, "episode_len_mean": 601.0, "episode_media": {}, "episodes_this_iter": 0, "policy_reward_min": {"policy_01": 0.0, "policy_02": 0.0}, "policy_reward_max": {"policy_01": 441.0, "policy_02": 441.0}, "policy_reward_mean": {"policy_01": 22.625, "policy_02": 9.1875}, "custom_metrics": {}, "hist_stats": {"episode_reward": [0.0, 204.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "episode_lengths": [601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601], "policy_policy_01_reward": [0.0, 204.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "policy_policy_02_reward": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.33691132771530313, "mean_inference_ms": 5.831664038379704, "mean_action_processing_ms": 0.08910747805142942, "mean_env_wait_ms": 8.110393282414515, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 2, "timesteps_total": 30000, "timesteps_this_iter": 256, "agent_timesteps_total": 60000, "timers": {"load_time_ms": 1.309, "load_throughput": 195617.02, "learn_time_ms": 11.363, "learn_throughput": 22528.494, "update_time_ms": 2.174}, "info": {"learner": {"policy_01": {"custom_metrics": {}, "learner_stats": {"mean_q": 60.145999908447266, "min_q": 29.509319305419922, "max_q": 322.70013427734375, "cur_lr": 0.0005}, "model": {}, "td_error": [-0.54327392578125, -0.6675758361816406, -2.6702842712402344, -4.0353851318359375, -1.0217742919921875, -0.7091560363769531, -1.1871337890625, -0.3260002136230469, 0.8970832824707031, -0.4272041320800781, -1.740997314453125, 1.577789306640625, -1.6283721923828125, -0.41233062744140625, -0.13832855224609375, -23.055156707763672, 0.07442855834960938, 0.95367431640625, -0.05560302734375, 0.4509468078613281, -1.5447006225585938, -0.6264419555664062, 0.8969192504882812, -0.4473304748535156, 2.5863494873046875, 0.3307647705078125, -0.8502426147460938, -0.14153671264648438, 0.24704742431640625, 0.1776885986328125, -1.2768440246582031, -0.4578399658203125, -0.2795066833496094, -7.45196533203125, 0.8442306518554688, -0.29047393798828125, -1.3449897766113281, -0.9228439331054688, -0.6208229064941406, -1.3740882873535156, -3.5129852294921875, -0.5062522888183594, 0.20959091186523438, 0.5271835327148438, 0.24361419677734375, 0.18521499633789062, -0.16048431396484375, 51.59816360473633, 23.362773895263672, 0.33473968505859375, 0.036041259765625, -0.22025299072265625, 0.15386581420898438, -0.24506378173828125, -0.4704551696777344, -0.9042701721191406, -0.7120094299316406, 1.4058723449707031, -0.060333251953125, -2.9246597290039062, 0.3239402770996094, 37.56032943725586, -0.7060356140136719, -1.4341049194335938, 1.6269607543945312, -0.3983116149902344, 7.000659942626953, -0.23176193237304688, 1.0255165100097656, -1.5125656127929688, -0.00444793701171875, -4.3225555419921875, -4.692768096923828, -4.590476989746094, 0.4985084533691406, 1.7676239013671875, -0.09851455688476562, -2.006195068359375, 0.13093948364257812, 5.256801605224609, 11.426761627197266, 0.16153717041015625, -1.0099563598632812, -0.08980560302734375, -0.5522499084472656, -0.6392135620117188, 0.22614669799804688, -0.8301963806152344, -0.6940498352050781, -0.5151214599609375, 0.06841659545898438, -0.2687835693359375, -0.573822021484375, -0.06412124633789062, -4.5684661865234375, 0.11754608154296875, -1.1302223205566406, -8.029945373535156, -5.6489715576171875, -0.44768524169921875, -0.19879150390625, -0.05504608154296875, -0.9765281677246094, -0.49025726318359375, -0.4781303405761719, 0.1415252685546875, 0.09889984130859375, -1.3936347961425781, 1.101654052734375, 0.2768096923828125, -1.8398780822753906, -0.09383773803710938, -9.961944580078125, -0.09613037109375, -0.4334831237792969, 0.399749755859375, 0.27870941162109375, 0.17598342895507812, 0.4482002258300781, -11.167736053466797, 0.061275482177734375, 11.62109375, 0.3967781066894531, 4.682456970214844, -0.6104278564453125, -0.1501922607421875, 0.23766708374023438, 16.01892852783203, -1.7286109924316406, -2.333221435546875, -0.8151206970214844, 0.027111053466796875, -2.2724151611328125, -0.6747398376464844, -0.0871124267578125, -0.3372459411621094, 0.00131988525390625, 0.19256973266601562, -0.16498947143554688, -0.4126091003417969, -8.112930297851562, 39.13792037963867, 0.4999237060546875, -1.0484428405761719, 2.146808624267578, -3.7837181091308594, 0.8273468017578125, -0.17142868041992188, -8.7789306640625, -1.8816375732421875, -0.03560638427734375, -5.308433532714844, 21.628515243530273, 1.2836418151855469, 51.59816360473633, 1.2285842895507812, 8.426803588867188, -0.0959930419921875, 13.243476867675781, 0.8086967468261719, -0.4273643493652344, 0.01966094970703125, 0.6325187683105469, -1.4290122985839844, 0.4985084533691406, 0.19795989990234375, 120.90144348144531, 0.2689247131347656, -0.0693206787109375, -0.104766845703125, -1.1088714599609375, -2.223011016845703, -0.5157928466796875, 0.0125274658203125, 3.8031692504882812, -2.8112106323242188, -4.14251708984375, 0.20217132568359375, -0.3159751892089844, -5.304115295410156, 4.296630859375, -16.46788787841797, 0.14581680297851562, 0.40727996826171875, 0.5792388916015625, -0.8708572387695312, -0.8966484069824219, -1.1177825927734375, -1.2272834777832031, -2.1298828125, -0.6876068115234375, -0.17327880859375, -1.5371513366699219, 0.6335525512695312, 0.4548492431640625, -20.640869140625, -1.38995361328125, 0.14841461181640625, -0.221466064453125, -0.19849014282226562, -0.15211868286132812, -24.03427505493164, 0.12787628173828125, 40.61895751953125, -0.7041358947753906, -1.6980400085449219, -0.6255416870117188, 0.30220794677734375, 0.5724334716796875, 1.4873199462890625, -0.247711181640625, 3.4370040893554688, -1.4325027465820312, -0.549560546875, -2.857696533203125, 0.570159912109375, -5.2939605712890625, -0.315093994140625, 0.3342704772949219, 0.19159317016601562, -20.640869140625, -0.07232284545898438, -6.132354736328125, -5.998752593994141, -0.2691459655761719, -1.5298233032226562, 6.541343688964844, -0.044933319091796875, 0.1232452392578125, 8.441131591796875, 0.917205810546875, 0.00376129150390625, -16.164413452148438, -1.7659378051757812, -5.2939605712890625, -0.4998931884765625, -0.9646835327148438, -9.433563232421875, -0.7412834167480469, 1.60009765625, -0.6450119018554688, -1.4275741577148438, -0.5688705444335938, -0.6153335571289062, 1.2457351684570312, 9.25198745727539, -1.7673568725585938, 1.7423782348632812, 0.17723846435546875, -2.1298828125, 0.23421859741210938, -0.367645263671875, -33.087493896484375, -20.577682495117188, -0.16332244873046875, 0.5933761596679688], "mean_td_error": 0.5123207569122314}}, "num_steps_sampled": 30000, "num_agent_steps_sampled": 60000, "num_steps_trained": 928256, "num_steps_trained_this_iter": 256, "num_agent_steps_trained": 1856512, "last_target_update_ts": 29728, "num_target_updates": 58}, "done": false, "episodes_total": 48, "training_iteration": 30, "trial_id": "e21e6_00000", "experiment_id": "434cd42d33fd4b638f17fc69fa01d74f", "date": "2022-06-14_19-12-20", "timestamp": 1655248340, "time_this_iter_s": 21.19751286506653, "time_total_s": 619.8813397884369, "pid": 2568314, "hostname": "lambda-dual", "node_ip": "10.104.51.19", "config": {"num_workers": 2, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.0005, "train_batch_size": 256, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "cnet", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "1v1env", "observation_space": null, "action_space": null, "env_config": {}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": true, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": null, "evaluation_duration": 10, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"num_workers": 2, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.0005, "train_batch_size": 256, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "cnet", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "1v1env", "observation_space": null, "action_space": null, "env_config": {}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": false, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": null, "evaluation_duration": 10, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"explore": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": true, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": null, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 2, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"policy_01": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]], [[[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]], (3, 64, 64), float32)", "Discrete(7)", {}], "policy_02": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]], [[[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]], (3, 64, 64), float32)", "Discrete(7)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": "<function <lambda> at 0x7f1aa3726b00>", "policies_to_train": ["policy_01"], "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": 100000, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": false, "hiddens": [], "double_q": false, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": true, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": null, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 2, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"policy_01": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]], [[[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]], (3, 64, 64), float32)", "Discrete(7)", {}], "policy_02": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]], [[[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]], (3, 64, 64), float32)", "Discrete(7)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": "<function <lambda> at 0x7f1aa3726b00>", "policies_to_train": ["policy_01"], "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": 100000, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": false, "hiddens": [], "double_q": false, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "time_since_restore": 619.8813397884369, "timesteps_since_restore": 7680, "iterations_since_restore": 30, "warmup_time": 45.46659183502197, "perf": {"cpu_util_percent": 24.753333333333337, "ram_util_percent": 11.413333333333332}}
{"episode_reward_max": 441.0, "episode_reward_min": 0.0, "episode_reward_mean": 30.54, "episode_len_mean": 601.0, "episode_media": {}, "episodes_this_iter": 2, "policy_reward_min": {"policy_01": 0.0, "policy_02": 0.0}, "policy_reward_max": {"policy_01": 441.0, "policy_02": 441.0}, "policy_reward_mean": {"policy_01": 21.72, "policy_02": 8.82}, "custom_metrics": {}, "hist_stats": {"episode_reward": [0.0, 204.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "episode_lengths": [601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601], "policy_policy_01_reward": [0.0, 204.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "policy_policy_02_reward": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.33674652015929263, "mean_inference_ms": 5.829508793354389, "mean_action_processing_ms": 0.08908597319456313, "mean_env_wait_ms": 8.099578132561271, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 2, "timesteps_total": 31000, "timesteps_this_iter": 256, "agent_timesteps_total": 62000, "timers": {"load_time_ms": 1.315, "load_throughput": 194652.446, "learn_time_ms": 11.867, "learn_throughput": 21572.56, "update_time_ms": 2.322}, "info": {"learner": {"policy_01": {"custom_metrics": {}, "learner_stats": {"mean_q": 68.09567260742188, "min_q": 38.755470275878906, "max_q": 389.27923583984375, "cur_lr": 0.0005}, "model": {}, "td_error": [0.592803955078125, 1.0814056396484375, 0.8347816467285156, 6.273792266845703, 12.195526123046875, -0.45700836181640625, 2.4362335205078125, -22.61919403076172, 0.9258155822753906, 0.6455726623535156, 0.16861724853515625, 2.0169105529785156, 0.905609130859375, 1.60662841796875, 8.013053894042969, -3.1369476318359375, 1.857330322265625, 0.0434722900390625, 4.029308319091797, -0.9644279479980469, 0.6132469177246094, 0.40531158447265625, 1.9340858459472656, -2.258514404296875, 0.47391510009765625, 0.11981201171875, 2.7746047973632812, 0.9061813354492188, -0.23207473754882812, 2.9198684692382812, 0.851043701171875, -3.4775962829589844, 0.4510841369628906, 2.5877609252929688, 6.414886474609375, 9.987060546875, 1.3346748352050781, 2.2884674072265625, 0.7418670654296875, 0.6000709533691406, -3.69091796875, 1.8487358093261719, 2.4177627563476562, 0.6940574645996094, 0.9820213317871094, 8.8330078125, 1.3778877258300781, 0.38726043701171875, -3.530071258544922, 0.6668510437011719, 0.5481834411621094, -2.741588592529297, -0.2923431396484375, 0.1143341064453125, 0.12146759033203125, 1.7620162963867188, 0.8805160522460938, 1.5078544616699219, 1.547637939453125, 1.4017753601074219, 0.92291259765625, 12.71875, 22.080810546875, 8.976966857910156, 0.5084266662597656, 1.4776420593261719, 10.170875549316406, -7.14593505859375, 8.847808837890625, -0.13394927978515625, 4.376213073730469, 2.1859474182128906, -9.58001708984375, -0.12260818481445312, 4.863777160644531, 1.4518852233886719, 2.2609100341796875, 0.4510841369628906, 0.874664306640625, -14.707221984863281, -0.2565765380859375, 3.167022705078125, 0.5926589965820312, 1.0116500854492188, 0.8638877868652344, 1.8969955444335938, 41.38013458251953, 1.8632011413574219, -4.5339508056640625, 0.8461036682128906, 0.8725814819335938, 0.9640998840332031, 1.2891311645507812, 1.4853057861328125, 1.8960990905761719, 0.9817008972167969, 8.62668228149414, -2.9196815490722656, 1.3859634399414062, 0.8320426940917969, 0.12875747680664062, 2.3902359008789062, 0.6041679382324219, 1.1846694946289062, 0.8809700012207031, 1.2094154357910156, 1.7807884216308594, 0.3216514587402344, -1.0762214660644531, 1.5736770629882812, -1.5835685729980469, 1.4757575988769531, 1.3246574401855469, 0.9229164123535156, 1.1414566040039062, -2.0605087280273438, 2.235004425048828, 0.5662460327148438, 2.182056427001953, 1.6091346740722656, 2.7746047973632812, 2.2172317504882812, 0.6420364379882812, -2.7282943725585938, 1.3757553100585938, 0.12895584106445312, 8.86355972290039, 0.34165191650390625, -1.3001518249511719, 1.8206062316894531, 0.2841911315917969, -4.3179168701171875, -7.14593505859375, 0.041484832763671875, -0.9763984680175781, 0.8554458618164062, 1.8904304504394531, 1.0177268981933594, 1.5338592529296875, 0.9667282104492188, 2.287017822265625, 1.3749618530273438, 0.3870506286621094, 38.848793029785156, 0.05039215087890625, 40.25628662109375, 2.401500701904297, 1.7462882995605469, -4.625297546386719, -1.422882080078125, 3.6951866149902344, 1.3748359680175781, 1.3073844909667969, 2.111297607421875, 0.906097412109375, 0.34687042236328125, 0.3929557800292969, 2.4177589416503906, 2.8178634643554688, 3.949432373046875, 0.24777603149414062, -14.707221984863281, 6.797756195068359, -0.5872421264648438, -0.14118576049804688, -1.6291236877441406, 2.4046249389648438, 0.60675048828125, 0.3429450988769531, -0.4154052734375, 1.4572563171386719, 1.206787109375, -0.5916252136230469, 0.03908538818359375, 1.3258171081542969, 3.0438308715820312, 0.7044754028320312, 5.258964538574219, 1.6227073669433594, -2.8986053466796875, 0.2987937927246094, 5.966976165771484, 2.213031768798828, 0.37392425537109375, 0.9105796813964844, 24.492584228515625, 1.5176048278808594, 1.5447845458984375, 0.9271621704101562, -0.2689094543457031, 0.54949951171875, 0.8998527526855469, 0.3837394714355469, 0.38780975341796875, 1.4574699401855469, -0.6354446411132812, 0.26009368896484375, 1.8976020812988281, 0.838653564453125, 4.449241638183594, 2.0790367126464844, -13.2806396484375, 0.7542152404785156, 7.9779205322265625, 0.41613006591796875, 0.2884674072265625, 1.0062522888183594, 1.9078445434570312, 0.3282661437988281, -0.00839996337890625, 0.2741737365722656, 0.2889823913574219, -1.1328659057617188, 0.9489631652832031, -0.27508544921875, 0.8301963806152344, 1.2223243713378906, 0.2972984313964844, 39.13719940185547, 0.8463401794433594, 0.7649574279785156, 1.0716972351074219, 1.653961181640625, 9.938552856445312, -1.3363037109375, -50.707275390625, 1.8059158325195312, 0.529083251953125, 0.09275054931640625, 0.6939697265625, 1.4302024841308594, 8.064834594726562, 2.22979736328125, 1.4172744750976562, 0.8048057556152344, 0.4124717712402344, 1.0976829528808594, 0.4800300598144531, 0.4205131530761719, 39.94820022583008, 0.4704170227050781, -5.343986511230469, -0.42779541015625, 0.892303466796875, 19.555252075195312, 0.6332931518554688, 10.340972900390625, -0.17786407470703125, 8.04705810546875, 1.2207565307617188, 16.373138427734375, 2.0668907165527344, 1.0562400817871094, 2.119304656982422, 1.8143730163574219, 0.3353233337402344], "mean_td_error": 1.8570897579193115}}, "num_steps_sampled": 31000, "num_agent_steps_sampled": 62000, "num_steps_trained": 960256, "num_steps_trained_this_iter": 256, "num_agent_steps_trained": 1920512, "last_target_update_ts": 30736, "num_target_updates": 60}, "done": false, "episodes_total": 50, "training_iteration": 31, "trial_id": "e21e6_00000", "experiment_id": "434cd42d33fd4b638f17fc69fa01d74f", "date": "2022-06-14_19-12-41", "timestamp": 1655248361, "time_this_iter_s": 21.29231905937195, "time_total_s": 641.1736588478088, "pid": 2568314, "hostname": "lambda-dual", "node_ip": "10.104.51.19", "config": {"num_workers": 2, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.0005, "train_batch_size": 256, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "cnet", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "1v1env", "observation_space": null, "action_space": null, "env_config": {}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": true, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": null, "evaluation_duration": 10, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"num_workers": 2, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.0005, "train_batch_size": 256, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "cnet", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "1v1env", "observation_space": null, "action_space": null, "env_config": {}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": false, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": null, "evaluation_duration": 10, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"explore": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": true, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": null, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 2, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"policy_01": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]], [[[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]], (3, 64, 64), float32)", "Discrete(7)", {}], "policy_02": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]], [[[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]], (3, 64, 64), float32)", "Discrete(7)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": "<function <lambda> at 0x7f1aa2ed3320>", "policies_to_train": ["policy_01"], "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": 100000, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": false, "hiddens": [], "double_q": false, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": true, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": null, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 2, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"policy_01": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]], [[[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]], (3, 64, 64), float32)", "Discrete(7)", {}], "policy_02": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]], [[[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]], (3, 64, 64), float32)", "Discrete(7)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": "<function <lambda> at 0x7f1aa2ed3320>", "policies_to_train": ["policy_01"], "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": 100000, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": false, "hiddens": [], "double_q": false, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "time_since_restore": 641.1736588478088, "timesteps_since_restore": 7936, "iterations_since_restore": 31, "warmup_time": 45.46659183502197, "perf": {"cpu_util_percent": 24.34000000000001, "ram_util_percent": 11.5}}
{"episode_reward_max": 441.0, "episode_reward_min": 0.0, "episode_reward_mean": 29.365384615384617, "episode_len_mean": 601.0, "episode_media": {}, "episodes_this_iter": 2, "policy_reward_min": {"policy_01": 0.0, "policy_02": 0.0}, "policy_reward_max": {"policy_01": 441.0, "policy_02": 441.0}, "policy_reward_mean": {"policy_01": 20.884615384615383, "policy_02": 8.48076923076923}, "custom_metrics": {}, "hist_stats": {"episode_reward": [0.0, 204.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "episode_lengths": [601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601], "policy_policy_01_reward": [0.0, 204.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "policy_policy_02_reward": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.3366085885353446, "mean_inference_ms": 5.827439868042555, "mean_action_processing_ms": 0.08906550279272414, "mean_env_wait_ms": 8.089363247010265, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 2, "timesteps_total": 32000, "timesteps_this_iter": 256, "agent_timesteps_total": 64000, "timers": {"load_time_ms": 1.32, "load_throughput": 194005.316, "learn_time_ms": 11.85, "learn_throughput": 21603.159, "update_time_ms": 2.267}, "info": {"learner": {"policy_01": {"custom_metrics": {}, "learner_stats": {"mean_q": 60.86663055419922, "min_q": 37.751182556152344, "max_q": 310.7313232421875, "cur_lr": 0.0005}, "model": {}, "td_error": [38.2576789855957, -1.7566108703613281, 0.25122833251953125, -0.4943389892578125, -2.189208984375, 2.961597442626953, 0.5608863830566406, -0.6595420837402344, -0.298828125, 1.5132408142089844, -0.5103492736816406, 0.02037811279296875, -0.7097091674804688, -16.59845733642578, 2.609783172607422, -0.0757293701171875, 0.355377197265625, 17.1898193359375, 0.8589935302734375, -1.8595428466796875, 0.4975242614746094, -0.79248046875, -0.4285316467285156, 0.367279052734375, 2.6212730407714844, -1.6204605102539062, 0.07576370239257812, 0.8054161071777344, 1.1314315795898438, -1.5383415222167969, -0.5077857971191406, -0.43621063232421875, -0.5265464782714844, -1.1105842590332031, -0.20650863647460938, -0.001972198486328125, -1.8184890747070312, -0.058811187744140625, -0.9978218078613281, 12.618179321289062, -1.7808265686035156, 0.11627197265625, -0.5920028686523438, -0.795989990234375, -1.4524040222167969, -1.1178665161132812, -0.02707672119140625, -0.6363449096679688, -3.12109375, 6.555732727050781, -0.2824516296386719, 1.0228080749511719, 6.141448974609375, -1.3178787231445312, -0.5942497253417969, -0.5771102905273438, 0.9282989501953125, 5.81195068359375, 3.5460281372070312, 0.19166946411132812, -0.17295074462890625, -0.13820648193359375, -2.0760574340820312, 1.063323974609375, -0.7903709411621094, 0.2911262512207031, -0.18198776245117188, 0.3196563720703125, 3.1242141723632812, 0.23570632934570312, 11.27490234375, -0.16066741943359375, -0.9276313781738281, 5.454151153564453, -0.8187637329101562, 0.5334815979003906, 1.0338325500488281, 1.3208770751953125, 0.3228950500488281, 0.6464118957519531, 0.15584182739257812, 4.057579040527344, 38.601051330566406, 0.497772216796875, 0.3739738464355469, 0.07646560668945312, 0.4907035827636719, 0.078216552734375, -1.9449043273925781, 0.9690971374511719, -1.1295166015625, -1.2530670166015625, -7.972625732421875, -1.10833740234375, 3.7736053466796875, -2.142993927001953, 1.6285514831542969, -0.215972900390625, 6.313030242919922, 0.32952117919921875, -3.131641387939453, 2.3448448181152344, -1.0130157470703125, 0.8259773254394531, -0.3332366943359375, -9.81707763671875, 0.37667083740234375, 38.29316329956055, -0.5497360229492188, 0.22074508666992188, 6.664173126220703, -0.010448455810546875, -0.2522735595703125, 4.03070068359375, 1.266143798828125, 0.64013671875, -0.4783821105957031, -0.18198776245117188, 0.3802146911621094, 0.54541015625, 0.17333221435546875, 0.33404541015625, -1.2687416076660156, -2.184246063232422, 0.9073944091796875, -0.22011566162109375, -1.7488021850585938, 2.335205078125, -0.3620338439941406, 41.78108215332031, -0.18357467651367188, -4.594085693359375, 9.115814208984375, -0.6540718078613281, 1.1553421020507812, 1.0893630981445312, -0.5079193115234375, -0.2644844055175781, -0.7523193359375, -1.2557792663574219, -0.7367630004882812, 6.310943603515625, -2.4531784057617188, 0.57415771484375, -0.23543548583984375, 40.553314208984375, -0.3081703186035156, 0.056976318359375, -1.7149810791015625, 1.7327232360839844, -0.4610481262207031, -4.1085968017578125, -0.7653236389160156, -1.6389732360839844, 0.16320037841796875, 0.6270408630371094, -1.9935684204101562, -7.569431304931641, 0.98028564453125, -1.06585693359375, -1.8315200805664062, -1.6759796142578125, 5.254463195800781, 0.3947105407714844, -0.01047515869140625, 0.3646278381347656, -1.2797393798828125, 6.45501708984375, 0.6547508239746094, -1.6490325927734375, -1.0123748779296875, -1.1390419006347656, -0.5574302673339844, 1.7500190734863281, -0.4028167724609375, -1.0061378479003906, 23.16693115234375, 0.7350082397460938, -0.2812919616699219, -1.1820640563964844, -1.0352783203125, -0.5106887817382812, 2.3563613891601562, 41.78108215332031, -0.21196746826171875, 0.5603370666503906, -1.7412261962890625, -1.3924903869628906, 0.5866012573242188, 1.3427581787109375, -1.8218765258789062, 0.5639991760253906, 0.891143798828125, 13.2823486328125, -1.3099708557128906, -2.0991287231445312, 3.498065948486328, -0.08081436157226562, 6.331378936767578, -0.015491485595703125, 1.0815887451171875, -0.33765411376953125, -0.4045295715332031, 0.12701416015625, -0.396270751953125, -0.417816162109375, 2.494781494140625, 3.17291259765625, -0.45147705078125, -0.8660964965820312, -0.8809280395507812, 2.936237335205078, 43.404197692871094, -0.3332366943359375, 0.5053558349609375, 2.4893455505371094, -0.4767265319824219, 42.29354476928711, 0.13742828369140625, -0.4350776672363281, -0.7136192321777344, 1.5992279052734375, 1.524017333984375, -0.01047515869140625, -0.5271759033203125, 0.3760414123535156, 0.3235359191894531, -0.0504913330078125, -9.917102813720703, 0.5978050231933594, -0.7529220581054688, 1.3607063293457031, 14.126609802246094, -0.06912994384765625, 0.4239959716796875, 0.97991943359375, 2.6593475341796875, -0.18433380126953125, -1.1586875915527344, 0.8828315734863281, -0.19303512573242188, -0.13726806640625, -0.5889739990234375, -2.105823516845703, -10.930206298828125, -3.4635658264160156, -0.6617088317871094, 0.2642059326171875, -0.24128341674804688, -0.673797607421875, 0.9070663452148438, -1.49688720703125, 0.622222900390625, 1.0367851257324219, -11.836700439453125, -0.30178070068359375], "mean_td_error": 1.5435454845428467}}, "num_steps_sampled": 32000, "num_agent_steps_sampled": 64000, "num_steps_trained": 992256, "num_steps_trained_this_iter": 256, "num_agent_steps_trained": 1984512, "last_target_update_ts": 31744, "num_target_updates": 62}, "done": false, "episodes_total": 52, "training_iteration": 32, "trial_id": "e21e6_00000", "experiment_id": "434cd42d33fd4b638f17fc69fa01d74f", "date": "2022-06-14_19-13-03", "timestamp": 1655248383, "time_this_iter_s": 21.38599395751953, "time_total_s": 662.5596528053284, "pid": 2568314, "hostname": "lambda-dual", "node_ip": "10.104.51.19", "config": {"num_workers": 2, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.0005, "train_batch_size": 256, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "cnet", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "1v1env", "observation_space": null, "action_space": null, "env_config": {}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": true, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": null, "evaluation_duration": 10, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"num_workers": 2, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.0005, "train_batch_size": 256, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "cnet", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "1v1env", "observation_space": null, "action_space": null, "env_config": {}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": false, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": null, "evaluation_duration": 10, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"explore": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": true, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": null, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 2, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"policy_01": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]], [[[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]], (3, 64, 64), float32)", "Discrete(7)", {}], "policy_02": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]], [[[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]], (3, 64, 64), float32)", "Discrete(7)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": "<function <lambda> at 0x7f1aa2ebe560>", "policies_to_train": ["policy_01"], "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": 100000, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": false, "hiddens": [], "double_q": false, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": true, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": null, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 2, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"policy_01": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]], [[[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]], (3, 64, 64), float32)", "Discrete(7)", {}], "policy_02": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]], [[[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]], (3, 64, 64), float32)", "Discrete(7)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": "<function <lambda> at 0x7f1aa2ebe560>", "policies_to_train": ["policy_01"], "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": 100000, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": false, "hiddens": [], "double_q": false, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "time_since_restore": 662.5596528053284, "timesteps_since_restore": 8192, "iterations_since_restore": 32, "warmup_time": 45.46659183502197, "perf": {"cpu_util_percent": 24.377419354838707, "ram_util_percent": 11.580645161290327}}
{"episode_reward_max": 441.0, "episode_reward_min": 0.0, "episode_reward_mean": 28.27777777777778, "episode_len_mean": 601.0, "episode_media": {}, "episodes_this_iter": 2, "policy_reward_min": {"policy_01": 0.0, "policy_02": 0.0}, "policy_reward_max": {"policy_01": 441.0, "policy_02": 441.0}, "policy_reward_mean": {"policy_01": 20.11111111111111, "policy_02": 8.166666666666666}, "custom_metrics": {}, "hist_stats": {"episode_reward": [0.0, 204.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "episode_lengths": [601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601], "policy_policy_01_reward": [0.0, 204.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "policy_policy_02_reward": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.3364937908178136, "mean_inference_ms": 5.8255414108396755, "mean_action_processing_ms": 0.08904736926666015, "mean_env_wait_ms": 8.079810558836924, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 2, "timesteps_total": 33000, "timesteps_this_iter": 256, "agent_timesteps_total": 66000, "timers": {"load_time_ms": 1.341, "load_throughput": 190880.649, "learn_time_ms": 11.976, "learn_throughput": 21375.526, "update_time_ms": 2.416}, "info": {"learner": {"policy_01": {"custom_metrics": {}, "learner_stats": {"mean_q": 71.7267837524414, "min_q": 40.87370300292969, "max_q": 405.87939453125, "cur_lr": 0.0005}, "model": {}, "td_error": [1.1820945739746094, -0.48270416259765625, -0.5233268737792969, -1.4891395568847656, -0.24524307250976562, -0.85113525390625, 0.062137603759765625, -0.3974151611328125, 1.0083770751953125, -0.5222282409667969, -0.5114250183105469, -1.2304000854492188, 0.6473388671875, 2.0111961364746094, 0.66571044921875, -0.7097206115722656, -8.208251953125, -3.7967147827148438, -0.7386398315429688, 1.6048049926757812, -0.5885238647460938, 0.42828369140625, -0.22368240356445312, 0.13039398193359375, -0.5902862548828125, 11.965774536132812, 0.1272125244140625, -0.10889816284179688, -0.18814849853515625, 0.23457717895507812, -8.141197204589844, -0.6670570373535156, -18.25262451171875, -0.639739990234375, 0.0452728271484375, -1.8744926452636719, -0.01799774169921875, 43.8938102722168, -8.435775756835938, -0.8571395874023438, -98.16653442382812, 1.11175537109375, -0.8270797729492188, 0.10012435913085938, -7.958343505859375, -4.093364715576172, -0.1876678466796875, -0.8147697448730469, -2.861114501953125, -0.9581871032714844, -0.7700881958007812, -0.7193870544433594, 0.058368682861328125, 3.032501220703125, -9.143329620361328, 4.7124176025390625, 0.09909820556640625, -0.22813796997070312, 1.6601104736328125, 1.212890625, -0.09312820434570312, 16.164276123046875, 0.07888031005859375, -0.39864349365234375, -1.7277603149414062, -0.07765579223632812, -0.9098167419433594, -0.19124603271484375, -0.49839019775390625, -0.70599365234375, -34.680599212646484, 35.760345458984375, -1.2126083374023438, -0.7097206115722656, -0.7081832885742188, 0.17497634887695312, -1.0817756652832031, 15.418891906738281, -1.0328750610351562, -15.785186767578125, 0.264404296875, 1.4583168029785156, 0.001811981201171875, -0.15152359008789062, -1.5157623291015625, 2.4722328186035156, -0.4864540100097656, 0.3115730285644531, -3.657501220703125, -2.5799560546875, 1.0992546081542969, 0.20515060424804688, -0.69915771484375, 0.2584419250488281, -0.4709320068359375, -32.95025634765625, 5.407955169677734, 29.656036376953125, -15.980072021484375, 1.9571647644042969, 1.9330902099609375, -0.021942138671875, -0.47071075439453125, -0.187408447265625, -0.40326690673828125, 0.19980239868164062, -1.8783340454101562, -0.4932403564453125, 6.0231475830078125, 0.6112632751464844, 2.0296707153320312, -0.8808860778808594, -0.2931671142578125, -1.0066413879394531, -0.20269775390625, -0.3539695739746094, -0.10265350341796875, 0.258514404296875, -0.5991477966308594, 0.46323394775390625, 0.33562469482421875, 0.15523910522460938, -1.8550834655761719, 4.789825439453125, -0.20824050903320312, 0.273406982421875, 0.273406982421875, -3.8706512451171875, -0.8096580505371094, -32.14013671875, 7.026123046875, 3.31005859375, -1.0798110961914062, 0.03472137451171875, -1.0098876953125, 2.0225677490234375, 0.011196136474609375, 3.8329811096191406, -0.6430702209472656, 1.4170341491699219, -0.035472869873046875, -0.07115936279296875, -0.8984527587890625, 1.2105598449707031, -0.9313163757324219, -2.1710357666015625, -0.6128883361816406, -1.254425048828125, 0.8520355224609375, -0.7202529907226562, -0.2856407165527344, -20.460968017578125, -0.46533966064453125, -34.778778076171875, -0.2508125305175781, 0.03899383544921875, 0.32958221435546875, 1.0781593322753906, -0.8861427307128906, -0.6792335510253906, -1.3477706909179688, 0.46492767333984375, -1.0873069763183594, -0.9095573425292969, -1.2545013427734375, 0.057476043701171875, -3.340362548828125, -0.08312606811523438, -1.9914093017578125, 10.879798889160156, -1.2330589294433594, -5.641910552978516, -0.5951576232910156, 1.43560791015625, 44.34201431274414, 0.331390380859375, 0.2710380554199219, 0.028415679931640625, -0.5431480407714844, -1.0698394775390625, -0.6123619079589844, 0.9176979064941406, 0.10477066040039062, -1.5436897277832031, -0.253997802734375, -1.7934074401855469, -8.302806854248047, -0.6741409301757812, -0.11534881591796875, -0.7104034423828125, -0.7241554260253906, -0.20273208618164062, -0.7550201416015625, 0.5464019775390625, 1.7452011108398438, -1.651702880859375, 0.059917449951171875, -1.897430419921875, 1.0918235778808594, 1.3331260681152344, -0.1885528564453125, 0.9484176635742188, -0.12357330322265625, -0.05535125732421875, -22.433246612548828, -0.4931831359863281, 45.58746337890625, -0.158477783203125, -0.11692428588867188, -0.19150543212890625, 1.3019905090332031, 0.322601318359375, 0.4904289245605469, 0.5424270629882812, -10.428436279296875, 0.21988296508789062, 13.377727508544922, -2.790496826171875, 0.7057037353515625, 48.49431610107422, 40.87335968017578, 24.000282287597656, -0.026042938232421875, -0.5955734252929688, -0.045192718505859375, -0.44417572021484375, -11.189849853515625, 0.5094566345214844, -0.07726669311523438, -0.9348297119140625, -0.6578102111816406, 0.35150146484375, 0.466644287109375, -9.647735595703125, 0.8323783874511719, 0.15251922607421875, 0.4614105224609375, -0.5091209411621094, 0.01663970947265625, -0.5402297973632812, -2.0032997131347656, -0.33742523193359375, -0.8351478576660156, -0.14685821533203125, 6.137592315673828, 0.00428009033203125, -4.976039886474609, 0.6978263854980469, -0.3494415283203125, 0.3748054504394531, -0.7996330261230469, 0.6303939819335938, 10.96173095703125, -0.7678794860839844, -0.5212593078613281, 0.340789794921875], "mean_td_error": -0.16191473603248596}}, "num_steps_sampled": 33000, "num_agent_steps_sampled": 66000, "num_steps_trained": 1024256, "num_steps_trained_this_iter": 256, "num_agent_steps_trained": 2048512, "last_target_update_ts": 32752, "num_target_updates": 64}, "done": false, "episodes_total": 54, "training_iteration": 33, "trial_id": "e21e6_00000", "experiment_id": "434cd42d33fd4b638f17fc69fa01d74f", "date": "2022-06-14_19-13-24", "timestamp": 1655248404, "time_this_iter_s": 21.45329737663269, "time_total_s": 684.0129501819611, "pid": 2568314, "hostname": "lambda-dual", "node_ip": "10.104.51.19", "config": {"num_workers": 2, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.0005, "train_batch_size": 256, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "cnet", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "1v1env", "observation_space": null, "action_space": null, "env_config": {}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": true, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": null, "evaluation_duration": 10, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"num_workers": 2, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.0005, "train_batch_size": 256, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "cnet", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "1v1env", "observation_space": null, "action_space": null, "env_config": {}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": false, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": null, "evaluation_duration": 10, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"explore": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": true, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": null, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 2, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"policy_01": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]], [[[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]], (3, 64, 64), float32)", "Discrete(7)", {}], "policy_02": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]], [[[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]], (3, 64, 64), float32)", "Discrete(7)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": "<function <lambda> at 0x7f1aa37648c0>", "policies_to_train": ["policy_01"], "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": 100000, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": false, "hiddens": [], "double_q": false, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": true, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": null, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 2, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"policy_01": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]], [[[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]], (3, 64, 64), float32)", "Discrete(7)", {}], "policy_02": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]], [[[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]], (3, 64, 64), float32)", "Discrete(7)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": "<function <lambda> at 0x7f1aa37648c0>", "policies_to_train": ["policy_01"], "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": 100000, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": false, "hiddens": [], "double_q": false, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "time_since_restore": 684.0129501819611, "timesteps_since_restore": 8448, "iterations_since_restore": 33, "warmup_time": 45.46659183502197, "perf": {"cpu_util_percent": 24.94516129032258, "ram_util_percent": 11.66774193548387}}
{"episode_reward_max": 441.0, "episode_reward_min": 0.0, "episode_reward_mean": 27.267857142857142, "episode_len_mean": 601.0, "episode_media": {}, "episodes_this_iter": 2, "policy_reward_min": {"policy_01": 0.0, "policy_02": 0.0}, "policy_reward_max": {"policy_01": 441.0, "policy_02": 441.0}, "policy_reward_mean": {"policy_01": 19.392857142857142, "policy_02": 7.875}, "custom_metrics": {}, "hist_stats": {"episode_reward": [0.0, 204.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "episode_lengths": [601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601], "policy_policy_01_reward": [0.0, 204.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "policy_policy_02_reward": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.3363986658201198, "mean_inference_ms": 5.82373558848978, "mean_action_processing_ms": 0.0890311539892481, "mean_env_wait_ms": 8.070857611148266, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 2, "timesteps_total": 34000, "timesteps_this_iter": 256, "agent_timesteps_total": 68000, "timers": {"load_time_ms": 1.335, "load_throughput": 191804.688, "learn_time_ms": 11.9, "learn_throughput": 21512.354, "update_time_ms": 2.217}, "info": {"learner": {"policy_01": {"custom_metrics": {}, "learner_stats": {"mean_q": 62.87537384033203, "min_q": 40.83514404296875, "max_q": 347.49432373046875, "cur_lr": 0.0005}, "model": {}, "td_error": [1.7027664184570312, -0.13368988037109375, -6.322731018066406, 1.0489578247070312, -1.1414222717285156, 0.11550140380859375, -9.069557189941406, -0.4780693054199219, -0.2574501037597656, -0.08435821533203125, 0.8955802917480469, -1.7411460876464844, 0.25518035888671875, 1.3562240600585938, -25.706451416015625, -0.173431396484375, -0.25289154052734375, -0.8582496643066406, 0.1796722412109375, 1.8681449890136719, -1.5019187927246094, 0.781646728515625, -7.272670745849609, 0.9139251708984375, -0.5470237731933594, 0.45073699951171875, 1.1927108764648438, 8.572235107421875, -0.5287361145019531, -0.7427749633789062, 0.23093032836914062, -15.628067016601562, -6.930145263671875, -1.8015556335449219, -1.4671096801757812, -2.617645263671875, 0.06784439086914062, -0.4737968444824219, 3.2958145141601562, -1.5103416442871094, 0.5774078369140625, -0.6564178466796875, -0.9264564514160156, -0.3448486328125, 0.3944511413574219, -0.6302261352539062, -0.9530220031738281, -0.6200981140136719, -0.44176483154296875, -12.651985168457031, 1.7235260009765625, -0.18674087524414062, -4.2003326416015625, -1.2070579528808594, -0.17744064331054688, 1.87127685546875, -0.7907600402832031, 2.1532211303710938, -0.2185211181640625, -1.9442329406738281, 0.1077728271484375, -0.6707763671875, 4.275566101074219, 22.0748291015625, -11.486042022705078, 0.8864669799804688, 0.055065155029296875, -0.15027236938476562, -1.65753173828125, 7.153343200683594, -6.107810974121094, -1.4513931274414062, 0.10811996459960938, -0.16161727905273438, -0.4703369140625, -0.3103523254394531, -0.20943450927734375, 6.196990966796875, 12.44415283203125, 0.07097625732421875, -0.024303436279296875, -0.47026824951171875, 0.224578857421875, -5.770713806152344, 3.65545654296875, 0.06586837768554688, -1.2700233459472656, 0.15689849853515625, -3.296833038330078, -0.24988555908203125, 0.025691986083984375, -2.6135292053222656, -1.376129150390625, -0.10220718383789062, -1.2128791809082031, -0.7242622375488281, 41.612117767333984, -0.2723960876464844, -0.2534828186035156, 1.5803947448730469, -6.139862060546875, 0.6766738891601562, -0.8362197875976562, -1.2767868041992188, -0.5514717102050781, -5.872047424316406, 0.2599372863769531, -0.2597846984863281, 0.02175140380859375, 1.7881126403808594, -0.6912117004394531, -0.5876960754394531, -0.5370712280273438, -0.8679046630859375, 0.8794326782226562, -4.06500244140625, -0.08263015747070312, -1.9387359619140625, -0.127960205078125, -1.2583236694335938, 0.3549308776855469, -0.2897453308105469, -1.4857559204101562, -0.9770584106445312, -0.35181427001953125, 0.8628425598144531, -3.9714126586914062, -0.11772537231445312, 1.3558197021484375, -0.06731796264648438, -0.3824310302734375, -23.447647094726562, -3.4751358032226562, -0.6305389404296875, -0.4060249328613281, 0.39861297607421875, 0.2445068359375, -1.4318008422851562, -4.1532440185546875, 1.3093490600585938, -0.22917556762695312, -1.1094589233398438, -1.7884101867675781, 0.327362060546875, -0.5702285766601562, 30.52954864501953, -0.035503387451171875, 0.7207603454589844, 3.8282127380371094, -0.8507843017578125, 0.5168075561523438, -2.4442672729492188, -0.4484901428222656, -0.22232437133789062, 0.2991142272949219, -0.1485443115234375, 3.3397064208984375, -38.62555694580078, 0.24139785766601562, 6.216098785400391, 0.0594482421875, -4.969676971435547, -0.15019607543945312, 1.0789718627929688, -1.5227928161621094, 0.3936767578125, -0.12688827514648438, 0.040500640869140625, -0.4948272705078125, -0.11095046997070312, -0.6709213256835938, 42.62044906616211, -0.2627449035644531, -1.0633964538574219, -0.4216766357421875, -2.4198455810546875, 0.667022705078125, 41.144596099853516, -0.6298027038574219, -1.0369796752929688, -0.5878715515136719, -0.47026824951171875, -0.7985954284667969, 34.66450500488281, 0.4020881652832031, -3.7245025634765625, -0.4179534912109375, -33.45014190673828, -3.5069122314453125, 0.21187210083007812, -0.8053436279296875, 0.12445449829101562, 0.4956626892089844, -1.8015556335449219, -0.22026824951171875, -0.6239891052246094, -7.70037841796875, 0.7034568786621094, -0.27060699462890625, 0.5604896545410156, 0.3609123229980469, -0.6208724975585938, 0.08438873291015625, -0.10151290893554688, -0.7446479797363281, -0.8503990173339844, -0.5355606079101562, -0.5605888366699219, -0.5135307312011719, 0.3021583557128906, -4.610107421875, -0.8832321166992188, -3.876739501953125, -0.033061981201171875, -0.19782638549804688, -0.48220062255859375, -2.3898468017578125, -5.022895812988281, -4.4363250732421875, -0.006580352783203125, -0.100738525390625, -0.6646003723144531, -3.29827880859375, 0.22774124145507812, -1.1721000671386719, -0.9229698181152344, -3.04217529296875, -0.34607696533203125, -0.6088638305664062, -0.6383171081542969, -0.34360504150390625, 0.5002632141113281, 1.1166229248046875, -1.6962509155273438, 0.041698455810546875, -0.5568695068359375, -0.4203147888183594, -0.6451377868652344, -8.052848815917969, -0.5135307312011719, 0.34146881103515625, -0.12264633178710938, -1.4954147338867188, 0.0603179931640625, 0.026805877685546875, 6.9580841064453125, -0.9917984008789062, -1.7177581787109375, -0.7128486633300781, 1.6827163696289062, -1.0055046081542969, -1.202880859375, 0.2949371337890625, -0.1910247802734375, -5.183712005615234, 0.3150787353515625], "mean_td_error": -0.2970101833343506}}, "num_steps_sampled": 34000, "num_agent_steps_sampled": 68000, "num_steps_trained": 1056256, "num_steps_trained_this_iter": 256, "num_agent_steps_trained": 2112512, "last_target_update_ts": 33760, "num_target_updates": 66}, "done": false, "episodes_total": 56, "training_iteration": 34, "trial_id": "e21e6_00000", "experiment_id": "434cd42d33fd4b638f17fc69fa01d74f", "date": "2022-06-14_19-13-46", "timestamp": 1655248426, "time_this_iter_s": 21.469627141952515, "time_total_s": 705.4825773239136, "pid": 2568314, "hostname": "lambda-dual", "node_ip": "10.104.51.19", "config": {"num_workers": 2, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.0005, "train_batch_size": 256, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "cnet", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "1v1env", "observation_space": null, "action_space": null, "env_config": {}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": true, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": null, "evaluation_duration": 10, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"num_workers": 2, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.0005, "train_batch_size": 256, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "cnet", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "1v1env", "observation_space": null, "action_space": null, "env_config": {}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": false, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": null, "evaluation_duration": 10, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"explore": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": true, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": null, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 2, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"policy_01": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]], [[[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]], (3, 64, 64), float32)", "Discrete(7)", {}], "policy_02": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]], [[[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]], (3, 64, 64), float32)", "Discrete(7)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": "<function <lambda> at 0x7f1aa3764b00>", "policies_to_train": ["policy_01"], "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": 100000, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": false, "hiddens": [], "double_q": false, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": true, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": null, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 2, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"policy_01": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]], [[[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]], (3, 64, 64), float32)", "Discrete(7)", {}], "policy_02": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]], [[[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]], (3, 64, 64), float32)", "Discrete(7)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": "<function <lambda> at 0x7f1aa3764b00>", "policies_to_train": ["policy_01"], "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": 100000, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": false, "hiddens": [], "double_q": false, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "time_since_restore": 705.4825773239136, "timesteps_since_restore": 8704, "iterations_since_restore": 34, "warmup_time": 45.46659183502197, "perf": {"cpu_util_percent": 25.01666666666667, "ram_util_percent": 11.773333333333335}}
{"episode_reward_max": 441.0, "episode_reward_min": 0.0, "episode_reward_mean": 26.32758620689655, "episode_len_mean": 601.0, "episode_media": {}, "episodes_this_iter": 2, "policy_reward_min": {"policy_01": 0.0, "policy_02": 0.0}, "policy_reward_max": {"policy_01": 441.0, "policy_02": 441.0}, "policy_reward_mean": {"policy_01": 18.724137931034484, "policy_02": 7.603448275862069}, "custom_metrics": {}, "hist_stats": {"episode_reward": [0.0, 204.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "episode_lengths": [601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601], "policy_policy_01_reward": [0.0, 204.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "policy_policy_02_reward": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.3363198521119748, "mean_inference_ms": 5.822166242977503, "mean_action_processing_ms": 0.0890166495179265, "mean_env_wait_ms": 8.062383607300466, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 2, "timesteps_total": 35000, "timesteps_this_iter": 256, "agent_timesteps_total": 70000, "timers": {"load_time_ms": 1.324, "load_throughput": 193379.887, "learn_time_ms": 12.165, "learn_throughput": 21043.116, "update_time_ms": 2.369}, "info": {"learner": {"policy_01": {"custom_metrics": {}, "learner_stats": {"mean_q": 74.30720520019531, "min_q": 41.56060028076172, "max_q": 404.3188171386719, "cur_lr": 0.0005}, "model": {}, "td_error": [0.02374267578125, -3.307342529296875, -0.9968452453613281, 0.5535392761230469, -0.90869140625, -1.407196044921875, -1.2054100036621094, -0.19120407104492188, -2.467742919921875, -0.7077827453613281, -0.3048286437988281, 0.6075172424316406, -0.5007743835449219, 1.0729179382324219, 0.00484466552734375, -4.4984893798828125, 1.1998634338378906, 4.24957275390625, -0.0935821533203125, -0.41182708740234375, -0.13637542724609375, -1.3244857788085938, -0.6561088562011719, 4.2819366455078125, 0.1399688720703125, 0.6207046508789062, 18.103118896484375, -1.4580345153808594, 1.330352783203125, -0.4234733581542969, 1.3149566650390625, -0.6904983520507812, -6.488037109375, 0.473114013671875, -0.8476524353027344, -1.69500732421875, -3.897918701171875, -0.16664886474609375, 0.8617324829101562, 0.5838661193847656, 0.3968658447265625, -0.29259490966796875, 0.07895278930664062, -1.5387420654296875, 2.7718505859375, -0.667205810546875, -0.9186019897460938, -0.6386070251464844, -0.493682861328125, -8.085700988769531, -1.1144256591796875, 2.702301025390625, -1.3728370666503906, -0.0711212158203125, -4.194087982177734, -0.7486381530761719, -0.5469436645507812, -0.6113967895507812, 0.4113578796386719, 7.531147003173828, -2.007671356201172, 4.794288635253906, 7.4049530029296875, -1.1994171142578125, 25.540206909179688, -0.61212158203125, -0.35100555419921875, -0.32209014892578125, -0.14113235473632812, -0.3450736999511719, -0.6444816589355469, -0.5141563415527344, -34.042236328125, 1.249664306640625, 1.1567764282226562, -0.24502944946289062, -1.4533309936523438, 43.024757385253906, -0.590667724609375, -0.6541099548339844, 0.12994384765625, -9.764930725097656, 0.7516517639160156, -0.373992919921875, -1.8781318664550781, 0.5962181091308594, -0.4949760437011719, 2.489013671875, 0.43647003173828125, -1.3278617858886719, -0.7611503601074219, 0.09292221069335938, -10.856098175048828, 92.08634948730469, -2.067798614501953, -0.30175018310546875, -0.8385543823242188, -9.0068359375, 1.2441749572753906, -10.454299926757812, 1.7459869384765625, 9.51910400390625, -0.7896347045898438, -0.24890899658203125, -2.244354248046875, -2.007671356201172, -4.99652099609375, 1.2565193176269531, 0.02867889404296875, 1.5536041259765625, 0.12222671508789062, 4.163410186767578, -0.27437591552734375, 12.787986755371094, -0.14634323120117188, -0.46160125732421875, 0.020587921142578125, 0.11545562744140625, -2.7098121643066406, 5.137298583984375, 1.10479736328125, -0.6249275207519531, -0.0598907470703125, 10.621997833251953, -0.7693595886230469, -0.6712188720703125, -1.2620925903320312, -0.3236656188964844, -0.4532737731933594, 22.5318603515625, 3.0867538452148438, -2.2669448852539062, -0.032855987548828125, -2.704742431640625, 0.5140838623046875, 10.5460205078125, 3.4588851928710938, 0.28281402587890625, -0.3091316223144531, 0.8173599243164062, 0.8834075927734375, -1.4679374694824219, -8.100723266601562, -2.150421142578125, 1.380340576171875, 1.2819786071777344, -0.9691009521484375, -0.6250419616699219, -9.418731689453125, 12.02227783203125, -0.06214141845703125, -3.3043365478515625, -0.1548004150390625, 0.11799240112304688, -0.05933380126953125, 0.074432373046875, -0.37328338623046875, 0.44818878173828125, 3.677154541015625, -0.8007431030273438, 2.534099578857422, -17.857009887695312, -1.3978805541992188, -0.27437591552734375, 0.799652099609375, 3.0823822021484375, 1.075653076171875, -0.0367431640625, 14.0968017578125, 41.61296081542969, -0.4320220947265625, 2.4488906860351562, -6.976776123046875, 1.0256233215332031, 0.2071380615234375, 7.88189697265625, 0.927520751953125, -0.16646957397460938, 2.13818359375, 0.19983673095703125, 6.0910186767578125, 1.1850509643554688, -0.7739791870117188, 8.3253173828125, -0.40048980712890625, -0.3320045471191406, 0.07210159301757812, -0.3985328674316406, 5.1403656005859375, 0.090728759765625, 2.1844139099121094, 0.3778495788574219, -1.5090599060058594, -0.5922050476074219, 2.4154815673828125, -0.4771881103515625, 0.0283203125, -0.15481948852539062, -1.7418708801269531, 2.5701675415039062, 43.3873176574707, -1.3123741149902344, 0.15916824340820312, 0.10845947265625, 1.0833244323730469, 1.1183547973632812, 0.54571533203125, -0.6631660461425781, 0.3250846862792969, 0.9846343994140625, 0.846710205078125, -1.0010337829589844, -5.589237213134766, 1.14642333984375, 0.36678314208984375, 4.129138946533203, 44.43999481201172, 0.43344879150390625, 4.694831848144531, -0.37107086181640625, 1.4189033508300781, -1.4413604736328125, -0.06961441040039062, 2.1429519653320312, -8.668174743652344, 0.4609260559082031, 11.934013366699219, -0.25292205810546875, 0.11406707763671875, 0.5482444763183594, -0.8795890808105469, -10.637351989746094, -7.7931060791015625, -0.6288833618164062, -0.331817626953125, 47.8486213684082, 1.9211540222167969, 0.01324462890625, 1.1252288818359375, -0.8264007568359375, 48.38362503051758, -0.7270431518554688, -0.39654541015625, -0.0851898193359375, -28.790184020996094, 0.07210159301757812, -1.3091239929199219, 0.9518013000488281, -5.588726043701172, -1.6116981506347656, 0.7299919128417969, 0.19739913940429688, -1.5613594055175781, -2.878864288330078, -1.9723091125488281, -8.085700988769531], "mean_td_error": 1.33823561668396}}, "num_steps_sampled": 35000, "num_agent_steps_sampled": 70000, "num_steps_trained": 1088256, "num_steps_trained_this_iter": 256, "num_agent_steps_trained": 2176512, "last_target_update_ts": 34768, "num_target_updates": 68}, "done": false, "episodes_total": 58, "training_iteration": 35, "trial_id": "e21e6_00000", "experiment_id": "434cd42d33fd4b638f17fc69fa01d74f", "date": "2022-06-14_19-14-07", "timestamp": 1655248447, "time_this_iter_s": 21.56367802619934, "time_total_s": 727.0462553501129, "pid": 2568314, "hostname": "lambda-dual", "node_ip": "10.104.51.19", "config": {"num_workers": 2, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.0005, "train_batch_size": 256, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "cnet", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "1v1env", "observation_space": null, "action_space": null, "env_config": {}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": true, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": null, "evaluation_duration": 10, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"num_workers": 2, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.0005, "train_batch_size": 256, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "cnet", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "1v1env", "observation_space": null, "action_space": null, "env_config": {}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": false, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": null, "evaluation_duration": 10, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"explore": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": true, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": null, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 2, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"policy_01": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]], [[[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]], (3, 64, 64), float32)", "Discrete(7)", {}], "policy_02": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]], [[[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]], (3, 64, 64), float32)", "Discrete(7)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": "<function <lambda> at 0x7f1aa3726680>", "policies_to_train": ["policy_01"], "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": 100000, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": false, "hiddens": [], "double_q": false, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": true, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": null, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 2, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"policy_01": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]], [[[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]], (3, 64, 64), float32)", "Discrete(7)", {}], "policy_02": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]], [[[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]], (3, 64, 64), float32)", "Discrete(7)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": "<function <lambda> at 0x7f1aa3726680>", "policies_to_train": ["policy_01"], "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": 100000, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": false, "hiddens": [], "double_q": false, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "time_since_restore": 727.0462553501129, "timesteps_since_restore": 8960, "iterations_since_restore": 35, "warmup_time": 45.46659183502197, "perf": {"cpu_util_percent": 24.770967741935483, "ram_util_percent": 11.845161290322574}}
{"episode_reward_max": 441.0, "episode_reward_min": 0.0, "episode_reward_mean": 26.32758620689655, "episode_len_mean": 601.0, "episode_media": {}, "episodes_this_iter": 0, "policy_reward_min": {"policy_01": 0.0, "policy_02": 0.0}, "policy_reward_max": {"policy_01": 441.0, "policy_02": 441.0}, "policy_reward_mean": {"policy_01": 18.724137931034484, "policy_02": 7.603448275862069}, "custom_metrics": {}, "hist_stats": {"episode_reward": [0.0, 204.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "episode_lengths": [601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601], "policy_policy_01_reward": [0.0, 204.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "policy_policy_02_reward": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.3363198521119748, "mean_inference_ms": 5.822166242977503, "mean_action_processing_ms": 0.0890166495179265, "mean_env_wait_ms": 8.062383607300466, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 2, "timesteps_total": 36000, "timesteps_this_iter": 256, "agent_timesteps_total": 72000, "timers": {"load_time_ms": 1.351, "load_throughput": 189442.62, "learn_time_ms": 11.89, "learn_throughput": 21530.169, "update_time_ms": 2.247}, "info": {"learner": {"policy_01": {"custom_metrics": {}, "learner_stats": {"mean_q": 74.05248260498047, "min_q": 40.82429504394531, "max_q": 396.339599609375, "cur_lr": 0.0005}, "model": {}, "td_error": [-1.2437629699707031, 0.008544921875, 0.715667724609375, 0.06397247314453125, -5.993804931640625, 0.011684417724609375, 0.2830657958984375, 1.9021453857421875, 7.132358551025391, 0.18669509887695312, -0.9949111938476562, 28.213150024414062, -1.0384140014648438, -0.5244903564453125, 1.0957679748535156, -1.6259307861328125, 0.7284164428710938, 0.2609901428222656, -2.849040985107422, -0.7470970153808594, -0.015964508056640625, 1.0406646728515625, -1.2773323059082031, -0.5098228454589844, 41.71193313598633, -1.4620132446289062, 2.1147384643554688, -0.7695121765136719, -0.004039764404296875, 0.5066413879394531, 0.47972869873046875, -3.5210838317871094, -0.5885467529296875, -1.6890182495117188, -11.718048095703125, 1.6556396484375, 4.133914947509766, 0.6968879699707031, 0.20711135864257812, -0.9294166564941406, -0.17596054077148438, -0.7034835815429688, -0.3495635986328125, -0.8188858032226562, 0.490936279296875, 46.7677001953125, 2.022857666015625, 0.3295173645019531, -4.9283905029296875, 0.6783294677734375, 1.0327568054199219, -0.4966087341308594, -9.758049011230469, -13.933914184570312, -11.86456298828125, -1.2320976257324219, 0.08385848999023438, -0.25388336181640625, 0.14594268798828125, -1.2915725708007812, 5.0163116455078125, 0.918487548828125, -0.2593193054199219, -0.6261062622070312, -0.568817138671875, 0.5396690368652344, 0.6302299499511719, -1.3807144165039062, 1.3312835693359375, -0.031208038330078125, -0.5509414672851562, -0.32184600830078125, 46.06694412231445, 0.44361114501953125, 1.3162498474121094, 0.3041954040527344, 0.3642158508300781, -1.5204200744628906, -2.5620994567871094, -3.15911865234375, -5.5936279296875, -0.44585418701171875, 5.6959075927734375, -5.851959228515625, 0.36472320556640625, 0.10963058471679688, -4.35626220703125, -3.1907577514648438, 3.608928680419922, -0.5729866027832031, 0.20609664916992188, -0.5561943054199219, 0.3369598388671875, -3.129077911376953, 1.029541015625, -0.8990325927734375, -40.80120849609375, -1.6307029724121094, -7.178749084472656, -0.20644378662109375, -15.873615264892578, -13.0323486328125, -0.9399833679199219, -12.269935607910156, -0.3699798583984375, -0.4368247985839844, 3.376708984375, 1.0384750366210938, -1.0846595764160156, -6.31256103515625, -0.470245361328125, -0.3626441955566406, -3.5687026977539062, 14.73193359375, -0.49677276611328125, 0.09096527099609375, -0.5938491821289062, 0.6302680969238281, -0.8455657958984375, -0.68603515625, -1.4818916320800781, -6.837249755859375, 0.19451904296875, -0.022914886474609375, 0.7561492919921875, -0.3191261291503906, -0.5936698913574219, -1.7104873657226562, -1.2200546264648438, -7.6688385009765625, -0.07641220092773438, -26.795745849609375, -0.459716796875, 1.0263595581054688, 49.142852783203125, -2.0788650512695312, 0.10469436645507812, 0.9320716857910156, -8.461078643798828, 0.9007568359375, 0.7408485412597656, -1.1668701171875, 0.6593017578125, 1.0508346557617188, -0.4023094177246094, -0.8352508544921875, -3.3164024353027344, -1.4187507629394531, 0.6064109802246094, -0.5574378967285156, -2.9760513305664062, -0.19406890869140625, -1.0097465515136719, -6.421882629394531, 0.7483596801757812, 1.5426292419433594, 0.8411903381347656, 1.3185882568359375, -1.1468009948730469, -0.033138275146484375, 0.50830078125, 0.5214958190917969, -13.807891845703125, -0.0883941650390625, -0.6407432556152344, -0.4317436218261719, 0.21026229858398438, 26.740966796875, -0.5285263061523438, 0.5165443420410156, 3.01068115234375, 0.99237060546875, -0.4726676940917969, -0.823028564453125, -1.55865478515625, -0.19709396362304688, 1.0613555908203125, 0.6640357971191406, -4.341407775878906, -4.706020355224609, -1.5677833557128906, 0.5457229614257812, -0.5665397644042969, -0.16678619384765625, -0.5305633544921875, -0.25276947021484375, -0.2671699523925781, -1.1410484313964844, -0.88458251953125, -0.5710678100585938, 0.22970199584960938, 0.32000732421875, 4.371112823486328, -4.9760894775390625, -0.773529052734375, 2.52593994140625, -0.3284263610839844, -0.7992744445800781, 1.4624862670898438, -21.29022216796875, -1.5826492309570312, 0.7608566284179688, -1.8949470520019531, 1.2442855834960938, 1.7795143127441406, -5.70489501953125, -20.76581573486328, -0.059215545654296875, -6.007667541503906, 0.822265625, -0.7435493469238281, -0.8354034423828125, -2.1655731201171875, 0.20151519775390625, -1.7253646850585938, -0.2746429443359375, -27.898101806640625, -4.882568359375, -8.59222412109375, -0.6934165954589844, -0.20663833618164062, 0.14017868041992188, 0.7649192810058594, -9.6910400390625, -1.2733192443847656, -0.5531463623046875, -0.43494415283203125, -1.8607254028320312, 0.9670677185058594, 6.313507080078125, -2.4635963439941406, -0.9821014404296875, -2.1113967895507812, 5.941802978515625, 0.1331787109375, -5.75140380859375, -1.6024551391601562, 1.8089981079101562, -0.20952987670898438, -1.3960762023925781, -0.3887062072753906, -0.4317436218261719, -0.14910507202148438, 0.036754608154296875, 16.826988220214844, -0.5712432861328125, -4.4187164306640625, -0.5029563903808594, -0.21105194091796875, 0.4162330627441406, 0.097137451171875, 2.7433204650878906, -10.109619140625, -0.5953903198242188, 1.0206298828125, 1.0604629516601562], "mean_td_error": -0.4473632574081421}}, "num_steps_sampled": 36000, "num_agent_steps_sampled": 72000, "num_steps_trained": 1120256, "num_steps_trained_this_iter": 256, "num_agent_steps_trained": 2240512, "last_target_update_ts": 35776, "num_target_updates": 70}, "done": false, "episodes_total": 58, "training_iteration": 36, "trial_id": "e21e6_00000", "experiment_id": "434cd42d33fd4b638f17fc69fa01d74f", "date": "2022-06-14_19-14-29", "timestamp": 1655248469, "time_this_iter_s": 21.899117469787598, "time_total_s": 748.9453728199005, "pid": 2568314, "hostname": "lambda-dual", "node_ip": "10.104.51.19", "config": {"num_workers": 2, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.0005, "train_batch_size": 256, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "cnet", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "1v1env", "observation_space": null, "action_space": null, "env_config": {}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": true, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": null, "evaluation_duration": 10, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"num_workers": 2, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.0005, "train_batch_size": 256, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "cnet", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "1v1env", "observation_space": null, "action_space": null, "env_config": {}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": false, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": null, "evaluation_duration": 10, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"explore": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": true, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": null, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 2, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"policy_01": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]], [[[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]], (3, 64, 64), float32)", "Discrete(7)", {}], "policy_02": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]], [[[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]], (3, 64, 64), float32)", "Discrete(7)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": "<function <lambda> at 0x7f1aa2ebedd0>", "policies_to_train": ["policy_01"], "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": 100000, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": false, "hiddens": [], "double_q": false, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": true, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": null, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 2, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"policy_01": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]], [[[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]], (3, 64, 64), float32)", "Discrete(7)", {}], "policy_02": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]], [[[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]], (3, 64, 64), float32)", "Discrete(7)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": "<function <lambda> at 0x7f1aa2ebedd0>", "policies_to_train": ["policy_01"], "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": 100000, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": false, "hiddens": [], "double_q": false, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "time_since_restore": 748.9453728199005, "timesteps_since_restore": 9216, "iterations_since_restore": 36, "warmup_time": 45.46659183502197, "perf": {"cpu_util_percent": 25.029032258064518, "ram_util_percent": 11.938709677419356}}
{"episode_reward_max": 441.0, "episode_reward_min": 0.0, "episode_reward_mean": 25.45, "episode_len_mean": 601.0, "episode_media": {}, "episodes_this_iter": 2, "policy_reward_min": {"policy_01": 0.0, "policy_02": 0.0}, "policy_reward_max": {"policy_01": 441.0, "policy_02": 441.0}, "policy_reward_mean": {"policy_01": 18.1, "policy_02": 7.35}, "custom_metrics": {}, "hist_stats": {"episode_reward": [0.0, 204.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "episode_lengths": [601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601], "policy_policy_01_reward": [0.0, 204.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "policy_policy_02_reward": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.3361960185698471, "mean_inference_ms": 5.8205905276489, "mean_action_processing_ms": 0.08900291055786178, "mean_env_wait_ms": 8.054083640212324, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 2, "timesteps_total": 37000, "timesteps_this_iter": 256, "agent_timesteps_total": 74000, "timers": {"load_time_ms": 1.32, "load_throughput": 193980.782, "learn_time_ms": 11.927, "learn_throughput": 21463.246, "update_time_ms": 2.218}, "info": {"learner": {"policy_01": {"custom_metrics": {}, "learner_stats": {"mean_q": 82.99137115478516, "min_q": 38.05527877807617, "max_q": 408.1641845703125, "cur_lr": 0.0005}, "model": {}, "td_error": [0.5570182800292969, 1.9192390441894531, -21.942398071289062, 0.27710723876953125, 1.305938720703125, -0.8688926696777344, 1.0444259643554688, 0.09111404418945312, 0.024105072021484375, 2.419281005859375, -3.613880157470703, 0.39504241943359375, 0.3464698791503906, 1.775238037109375, 0.921234130859375, 0.5527496337890625, 0.20521926879882812, 0.6594696044921875, -16.178245544433594, 0.5249595642089844, 17.202667236328125, 3.5829010009765625, 51.193382263183594, -0.614410400390625, 1.7692451477050781, 0.4054756164550781, 2.0483322143554688, 1.1377067565917969, 0.4104347229003906, 61.72715759277344, -0.9133949279785156, 0.3475532531738281, 1.4033470153808594, -0.782257080078125, 0.4936790466308594, -3.807525634765625, -0.5855789184570312, 1.3994178771972656, -3.0911026000976562, 0.5250244140625, -0.4280662536621094, 0.424468994140625, 1.199249267578125, 2.2203369140625, 0.4038047790527344, 1.4315414428710938, 0.7198982238769531, -2.529815673828125, -0.132171630859375, 43.093021392822266, -0.3576240539550781, 0.5431785583496094, 0.2811012268066406, -0.08638763427734375, 1.3085670471191406, 0.16572952270507812, 0.633026123046875, 0.8192176818847656, -0.22895050048828125, 1.3922882080078125, -7.442089080810547, 0.26548004150390625, -6.141197204589844, 0.7257804870605469, -1.6385650634765625, 0.1308135986328125, 16.7161865234375, 0.38309478759765625, -6.81365966796875, 0.6613311767578125, 0.8161468505859375, -0.7843437194824219, 1.9412803649902344, -2.1245803833007812, -1.0681228637695312, -10.537834167480469, -0.6409187316894531, 0.023220062255859375, 2.217193603515625, 0.2863121032714844, 0.3532981872558594, 0.2846031188964844, -1.0446929931640625, -25.581207275390625, -3.5945816040039062, 1.2623176574707031, 0.3759040832519531, -0.0280914306640625, 0.4648895263671875, 0.45993804931640625, 49.59028244018555, -0.10333633422851562, 0.2665367126464844, -7.1653900146484375, 42.0563850402832, -0.5363998413085938, 0.4830322265625, 2.985687255859375, 1.305938720703125, 4.574596405029297, 0.8903694152832031, 0.8876113891601562, 0.7971839904785156, -0.12994384765625, -0.6445693969726562, 10.296112060546875, 10.708541870117188, 0.927642822265625, -2.789958953857422, -23.547195434570312, 0.7588272094726562, 1.2635116577148438, 2.709217071533203, -3.6610069274902344, -0.7874069213867188, -10.703750610351562, 1.9342041015625, 0.4645233154296875, 0.039699554443359375, 0.8977508544921875, -3.415363311767578, 2.422863006591797, 0.4154167175292969, -6.794376373291016, 1.0906143188476562, 0.7712135314941406, 1.998504638671875, -0.7737922668457031, -1.2413520812988281, -2.46514892578125, -2.1431922912597656, 0.586822509765625, -0.3958091735839844, 0.14990997314453125, 1.3968353271484375, -1.5940628051757812, 2.2418174743652344, 1.4196891784667969, 2.9030380249023438, -5.5702056884765625, 0.7340431213378906, -0.7843437194824219, -0.5601539611816406, 1.0479507446289062, 0.9770698547363281, -5.814826965332031, -4.5020751953125, -12.009475708007812, -1.8920822143554688, 0.750518798828125, -0.3587188720703125, 0.4068946838378906, 0.6218376159667969, 1.0613632202148438, 51.464759826660156, -0.8907432556152344, -18.893394470214844, 0.3917427062988281, -0.3880500793457031, 2.208446502685547, 17.353363037109375, 0.2056732177734375, 0.297760009765625, 0.6212882995605469, -1.500518798828125, 0.7014694213867188, -11.9849853515625, -14.5213623046875, -0.12441253662109375, -0.5649909973144531, 0.1421356201171875, 0.8579597473144531, 2.5724105834960938, -31.92498779296875, 1.1968269348144531, 1.4724540710449219, 0.4632911682128906, 1.1032257080078125, 0.4790534973144531, 0.7249717712402344, 0.9435043334960938, 5.894378662109375, 0.6768569946289062, -4.103485107421875, 2.54730224609375, 1.6880989074707031, 0.3792839050292969, 4.446342468261719, -1.2837715148925781, -2.3595809936523438, 0.3846015930175781, 1.1988983154296875, 0.35802459716796875, 8.929073333740234, 0.9466514587402344, 0.6802978515625, 1.6090011596679688, -12.164779663085938, 0.6179962158203125, 1.0740242004394531, -31.504440307617188, -0.25879669189453125, -5.53948974609375, -0.071075439453125, 0.5141639709472656, -7.567569732666016, 0.4480476379394531, 6.2767791748046875, 1.1917572021484375, 4.766387939453125, -24.14710235595703, -0.2445220947265625, 0.4435577392578125, 1.3492965698242188, 0.605560302734375, -3.575672149658203, 0.25531005859375, 4.8878631591796875, -0.16075897216796875, -0.6294822692871094, -0.20888137817382812, -18.75885009765625, -0.6825027465820312, 0.41127777099609375, 0.23716354370117188, 0.6449317932128906, 1.2636909484863281, 0.5551834106445312, -0.4759330749511719, -0.4414482116699219, 0.3700828552246094, 0.51666259765625, -1.0692024230957031, -11.9849853515625, 1.7967300415039062, 0.4488525390625, -36.288482666015625, 2.410045623779297, 4.376583099365234, -0.4418220520019531, 0.6364898681640625, -0.06885528564453125, 0.9756126403808594, 7.419879913330078, 0.038303375244140625, 0.27591705322265625, 0.5405235290527344, 0.5243606567382812, 1.50390625, 0.019870758056640625, 3.231201171875, -9.567245483398438, -8.902626037597656, 1.8232879638671875, 12.159637451171875, -0.026966094970703125], "mean_td_error": 0.3271327614784241}}, "num_steps_sampled": 37000, "num_agent_steps_sampled": 74000, "num_steps_trained": 1152256, "num_steps_trained_this_iter": 256, "num_agent_steps_trained": 2304512, "last_target_update_ts": 36784, "num_target_updates": 72}, "done": false, "episodes_total": 60, "training_iteration": 37, "trial_id": "e21e6_00000", "experiment_id": "434cd42d33fd4b638f17fc69fa01d74f", "date": "2022-06-14_19-14-51", "timestamp": 1655248491, "time_this_iter_s": 21.36575984954834, "time_total_s": 770.3111326694489, "pid": 2568314, "hostname": "lambda-dual", "node_ip": "10.104.51.19", "config": {"num_workers": 2, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.0005, "train_batch_size": 256, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "cnet", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "1v1env", "observation_space": null, "action_space": null, "env_config": {}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": true, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": null, "evaluation_duration": 10, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"num_workers": 2, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.0005, "train_batch_size": 256, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "cnet", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "1v1env", "observation_space": null, "action_space": null, "env_config": {}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": false, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": null, "evaluation_duration": 10, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"explore": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": true, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": null, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 2, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"policy_01": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]], [[[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]], (3, 64, 64), float32)", "Discrete(7)", {}], "policy_02": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]], [[[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]], (3, 64, 64), float32)", "Discrete(7)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": "<function <lambda> at 0x7f1aa372add0>", "policies_to_train": ["policy_01"], "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": 100000, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": false, "hiddens": [], "double_q": false, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": true, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": null, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 2, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"policy_01": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]], [[[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]], (3, 64, 64), float32)", "Discrete(7)", {}], "policy_02": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]], [[[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]], (3, 64, 64), float32)", "Discrete(7)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": "<function <lambda> at 0x7f1aa372add0>", "policies_to_train": ["policy_01"], "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": 100000, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": false, "hiddens": [], "double_q": false, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "time_since_restore": 770.3111326694489, "timesteps_since_restore": 9472, "iterations_since_restore": 37, "warmup_time": 45.46659183502197, "perf": {"cpu_util_percent": 24.87741935483871, "ram_util_percent": 12.012903225806454}}
{"episode_reward_max": 441.0, "episode_reward_min": 0.0, "episode_reward_mean": 31.741935483870968, "episode_len_mean": 601.0, "episode_media": {}, "episodes_this_iter": 2, "policy_reward_min": {"policy_01": 0.0, "policy_02": 0.0}, "policy_reward_max": {"policy_01": 441.0, "policy_02": 441.0}, "policy_reward_mean": {"policy_01": 17.516129032258064, "policy_02": 14.225806451612904}, "custom_metrics": {}, "hist_stats": {"episode_reward": [0.0, 204.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0], "episode_lengths": [601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601], "policy_policy_01_reward": [0.0, 204.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "policy_policy_02_reward": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.33608548563581897, "mean_inference_ms": 5.819136982734132, "mean_action_processing_ms": 0.08899021419536296, "mean_env_wait_ms": 8.046103261360253, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 2, "timesteps_total": 38000, "timesteps_this_iter": 256, "agent_timesteps_total": 76000, "timers": {"load_time_ms": 1.34, "load_throughput": 191053.865, "learn_time_ms": 11.742, "learn_throughput": 21801.289, "update_time_ms": 2.389}, "info": {"learner": {"policy_01": {"custom_metrics": {}, "learner_stats": {"mean_q": 70.01183319091797, "min_q": 40.67236328125, "max_q": 450.217529296875, "cur_lr": 0.0005}, "model": {}, "td_error": [-0.0069580078125, 1.1075286865234375, -0.6451835632324219, -9.16717529296875, -0.8035774230957031, -0.7370758056640625, 0.6962928771972656, 11.766769409179688, -1.3282356262207031, -1.0156898498535156, 0.03517913818359375, -2.4886245727539062, -0.12747955322265625, -4.701465606689453, -1.10577392578125, -0.00159454345703125, 4.748069763183594, 0.19553756713867188, -0.670379638671875, -1.6351661682128906, -1.8885688781738281, 0.5345497131347656, 97.56278228759766, 0.025020599365234375, 0.043975830078125, 0.0080108642578125, -2.8221397399902344, -0.3756370544433594, -10.600013732910156, -0.9513893127441406, -11.614364624023438, 1.6507987976074219, 0.14311599731445312, 1.0820770263671875, -0.6561470031738281, -0.7424850463867188, -1.10638427734375, -0.15065383911132812, -1.1720161437988281, 2.927112579345703, -1.8407249450683594, -3.55023193359375, -0.8070411682128906, -0.5396957397460938, 0.16027069091796875, -2.481304168701172, -1.5171699523925781, 1.0062217712402344, -1.1022186279296875, 1.2671661376953125, -0.6604766845703125, 0.09666061401367188, -0.2836151123046875, -1.060272216796875, 48.38832092285156, 0.15650177001953125, -2.2273483276367188, -2.0091934204101562, -0.6008644104003906, -0.8361167907714844, -3.124675750732422, -0.0648345947265625, 1.4179420471191406, -3.4824867248535156, -0.8162689208984375, 0.052524566650390625, -0.48931884765625, -1.6211471557617188, -3.579029083251953, -0.7435379028320312, -0.5682144165039062, -2.1292953491210938, -0.9161834716796875, -3.7772674560546875, 20.5985107421875, -5.988014221191406, -0.67987060546875, 0.09284210205078125, 1.879150390625, 0.9127197265625, 0.10936355590820312, -1.3487548828125, -0.06282806396484375, -0.813232421875, -30.382461547851562, -0.6666374206542969, -0.5541572570800781, -0.8656997680664062, -13.25811767578125, 0.10308837890625, -0.16735076904296875, -1.5065994262695312, -1.4716873168945312, -2.1964988708496094, -1.0089797973632812, 0.3667030334472656, -19.310546875, -8.485244750976562, 11.098518371582031, -9.18448257446289, -1.1662216186523438, -0.5295143127441406, -1.5627632141113281, -0.5644989013671875, -0.5166893005371094, -9.962223052978516, -1.5627632141113281, -0.3986396789550781, -0.7470054626464844, 52.5921630859375, -0.4883079528808594, -1.5311851501464844, -0.48538970947265625, 0.4047393798828125, -0.5832672119140625, -0.7703056335449219, -4.595439910888672, -0.34442138671875, -23.682098388671875, 0.43044281005859375, -3.7965965270996094, 0.347686767578125, 0.5079460144042969, -0.5359039306640625, 0.3916511535644531, -0.9474754333496094, 0.25324249267578125, 0.12769317626953125, 1.0280914306640625, 1.2762565612792969, -1.1310081481933594, 0.043689727783203125, -0.4623565673828125, -0.28932952880859375, -2.644123077392578, -0.5035247802734375, -0.14854812622070312, 1.2535858154296875, -0.22118377685546875, 0.3246002197265625, 7.589332580566406, -0.8650894165039062, -0.2527351379394531, 0.4453582763671875, 13.945556640625, -1.2793159484863281, -0.874786376953125, -0.4488945007324219, -3.717792510986328, 47.778499603271484, -0.21952438354492188, -0.08203887939453125, -0.6200332641601562, -1.0239753723144531, 5.1605072021484375, -0.598358154296875, 0.16812515258789062, -0.9221343994140625, 0.15650177001953125, 0.7009811401367188, -2.554595947265625, -0.8831634521484375, 0.01882171630859375, 1.653350830078125, -0.4705810546875, -3.1790008544921875, 1.1577835083007812, -2.00543212890625, -0.3408851623535156, 0.068634033203125, -0.21862030029296875, -0.384002685546875, 0.0127105712890625, -0.28031158447265625, -1.3108634948730469, 3.184844970703125, -0.7775230407714844, -0.35086822509765625, -0.0446624755859375, 2.1331939697265625, -0.3465843200683594, 0.1372528076171875, 0.3697166442871094, 0.5667076110839844, -2.008575439453125, -0.13177871704101562, -1.6216468811035156, -11.284931182861328, -0.2733612060546875, -2.6434783935546875, 1.2029151916503906, -0.9831199645996094, -0.48114013671875, 0.8664283752441406, -0.90435791015625, -1.0011749267578125, -1.2257575988769531, -1.2137603759765625, -6.9487457275390625, -5.023365020751953, -0.027347564697265625, -0.16279983520507812, -0.9335975646972656, -0.4202117919921875, -0.5745468139648438, 21.382080078125, 0.6092491149902344, -0.10500335693359375, -0.046833038330078125, -36.50947570800781, -1.5840644836425781, -154.12281799316406, -0.4887580871582031, -5.53582763671875, 0.5978584289550781, -0.04247283935546875, -24.239952087402344, -0.6382522583007812, -0.09017181396484375, -1.9507331848144531, -166.61605834960938, 3.98175048828125, 0.2705535888671875, -8.398696899414062, -3.330432891845703, -0.10504150390625, -3.635723114013672, 2.483306884765625, -0.6073799133300781, 0.5500411987304688, -0.20410919189453125, -2.3217620849609375, 0.6451797485351562, -0.8385429382324219, -0.9419670104980469, -7.7470855712890625, 0.16147232055664062, -0.22647857666015625, 0.058116912841796875, -0.7532196044921875, -0.778350830078125, -1.791717529296875, -2.598064422607422, 48.47490692138672, -0.30338287353515625, -24.81000518798828, -2.6776275634765625, -0.49753570556640625, -1.3599853515625, -2.188030242919922, 0.5327491760253906, -1.1775360107421875, -0.5397911071777344, -0.10055923461914062, -0.019763946533203125, -0.2606315612792969], "mean_td_error": -1.3331420421600342}}, "num_steps_sampled": 38000, "num_agent_steps_sampled": 76000, "num_steps_trained": 1184256, "num_steps_trained_this_iter": 256, "num_agent_steps_trained": 2368512, "last_target_update_ts": 37792, "num_target_updates": 74}, "done": false, "episodes_total": 62, "training_iteration": 38, "trial_id": "e21e6_00000", "experiment_id": "434cd42d33fd4b638f17fc69fa01d74f", "date": "2022-06-14_19-15-12", "timestamp": 1655248512, "time_this_iter_s": 21.55702781677246, "time_total_s": 791.8681604862213, "pid": 2568314, "hostname": "lambda-dual", "node_ip": "10.104.51.19", "config": {"num_workers": 2, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.0005, "train_batch_size": 256, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "cnet", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "1v1env", "observation_space": null, "action_space": null, "env_config": {}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": true, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": null, "evaluation_duration": 10, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"num_workers": 2, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.0005, "train_batch_size": 256, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "cnet", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "1v1env", "observation_space": null, "action_space": null, "env_config": {}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": false, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": null, "evaluation_duration": 10, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"explore": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": true, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": null, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 2, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"policy_01": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]], [[[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]], (3, 64, 64), float32)", "Discrete(7)", {}], "policy_02": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]], [[[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]], (3, 64, 64), float32)", "Discrete(7)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": "<function <lambda> at 0x7f1aa372aef0>", "policies_to_train": ["policy_01"], "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": 100000, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": false, "hiddens": [], "double_q": false, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": true, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": null, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 2, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"policy_01": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]], [[[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]], (3, 64, 64), float32)", "Discrete(7)", {}], "policy_02": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]], [[[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]], (3, 64, 64), float32)", "Discrete(7)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": "<function <lambda> at 0x7f1aa372aef0>", "policies_to_train": ["policy_01"], "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": 100000, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": false, "hiddens": [], "double_q": false, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "time_since_restore": 791.8681604862213, "timesteps_since_restore": 9728, "iterations_since_restore": 38, "warmup_time": 45.46659183502197, "perf": {"cpu_util_percent": 24.809999999999995, "ram_util_percent": 12.103333333333335}}
{"episode_reward_max": 441.0, "episode_reward_min": 0.0, "episode_reward_mean": 30.75, "episode_len_mean": 601.0, "episode_media": {}, "episodes_this_iter": 2, "policy_reward_min": {"policy_01": 0.0, "policy_02": 0.0}, "policy_reward_max": {"policy_01": 441.0, "policy_02": 441.0}, "policy_reward_mean": {"policy_01": 16.96875, "policy_02": 13.78125}, "custom_metrics": {}, "hist_stats": {"episode_reward": [0.0, 204.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0], "episode_lengths": [601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601], "policy_policy_01_reward": [0.0, 204.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "policy_policy_02_reward": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.3359912386257017, "mean_inference_ms": 5.817797942339528, "mean_action_processing_ms": 0.08897858272628167, "mean_env_wait_ms": 8.038438145943942, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 2, "timesteps_total": 39000, "timesteps_this_iter": 256, "agent_timesteps_total": 78000, "timers": {"load_time_ms": 1.322, "load_throughput": 193711.316, "learn_time_ms": 11.91, "learn_throughput": 21494.655, "update_time_ms": 2.366}, "info": {"learner": {"policy_01": {"custom_metrics": {}, "learner_stats": {"mean_q": 76.6948013305664, "min_q": 40.07257080078125, "max_q": 444.8346252441406, "cur_lr": 0.0005}, "model": {}, "td_error": [-0.5506591796875, -0.6474189758300781, 0.44532012939453125, -0.3107032775878906, -0.07458877563476562, -1.3601036071777344, 0.8060760498046875, -1.5861930847167969, -0.864471435546875, -0.4329833984375, -0.20643234252929688, 0.07031631469726562, -0.00429534912109375, -2.3872528076171875, 9.195304870605469, -0.8529510498046875, -0.3497581481933594, 0.9806900024414062, -0.8399734497070312, -0.87200927734375, -39.412811279296875, -0.5511512756347656, 47.233978271484375, 0.6957206726074219, -1.024749755859375, 0.3065223693847656, -1.60552978515625, -0.1055908203125, -0.04712677001953125, 9.4312744140625, 0.22058868408203125, 2.0307693481445312, 0.7110443115234375, -5.905452728271484, -0.8776283264160156, 49.4283561706543, 10.78033447265625, -9.8035888671875, 0.3623924255371094, -0.11473464965820312, -0.03131866455078125, -1.8566398620605469, -0.9202957153320312, 0.1331939697265625, -0.7706298828125, 0.4893913269042969, -1.4462165832519531, 0.2885169982910156, -0.6843109130859375, 1.7900199890136719, -13.957878112792969, -0.020771026611328125, 10.3291015625, 1.2782821655273438, 0.20831298828125, -0.3343658447265625, 2.5443878173828125, 0.44525146484375, 1.2921600341796875, -1.7641983032226562, -1.2879676818847656, -0.7840614318847656, 0.3797798156738281, -6.709445953369141, -4.455204010009766, -6.519500732421875, 3.0623855590820312, -3.3475723266601562, 5.23724365234375, 1.1370010375976562, -0.4346961975097656, 0.8180503845214844, 0.6867408752441406, 0.9552726745605469, -3.8955230712890625, -0.41770172119140625, 0.19651412963867188, 0.7352943420410156, 0.1136322021484375, -1.3607254028320312, -1.5760612487792969, 7.7475128173828125, -0.3359565734863281, -2.246490478515625, -0.8818016052246094, 0.00295257568359375, -5.101806640625, 22.355224609375, -1.5378837585449219, -0.18517684936523438, 1.8621253967285156, -0.0341644287109375, -0.7800750732421875, -2.896514892578125, 12.45294189453125, -0.5134696960449219, 4.9034881591796875, -0.28418731689453125, 1.15380859375, 0.4556083679199219, 11.1553955078125, -2.0153541564941406, -0.31647491455078125, -0.17254638671875, -1.5825996398925781, 21.396392822265625, 0.18375015258789062, 3.2469329833984375, -0.16779708862304688, -0.13173675537109375, 2.168365478515625, 48.418251037597656, 2.3657760620117188, 0.027690887451171875, -1.0589027404785156, -0.34136199951171875, -0.6016502380371094, 1.4161491394042969, -0.5108528137207031, -1.9147796630859375, 0.9585342407226562, 2.139984130859375, 0.4985504150390625, -0.8085365295410156, 6.4897918701171875, -0.8254585266113281, 0.6142997741699219, 41.0979118347168, -1.5056724548339844, -1.1164093017578125, -1.5846481323242188, 0.27838134765625, 141.51815795898438, -0.15192413330078125, 120.56403350830078, -2.032062530517578, -0.3749732971191406, -1.0666046142578125, -0.6863212585449219, -1.3068275451660156, 1.03509521484375, -3.0809097290039062, 2.2453460693359375, 0.3823280334472656, -0.1715545654296875, 0.019962310791015625, -0.021099090576171875, -5.4482421875, 40.53565216064453, -1.0407295227050781, -0.02513885498046875, 0.35700225830078125, 6.750949859619141, -0.5330657958984375, -29.237548828125, -0.5578651428222656, -0.6281661987304688, 0.15874862670898438, -0.5896072387695312, 0.026302337646484375, -5.0026702880859375, 48.99517822265625, 0.3408355712890625, -0.048191070556640625, -1.8566398620605469, -0.5999794006347656, -0.7669486999511719, -0.7799720764160156, -0.11664962768554688, 0.4036216735839844, -2.8726806640625, 2.198272705078125, 1.2921600341796875, 0.43157958984375, -0.47919464111328125, -0.032794952392578125, 0.5066299438476562, -0.23664093017578125, -0.038661956787109375, -0.3820037841796875, -1.5494613647460938, 1.940948486328125, -3.825653076171875, -0.2279052734375, -0.5341567993164062, 0.15829086303710938, 0.9585952758789062, -0.11572647094726562, -0.5709266662597656, 0.08851242065429688, -1.6856117248535156, 0.8170623779296875, 0.025936126708984375, -0.3062400817871094, 4.8059844970703125, -1.0392799377441406, 0.025600433349609375, 0.3988761901855469, 4.9473876953125, -0.28234100341796875, 0.3012351989746094, 0.9074249267578125, -26.37237548828125, -17.807525634765625, -2.6515884399414062, 0.6629905700683594, 0.9412384033203125, 2.5942001342773438, -11.455623626708984, 0.06891632080078125, 10.327499389648438, -1.5890274047851562, -11.12799072265625, -5.786384582519531, -0.19877243041992188, 0.7428970336914062, 0.5238685607910156, -1.955657958984375, -1.2615203857421875, -1.893524169921875, -0.51806640625, 47.817283630371094, -0.06127166748046875, -0.029361724853515625, -3.1241493225097656, 0.2591552734375, 0.6834564208984375, -0.7408561706542969, -0.69549560546875, 1.8336868286132812, -5.073463439941406, -0.32694244384765625, -0.7294731140136719, -2.442310333251953, 0.5721015930175781, 0.11392593383789062, 0.02703094482421875, -1.2660713195800781, 4.289703369140625, -1.60833740234375, -6.510555267333984, -2.618640899658203, -0.28466033935546875, -2.1916656494140625, 1.3387184143066406, -0.9865226745605469, 0.49810791015625, 0.6702728271484375, 0.3988761901855469, 0.5786628723144531, 0.3165702819824219, -0.008453369140625, -0.6275558471679688, -157.5718994140625, -0.056720733642578125, 0.5553436279296875], "mean_td_error": 1.2674620151519775}}, "num_steps_sampled": 39000, "num_agent_steps_sampled": 78000, "num_steps_trained": 1216256, "num_steps_trained_this_iter": 256, "num_agent_steps_trained": 2432512, "last_target_update_ts": 38800, "num_target_updates": 76}, "done": false, "episodes_total": 64, "training_iteration": 39, "trial_id": "e21e6_00000", "experiment_id": "434cd42d33fd4b638f17fc69fa01d74f", "date": "2022-06-14_19-15-34", "timestamp": 1655248534, "time_this_iter_s": 21.737507581710815, "time_total_s": 813.6056680679321, "pid": 2568314, "hostname": "lambda-dual", "node_ip": "10.104.51.19", "config": {"num_workers": 2, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.0005, "train_batch_size": 256, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "cnet", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "1v1env", "observation_space": null, "action_space": null, "env_config": {}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": true, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": null, "evaluation_duration": 10, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"num_workers": 2, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.0005, "train_batch_size": 256, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "cnet", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "1v1env", "observation_space": null, "action_space": null, "env_config": {}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": false, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": null, "evaluation_duration": 10, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"explore": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": true, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": null, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 2, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"policy_01": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]], [[[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]], (3, 64, 64), float32)", "Discrete(7)", {}], "policy_02": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]], [[[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]], (3, 64, 64), float32)", "Discrete(7)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": "<function <lambda> at 0x7f1aa3764680>", "policies_to_train": ["policy_01"], "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": 100000, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": false, "hiddens": [], "double_q": false, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": true, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": null, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 2, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"policy_01": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]], [[[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]], (3, 64, 64), float32)", "Discrete(7)", {}], "policy_02": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]], [[[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]], (3, 64, 64), float32)", "Discrete(7)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": "<function <lambda> at 0x7f1aa3764680>", "policies_to_train": ["policy_01"], "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": 100000, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": false, "hiddens": [], "double_q": false, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "time_since_restore": 813.6056680679321, "timesteps_since_restore": 9984, "iterations_since_restore": 39, "warmup_time": 45.46659183502197, "perf": {"cpu_util_percent": 24.700000000000003, "ram_util_percent": 12.19354838709677}}
{"episode_reward_max": 441.0, "episode_reward_min": 0.0, "episode_reward_mean": 29.818181818181817, "episode_len_mean": 601.0, "episode_media": {}, "episodes_this_iter": 2, "policy_reward_min": {"policy_01": 0.0, "policy_02": 0.0}, "policy_reward_max": {"policy_01": 441.0, "policy_02": 441.0}, "policy_reward_mean": {"policy_01": 16.454545454545453, "policy_02": 13.363636363636363}, "custom_metrics": {}, "hist_stats": {"episode_reward": [0.0, 204.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0], "episode_lengths": [601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601], "policy_policy_01_reward": [0.0, 204.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "policy_policy_02_reward": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.33590605604498613, "mean_inference_ms": 5.816473279365569, "mean_action_processing_ms": 0.08896705189488577, "mean_env_wait_ms": 8.03103955812269, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 2, "timesteps_total": 40000, "timesteps_this_iter": 256, "agent_timesteps_total": 80000, "timers": {"load_time_ms": 1.362, "load_throughput": 188022.804, "learn_time_ms": 11.541, "learn_throughput": 22181.403, "update_time_ms": 2.345}, "info": {"learner": {"policy_01": {"custom_metrics": {}, "learner_stats": {"mean_q": 85.55427551269531, "min_q": 42.93935012817383, "max_q": 517.1806640625, "cur_lr": 0.0005}, "model": {}, "td_error": [-0.728851318359375, 0.36585235595703125, 0.08377456665039062, -0.2293853759765625, -0.02828216552734375, 0.4572868347167969, 0.22600173950195312, 0.14268875122070312, -1.9212493896484375, 1.341033935546875, 0.43775177001953125, 4.921405792236328, 0.3089027404785156, 0.47797393798828125, 0.045696258544921875, 0.4175300598144531, -4.8863067626953125, 1.9672088623046875, -0.6472816467285156, 1.9181785583496094, 0.373077392578125, -0.5458335876464844, -0.3009033203125, -0.8877716064453125, -0.21158981323242188, -0.3132209777832031, -1.68157958984375, 0.573455810546875, 0.3175315856933594, 0.7621421813964844, 0.6689949035644531, -0.09670639038085938, 1.3301124572753906, -21.175186157226562, -1.7958030700683594, 10.17303466796875, -27.987205505371094, 1.1579742431640625, 2.2750587463378906, -0.34867095947265625, 0.13411331176757812, 1.9760246276855469, 0.25545501708984375, -0.28814697265625, -0.7519187927246094, -5.0457763671875, -10.29458999633789, -3.3637542724609375, 5.094879150390625, 3.567760467529297, 0.3838462829589844, 2.2763290405273438, 0.20837020874023438, 0.110504150390625, -0.6282081604003906, -0.15742111206054688, -0.93621826171875, -0.1519775390625, 0.7092742919921875, 0.6812400817871094, 9.330169677734375, -2.155181884765625, 7.231494903564453, -1.3967437744140625, 2.1828155517578125, -0.7336578369140625, -1.68157958984375, -0.044677734375, -0.3440742492675781, 0.3488311767578125, -0.4166145324707031, 0.971466064453125, -64.86538696289062, 0.46744537353515625, 0.893096923828125, 0.295867919921875, 43.38734436035156, -0.6320457458496094, -1.7532424926757812, 0.28057098388671875, 0.03650665283203125, 0.003753662109375, -0.117095947265625, 0.8029975891113281, -0.9869041442871094, -0.537994384765625, 0.012042999267578125, 2.843402862548828, -9.9757080078125, -0.029422760009765625, -152.2595977783203, -8.66253662109375, 3.2498741149902344, -0.4717445373535156, 0.7709121704101562, -0.4670906066894531, 44.438209533691406, -0.47281646728515625, 0.25429534912109375, -0.13198471069335938, -8.146896362304688, 0.801971435546875, -1.0499229431152344, 1.4991302490234375, 4.1593170166015625, -0.054935455322265625, -0.4755973815917969, 0.3395118713378906, -16.284286499023438, -0.8777732849121094, 41.71728515625, -0.7067298889160156, 0.3222198486328125, 0.4779014587402344, -5.307319641113281, -1.201263427734375, 44.420291900634766, -0.540374755859375, 0.8286399841308594, 0.8442268371582031, -0.179534912109375, -0.06137847900390625, 0.24190902709960938, -6.9224395751953125, -0.8263893127441406, -0.4770545959472656, 3.5821151733398438, 0.16547775268554688, 1.1590576171875, 2.3629302978515625, 0.042324066162109375, -21.089576721191406, -0.3089599609375, -1.781646728515625, -3.5172271728515625, 52.575565338134766, -8.599624633789062, -0.7931747436523438, 0.0625, -0.41454315185546875, -0.7604217529296875, -1.4762229919433594, 0.2342529296875, -0.3406105041503906, 0.4144401550292969, 49.28947448730469, -0.5588836669921875, -30.22021484375, 17.4549560546875, -2.8536148071289062, 11.354446411132812, 10.979965209960938, -0.175933837890625, -0.200103759765625, 0.3076591491699219, -1.0818595886230469, -2.3722991943359375, 20.338424682617188, -7.2010498046875, 3.3592872619628906, -0.014469146728515625, 0.026653289794921875, -9.621391296386719, 0.5602455139160156, -4.144203186035156, -1.766571044921875, -0.6257133483886719, 0.3204460144042969, 0.3635139465332031, -0.5060272216796875, 23.654571533203125, -0.1949005126953125, 0.6414260864257812, 0.23543167114257812, -0.041393280029296875, 0.5908012390136719, 0.053314208984375, 0.4346122741699219, -1.68157958984375, 1.1319236755371094, 2.331939697265625, -0.3425140380859375, 181.44985961914062, -1.7278594970703125, 1.6227149963378906, -2.006805419921875, 0.1063385009765625, 0.5748863220214844, -0.731903076171875, -1.2106208801269531, -3.728179931640625, 0.3821372985839844, -0.09108734130859375, -0.014270782470703125, 1.4879570007324219, 0.6222763061523438, -27.359046936035156, -3.2640304565429688, 47.98479461669922, 0.4756584167480469, 1.68194580078125, 1.353240966796875, 16.36041259765625, 0.34900665283203125, -8.86834716796875, 0.18901443481445312, -9.24981689453125, 1.3290786743164062, 0.1236419677734375, 0.07544326782226562, -92.451904296875, 2.0571060180664062, 0.15583419799804688, -5.388988494873047, -1.9607658386230469, -0.9912338256835938, -8.525726318359375, -9.754364013671875, 0.3776741027832031, -1.5115890502929688, 0.16942214965820312, -0.199737548828125, -0.7289772033691406, 0.912841796875, -4.345558166503906, 0.6419944763183594, -0.5864906311035156, -0.3672523498535156, -1.01409912109375, -0.5179901123046875, 0.021434783935546875, -0.2565956115722656, -0.05999755859375, 0.2666664123535156, 0.37125396728515625, 0.3981590270996094, 0.963958740234375, -0.6040878295898438, 0.5245513916015625, 8.055282592773438, -1.804901123046875, 0.3203163146972656, 0.32071685791015625, 0.06291961669921875, 1.224456787109375, -0.37378692626953125, -0.34813690185546875, 1.2906227111816406, -2.366912841796875, -0.5217781066894531, -1.0142936706542969, -5.0070343017578125, -0.7474288940429688, 5.121063232421875, 0.5681304931640625, -1.3237533569335938], "mean_td_error": 0.24708056449890137}}, "num_steps_sampled": 40000, "num_agent_steps_sampled": 80000, "num_steps_trained": 1248256, "num_steps_trained_this_iter": 256, "num_agent_steps_trained": 2496512, "last_target_update_ts": 39808, "num_target_updates": 78}, "done": false, "episodes_total": 66, "training_iteration": 40, "trial_id": "e21e6_00000", "experiment_id": "434cd42d33fd4b638f17fc69fa01d74f", "date": "2022-06-14_19-15-56", "timestamp": 1655248556, "time_this_iter_s": 21.60704731941223, "time_total_s": 835.2127153873444, "pid": 2568314, "hostname": "lambda-dual", "node_ip": "10.104.51.19", "config": {"num_workers": 2, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.0005, "train_batch_size": 256, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "cnet", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "1v1env", "observation_space": null, "action_space": null, "env_config": {}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": true, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": null, "evaluation_duration": 10, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"num_workers": 2, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.0005, "train_batch_size": 256, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "cnet", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "1v1env", "observation_space": null, "action_space": null, "env_config": {}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": false, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": null, "evaluation_duration": 10, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"explore": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": true, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": null, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 2, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"policy_01": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]], [[[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]], (3, 64, 64), float32)", "Discrete(7)", {}], "policy_02": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]], [[[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]], (3, 64, 64), float32)", "Discrete(7)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": "<function <lambda> at 0x7f1aa3764b90>", "policies_to_train": ["policy_01"], "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": 100000, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": false, "hiddens": [], "double_q": false, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": true, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": null, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 2, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"policy_01": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]], [[[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]], (3, 64, 64), float32)", "Discrete(7)", {}], "policy_02": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]], [[[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]], (3, 64, 64), float32)", "Discrete(7)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": "<function <lambda> at 0x7f1aa3764b90>", "policies_to_train": ["policy_01"], "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": 100000, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": false, "hiddens": [], "double_q": false, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "time_since_restore": 835.2127153873444, "timesteps_since_restore": 10240, "iterations_since_restore": 40, "warmup_time": 45.46659183502197, "perf": {"cpu_util_percent": 24.816129032258058, "ram_util_percent": 12.277419354838711}}
{"episode_reward_max": 441.0, "episode_reward_min": 0.0, "episode_reward_mean": 28.941176470588236, "episode_len_mean": 601.0, "episode_media": {}, "episodes_this_iter": 2, "policy_reward_min": {"policy_01": 0.0, "policy_02": 0.0}, "policy_reward_max": {"policy_01": 441.0, "policy_02": 441.0}, "policy_reward_mean": {"policy_01": 15.970588235294118, "policy_02": 12.970588235294118}, "custom_metrics": {}, "hist_stats": {"episode_reward": [0.0, 204.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "episode_lengths": [601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601], "policy_policy_01_reward": [0.0, 204.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "policy_policy_02_reward": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.3358329409641562, "mean_inference_ms": 5.815173745673145, "mean_action_processing_ms": 0.08895627934918812, "mean_env_wait_ms": 8.02404165429502, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 2, "timesteps_total": 41000, "timesteps_this_iter": 256, "agent_timesteps_total": 82000, "timers": {"load_time_ms": 1.309, "load_throughput": 195535.087, "learn_time_ms": 11.705, "learn_throughput": 21870.473, "update_time_ms": 2.218}, "info": {"learner": {"policy_01": {"custom_metrics": {}, "learner_stats": {"mean_q": 82.6220703125, "min_q": 47.79618835449219, "max_q": 449.2841796875, "cur_lr": 0.0005}, "model": {}, "td_error": [-23.66461181640625, -1.2709732055664062, 2.664642333984375, 9.610122680664062, 2.908649444580078, 3.0854263305664062, 4.05108642578125, 4.877525329589844, -6.347278594970703, 2.043914794921875, 4.993595123291016, -2.9623870849609375, 4.0153045654296875, 1.9375686645507812, 2.7606124877929688, 1.9661064147949219, -11.646759033203125, 2.006671905517578, 4.27813720703125, 1.3432960510253906, 1.6056556701660156, 2.2192001342773438, 0.6276130676269531, 3.13995361328125, 1.9568634033203125, -13.240631103515625, 0.48565673828125, 1.1132774353027344, 5.8109893798828125, 7.5623779296875, 1.9539756774902344, 2.4624862670898438, 4.191001892089844, -0.3912506103515625, 14.081573486328125, 20.42132568359375, 3.791595458984375, 48.56480407714844, -18.39282989501953, 3.0893592834472656, 2.4364166259765625, 0.2474212646484375, 3.2254562377929688, 0.548980712890625, 1.0078620910644531, 3.8006362915039062, 1.7716445922851562, -8.655532836914062, 2.6472320556640625, 15.213607788085938, 0.2645759582519531, 2.093181610107422, 2.6471710205078125, 7.702995300292969, 2.034626007080078, 1.4245262145996094, -6.257190704345703, 1.1238632202148438, 2.0232086181640625, -3.2056121826171875, 52.37297058105469, 12.913726806640625, 2.9289894104003906, 0.19919967651367188, 19.503631591796875, 2.4607505798339844, -0.026760101318359375, 3.1028289794921875, 2.1008872985839844, -7.1366424560546875, 3.044963836669922, -2.7267608642578125, 2.1384811401367188, 1.9292068481445312, 1.5107460021972656, 1.7517662048339844, -5.447479248046875, 3.036102294921875, 3.533649444580078, -1.4528236389160156, 3.8212432861328125, 0.5088348388671875, 2.4222755432128906, 13.800567626953125, -3.879119873046875, 4.831531524658203, 1.5648841857910156, 0.17062759399414062, -1.9367713928222656, 1.5341758728027344, 3.061981201171875, 3.007465362548828, 0.36721038818359375, 1.5045013427734375, 2.5123252868652344, 2.986286163330078, 3.3423385620117188, 4.706615447998047, 1.2638206481933594, 0.7994003295898438, 13.006561279296875, 1.9519271850585938, 1.23748779296875, -0.5973052978515625, 2.2620582580566406, 1.8130645751953125, 2.1568222045898438, -10.344940185546875, 2.0183868408203125, 12.80120849609375, -0.9762725830078125, 2.5127639770507812, 2.3575477600097656, 10.801429748535156, 2.3814163208007812, 0.124664306640625, 1.2337646484375, 3.027568817138672, 0.7052536010742188, 0.08107376098632812, 2.008983612060547, 5.457649230957031, 3.0891189575195312, 2.4478111267089844, -0.586151123046875, -0.4349403381347656, 4.493862152099609, -15.261421203613281, 8.941680908203125, 0.4427680969238281, 2.240184783935547, 0.9287033081054688, 1.7928848266601562, -0.4795417785644531, 53.56964111328125, 0.9166717529296875, -9.867034912109375, -0.7631072998046875, 3.1743125915527344, 1.5049552917480469, 2.16729736328125, 6.368278503417969, 0.6083641052246094, -1.4041976928710938, 1.7883987426757812, 2.486103057861328, 2.800628662109375, 3.7854156494140625, -0.20135116577148438, -2.067901611328125, -0.00225067138671875, 0.89202880859375, 4.175922393798828, 52.50108337402344, 1.5589027404785156, 2.8450889587402344, 7.663627624511719, 1.8513717651367188, 2.47698974609375, 1.4204940795898438, 7.733440399169922, 0.04784393310546875, -0.5677490234375, 0.8787307739257812, 1.3372535705566406, 1.4490280151367188, -4.170135498046875, 5.020843505859375, 17.826950073242188, 1.6352195739746094, -121.321533203125, 2.129741668701172, 2.0506553649902344, -0.0655517578125, 3.4597625732421875, 2.3245468139648438, 1.8117218017578125, 5.522308349609375, 0.8882026672363281, 2.042572021484375, 2.353801727294922, 5.137138366699219, 4.2507781982421875, 1.8123931884765625, 12.987701416015625, -1.2709732055664062, 2.937976837158203, -0.1517333984375, 3.1153907775878906, 2.8259735107421875, 3.6352195739746094, 1.794708251953125, 1.9342422485351562, -5.2610015869140625, 2.0122833251953125, -0.4066429138183594, 1.595458984375, 1.8637657165527344, 3.4838104248046875, -6.803276062011719, 3.3473892211914062, 1.6491889953613281, 3.830963134765625, -2.0412635803222656, 0.5811042785644531, 13.26662826538086, 3.899322509765625, 0.9039268493652344, 2.0067787170410156, -0.08739089965820312, 1.9505653381347656, 2.511016845703125, -0.06543350219726562, 2.2510910034179688, -51.30296325683594, 4.330760955810547, 0.4928321838378906, 0.03644561767578125, 6.4207000732421875, 1.8109283447265625, 5.211467742919922, 49.85514831542969, 3.792510986328125, 2.8689041137695312, 3.000823974609375, 2.6032333374023438, -0.218597412109375, 1.2883949279785156, -3.511737823486328, 2.2909202575683594, 0.9174537658691406, 2.124187469482422, 1.1509857177734375, 1.1528701782226562, -3.102325439453125, 0.16513442993164062, 1.1828765869140625, 2.8541717529296875, 4.207054138183594, 5.107364654541016, 3.1887435913085938, 3.4151229858398438, 3.093952178955078, 2.24896240234375, 3.58831787109375, 2.838520050048828, -7.098987579345703, -0.37105560302734375, 1.8443222045898438, -146.4878387451172, 2.9915504455566406, 1.5065231323242188, -0.6373100280761719, 2.5509185791015625, 4.5966033935546875, 2.132953643798828], "mean_td_error": 1.5696396827697754}}, "num_steps_sampled": 41000, "num_agent_steps_sampled": 82000, "num_steps_trained": 1280256, "num_steps_trained_this_iter": 256, "num_agent_steps_trained": 2560512, "last_target_update_ts": 40816, "num_target_updates": 80}, "done": false, "episodes_total": 68, "training_iteration": 41, "trial_id": "e21e6_00000", "experiment_id": "434cd42d33fd4b638f17fc69fa01d74f", "date": "2022-06-14_19-16-18", "timestamp": 1655248578, "time_this_iter_s": 21.837355613708496, "time_total_s": 857.0500710010529, "pid": 2568314, "hostname": "lambda-dual", "node_ip": "10.104.51.19", "config": {"num_workers": 2, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.0005, "train_batch_size": 256, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "cnet", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "1v1env", "observation_space": null, "action_space": null, "env_config": {}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": true, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": null, "evaluation_duration": 10, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"num_workers": 2, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.0005, "train_batch_size": 256, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "cnet", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "1v1env", "observation_space": null, "action_space": null, "env_config": {}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": false, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": null, "evaluation_duration": 10, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"explore": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": true, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": null, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 2, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"policy_01": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]], [[[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]], (3, 64, 64), float32)", "Discrete(7)", {}], "policy_02": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]], [[[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]], (3, 64, 64), float32)", "Discrete(7)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": "<function <lambda> at 0x7f1aa372a3b0>", "policies_to_train": ["policy_01"], "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": 100000, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": false, "hiddens": [], "double_q": false, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": true, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": null, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 2, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"policy_01": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]], [[[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]], (3, 64, 64), float32)", "Discrete(7)", {}], "policy_02": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]], [[[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]], (3, 64, 64), float32)", "Discrete(7)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": "<function <lambda> at 0x7f1aa372a3b0>", "policies_to_train": ["policy_01"], "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": 100000, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": false, "hiddens": [], "double_q": false, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "time_since_restore": 857.0500710010529, "timesteps_since_restore": 10496, "iterations_since_restore": 41, "warmup_time": 45.46659183502197, "perf": {"cpu_util_percent": 24.9258064516129, "ram_util_percent": 12.36129032258064}}
{"episode_reward_max": 441.0, "episode_reward_min": 0.0, "episode_reward_mean": 28.941176470588236, "episode_len_mean": 601.0, "episode_media": {}, "episodes_this_iter": 0, "policy_reward_min": {"policy_01": 0.0, "policy_02": 0.0}, "policy_reward_max": {"policy_01": 441.0, "policy_02": 441.0}, "policy_reward_mean": {"policy_01": 15.970588235294118, "policy_02": 12.970588235294118}, "custom_metrics": {}, "hist_stats": {"episode_reward": [0.0, 204.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "episode_lengths": [601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601], "policy_policy_01_reward": [0.0, 204.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "policy_policy_02_reward": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.3358329409641562, "mean_inference_ms": 5.815173745673145, "mean_action_processing_ms": 0.08895627934918812, "mean_env_wait_ms": 8.02404165429502, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 2, "timesteps_total": 42000, "timesteps_this_iter": 256, "agent_timesteps_total": 84000, "timers": {"load_time_ms": 1.326, "load_throughput": 193001.011, "learn_time_ms": 12.232, "learn_throughput": 20928.926, "update_time_ms": 2.387}, "info": {"learner": {"policy_01": {"custom_metrics": {}, "learner_stats": {"mean_q": 69.46734619140625, "min_q": 41.406227111816406, "max_q": 442.32635498046875, "cur_lr": 0.0005}, "model": {}, "td_error": [0.5285606384277344, 0.800506591796875, -0.290435791015625, -28.291473388671875, -0.173919677734375, 0.3604011535644531, -50.885009765625, -1.3463592529296875, 6.957401275634766, -0.2622566223144531, -1.120880126953125, 0.45801544189453125, -1.019500732421875, -1.2654266357421875, 3.006072998046875, -0.3775367736816406, 0.9952850341796875, 2.4755935668945312, -1.4304618835449219, -15.45513916015625, -2.0759124755859375, -1.5187034606933594, -0.3170433044433594, 0.2788505554199219, 0.6737060546875, -0.2133026123046875, 0.8619461059570312, -1.0020675659179688, -1.2152252197265625, -2.363597869873047, -1.7983283996582031, -1.7632064819335938, -1.990081787109375, -1.6841506958007812, -0.62109375, -0.005458831787109375, 316.6604309082031, -1.2432632446289062, -6.599822998046875, -0.4975433349609375, 0.01476287841796875, 1.063201904296875, 0.42478179931640625, -0.3458404541015625, -0.48078155517578125, -0.42504119873046875, -1.94903564453125, 0.4761314392089844, -1.1871566772460938, -0.5038719177246094, -0.7931976318359375, 0.025417327880859375, 0.5944023132324219, 3.738208770751953, -3.12982177734375, -0.5738945007324219, 0.3671531677246094, -0.4984626770019531, -1.3339881896972656, -0.3214149475097656, -0.4466361999511719, -1.0594100952148438, -1.3021469116210938, -1.4456787109375, 6.80029296875, -0.902557373046875, -0.20252609252929688, -1.9284210205078125, 14.656532287597656, 0.6261787414550781, -3.0396347045898438, 11.50973892211914, -0.8758697509765625, -0.9023017883300781, -1.7148284912109375, -0.5653877258300781, 3.4352951049804688, -1.5395736694335938, 0.40594482421875, -7.766105651855469, -1.3893966674804688, -0.27289581298828125, 2.8582000732421875, -0.9989089965820312, -3.5728607177734375, 0.19639968872070312, -0.0067291259765625, -0.5421028137207031, -0.6646766662597656, -3.0624847412109375, -0.5906486511230469, -3.222259521484375, -2.6227149963378906, 0.9543571472167969, -0.7410964965820312, 0.4151115417480469, -0.817230224609375, -14.259002685546875, -6.5033721923828125, 1.7704048156738281, 0.8929786682128906, 5.017845153808594, -0.6464691162109375, -2.4163246154785156, -6.4169921875, -0.8041839599609375, -1.5142936706542969, -8.302642822265625, -0.8558349609375, -5.343719482421875, -0.11675643920898438, 0.3325653076171875, -0.15251541137695312, -3.1308975219726562, -10.721298217773438, -1.5831069946289062, -0.821990966796875, -0.09960174560546875, -0.18386459350585938, -1.8311691284179688, -15.260139465332031, -1.7039222717285156, -0.8113021850585938, -0.611572265625, -1.7670555114746094, -1.2288475036621094, 0.8661689758300781, -0.8155593872070312, -0.9884300231933594, -1.6339874267578125, -6.572483062744141, -13.519214630126953, -4.4107666015625, 0.7780036926269531, -3.1971893310546875, -3.12982177734375, -2.2591781616210938, 0.16046905517578125, 0.07379150390625, -5.3101043701171875, -0.03011322021484375, -7.4029998779296875, 0.5976066589355469, -0.5927658081054688, -1.2654266357421875, 0.20125198364257812, -1.3885002136230469, -0.2803459167480469, 1.8068923950195312, -0.7952613830566406, -0.4817771911621094, -0.8906173706054688, -10.920215606689453, -0.1487274169921875, 0.5607452392578125, 0.2778892517089844, -1.2284393310546875, -0.6196670532226562, 49.85093688964844, -2.395030975341797, -15.45513916015625, -0.4391059875488281, -20.22998046875, -1.4587326049804688, -1.6429824829101562, -1.5978813171386719, -1.3690032958984375, 1.5320930480957031, -0.4841766357421875, -0.24249267578125, 0.053863525390625, 2.0515174865722656, -5.3820648193359375, -9.702129364013672, -0.5141258239746094, 0.5149307250976562, -1.6601409912109375, 0.2743568420410156, -1.8641204833984375, -1.3533706665039062, 0.3051948547363281, -1.0391464233398438, 0.7105560302734375, -0.6028633117675781, 0.12990570068359375, 1.9582595825195312, -1.8375282287597656, 0.092926025390625, -1.2499618530273438, -0.3468666076660156, 0.5894699096679688, 0.5124702453613281, -1.3275222778320312, -1.4076194763183594, -3.350841522216797, -7.557716369628906, 0.032474517822265625, -1.5774993896484375, -0.5409736633300781, -1.1510391235351562, -1.5301971435546875, -0.9813766479492188, -1.0556221008300781, -8.590606689453125, -1.2573966979980469, 1.2093505859375, 0.6993522644042969, -1.0740814208984375, -16.78958511352539, -1.2609481811523438, -0.8435249328613281, 316.6604309082031, -2.9236526489257812, -1.4864845275878906, 0.6267166137695312, -0.385650634765625, -1.170379638671875, 0.04100799560546875, 28.334136962890625, -0.5618743896484375, 0.918304443359375, -0.745635986328125, 0.036968231201171875, -1.401641845703125, -0.387115478515625, -0.9268531799316406, -4.884479522705078, 0.18489837646484375, -0.2442779541015625, -1.5619125366210938, -1.3417816162109375, -1.2133522033691406, 0.3751869201660156, -1.0751228332519531, -0.3242530822753906, -0.9108505249023438, -2.235687255859375, -0.8149452209472656, -0.1327972412109375, -3.7551651000976562, 0.2296600341796875, -0.17426300048828125, -2.329681396484375, -0.738067626953125, -0.72698974609375, -0.09304428100585938, -0.11323928833007812, -2.674785614013672, 0.3042869567871094, 0.6737060546875, 0.17722702026367188, -0.8475379943847656, -2.5845489501953125, 0.32773590087890625, -1.1684226989746094, -0.6136627197265625], "mean_td_error": 1.2035884857177734}}, "num_steps_sampled": 42000, "num_agent_steps_sampled": 84000, "num_steps_trained": 1312256, "num_steps_trained_this_iter": 256, "num_agent_steps_trained": 2624512, "last_target_update_ts": 41824, "num_target_updates": 82}, "done": false, "episodes_total": 68, "training_iteration": 42, "trial_id": "e21e6_00000", "experiment_id": "434cd42d33fd4b638f17fc69fa01d74f", "date": "2022-06-14_19-16-39", "timestamp": 1655248599, "time_this_iter_s": 21.71528911590576, "time_total_s": 878.7653601169586, "pid": 2568314, "hostname": "lambda-dual", "node_ip": "10.104.51.19", "config": {"num_workers": 2, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.0005, "train_batch_size": 256, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "cnet", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "1v1env", "observation_space": null, "action_space": null, "env_config": {}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": true, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": null, "evaluation_duration": 10, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"num_workers": 2, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.0005, "train_batch_size": 256, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "cnet", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "1v1env", "observation_space": null, "action_space": null, "env_config": {}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": false, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": null, "evaluation_duration": 10, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"explore": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": true, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": null, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 2, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"policy_01": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]], [[[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]], (3, 64, 64), float32)", "Discrete(7)", {}], "policy_02": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]], [[[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]], (3, 64, 64), float32)", "Discrete(7)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": "<function <lambda> at 0x7f1aa372a7a0>", "policies_to_train": ["policy_01"], "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": 100000, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": false, "hiddens": [], "double_q": false, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": true, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": null, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 2, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"policy_01": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]], [[[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]], (3, 64, 64), float32)", "Discrete(7)", {}], "policy_02": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]], [[[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]], (3, 64, 64), float32)", "Discrete(7)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": "<function <lambda> at 0x7f1aa372a7a0>", "policies_to_train": ["policy_01"], "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": 100000, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": false, "hiddens": [], "double_q": false, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "time_since_restore": 878.7653601169586, "timesteps_since_restore": 10752, "iterations_since_restore": 42, "warmup_time": 45.46659183502197, "perf": {"cpu_util_percent": 25.041935483870965, "ram_util_percent": 12.451612903225806}}
{"episode_reward_max": 441.0, "episode_reward_min": 0.0, "episode_reward_mean": 28.114285714285714, "episode_len_mean": 601.0, "episode_media": {}, "episodes_this_iter": 2, "policy_reward_min": {"policy_01": 0.0, "policy_02": 0.0}, "policy_reward_max": {"policy_01": 441.0, "policy_02": 441.0}, "policy_reward_mean": {"policy_01": 15.514285714285714, "policy_02": 12.6}, "custom_metrics": {}, "hist_stats": {"episode_reward": [0.0, 204.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "episode_lengths": [601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601], "policy_policy_01_reward": [0.0, 204.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "policy_policy_02_reward": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.3357174425278322, "mean_inference_ms": 5.813817943374587, "mean_action_processing_ms": 0.08894456199548113, "mean_env_wait_ms": 8.017267813345947, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 2, "timesteps_total": 43000, "timesteps_this_iter": 256, "agent_timesteps_total": 86000, "timers": {"load_time_ms": 1.315, "load_throughput": 194663.033, "learn_time_ms": 11.771, "learn_throughput": 21748.696, "update_time_ms": 2.267}, "info": {"learner": {"policy_01": {"custom_metrics": {}, "learner_stats": {"mean_q": 86.62623596191406, "min_q": 46.59125900268555, "max_q": 613.4393310546875, "cur_lr": 0.0005}, "model": {}, "td_error": [1.3695220947265625, 0.001827239990234375, 1.2337532043457031, -0.492919921875, 0.7754440307617188, 1.4936904907226562, 1.2896308898925781, -19.72463607788086, -103.96165466308594, 8.964859008789062, 2.8551902770996094, 10.323471069335938, 1.0000572204589844, 24.211669921875, 0.7478294372558594, 1.3540992736816406, 1.5661697387695312, 1.4544410705566406, 0.3940849304199219, 6.565277099609375, 0.48150634765625, -1.1947174072265625, 1.1387786865234375, -0.5989227294921875, 2.52899169921875, 2.9503402709960938, 1.4360275268554688, 4.442943572998047, -1.7042427062988281, 0.034572601318359375, 1.5373916625976562, 1.2103080749511719, 1.7657661437988281, 2.687480926513672, -0.0825958251953125, 0.07299423217773438, 1.1194686889648438, -10.471183776855469, 1.678680419921875, 2.1291580200195312, 1.1502151489257812, -2.2587623596191406, 2.785938262939453, -1.1049728393554688, 0.4810028076171875, 0.41280364990234375, 2.636383056640625, 3.0906639099121094, 0.23992538452148438, 18.731903076171875, 1.0870475769042969, 2.431671142578125, 1.5810127258300781, 2.4383888244628906, 2.9360885620117188, 2.5461578369140625, 1.2032089233398438, -0.6068840026855469, 10.10528564453125, 2.933887481689453, 0.24596786499023438, 0.15325927734375, -0.09600448608398438, -0.6448631286621094, 1.7845344543457031, 55.048309326171875, 1.5116729736328125, 3.5062942504882812, -0.2519683837890625, 2.3224525451660156, 1.5766525268554688, 7.3378753662109375, 2.5179176330566406, 2.5948448181152344, 1.7734107971191406, -1.8936195373535156, 21.04993438720703, 2.530975341796875, -6.776798248291016, 0.33394622802734375, 1.5935020446777344, 0.69342041015625, 0.20452880859375, -0.0458984375, 1.2644004821777344, 3.002044677734375, -6.065116882324219, 0.5635604858398438, 16.332733154296875, 2.1068801879882812, -1.4525871276855469, 1.5343437194824219, 1.1681327819824219, 4.486454010009766, -1.8259925842285156, -2.213848114013672, 2.6373863220214844, 1.7534523010253906, -2.5003395080566406, 2.0634994506835938, 1.35162353515625, -0.8620491027832031, 3.7517471313476562, 1.964141845703125, 0.7119979858398438, 10.424392700195312, 1.3331680297851562, 10.308425903320312, -0.3966941833496094, 1.6685943603515625, 2.62579345703125, 1.3318901062011719, 1.3929252624511719, 2.324993133544922, -0.6190147399902344, 2.1138839721679688, 1.0660285949707031, 48.99393844604492, 0.7183914184570312, -0.733123779296875, 7.2560272216796875, 3.3884620666503906, 1.2471961975097656, -1.534942626953125, 1.7651939392089844, -0.584991455078125, 1.5415267944335938, 0.22417449951171875, 2.4590072631835938, -0.13465118408203125, -40.80645751953125, -0.24232864379882812, 3.8773040771484375, -109.29025268554688, 1.8261947631835938, 5.2923583984375, 36.480918884277344, 2.1848373413085938, 2.044189453125, -1.329864501953125, -45.45018005371094, -15.0762939453125, 10.3890380859375, -5.16693115234375, 0.3294525146484375, 4.6739959716796875, -1.80084228515625, 2.156513214111328, -1.0807228088378906, -0.9692726135253906, -13.64898681640625, 1.0962181091308594, 1.8817787170410156, 0.664764404296875, 1.0716171264648438, 1.9644432067871094, -4.5169525146484375, 19.05068588256836, 0.043010711669921875, 2.1548233032226562, 2.19879150390625, 1.0745735168457031, -102.77485656738281, 1.8078536987304688, 0.7368545532226562, 0.6040191650390625, 1.0081977844238281, 51.39805603027344, -0.6241912841796875, 2.1020545959472656, 7.2560272216796875, 1.1971702575683594, -2.456951141357422, 0.3912696838378906, 14.809036254882812, 8.394882202148438, -1.3948593139648438, 1.1690597534179688, 0.32543182373046875, -2.6852760314941406, 0.004150390625, 17.188323974609375, -1.1815948486328125, 3.380908966064453, 2.000507354736328, 1.748199462890625, 1.2195968627929688, 0.641937255859375, -0.9452438354492188, 0.3409080505371094, -0.6821861267089844, 1.0310096740722656, 1.1054039001464844, -5.2308349609375, 10.661590576171875, -2.3654518127441406, 0.4582633972167969, -0.02407073974609375, 2.146106719970703, 3.0530242919921875, 0.6483116149902344, -36.607967376708984, -1.15185546875, 3.9803619384765625, 1.3690147399902344, -18.06793212890625, 2.1858863830566406, -3.792064666748047, 2.3748855590820312, 0.5478248596191406, 1.2101631164550781, -2.8945541381835938, 3.380908966064453, -0.6680374145507812, 2.4072532653808594, 2.0837631225585938, 0.566925048828125, -1.0855598449707031, 441.5956115722656, 1.1834640502929688, -2.9890289306640625, 0.67047119140625, 0.5925369262695312, -0.602081298828125, 1.5723304748535156, 1.1763763427734375, 1.6031913757324219, 3.7926559448242188, -230.8056182861328, 1.9967918395996094, 0.1231536865234375, -0.7042007446289062, -1.1491050720214844, 0.6690254211425781, -7.747711181640625, 3.202930450439453, -0.21065521240234375, 1.9714851379394531, -6.191810607910156, 3.3614425659179688, -0.9975013732910156, 1.4178123474121094, 26.650054931640625, 1.6318702697753906, 0.9846649169921875, 1.242645263671875, 1.1483726501464844, 1.3337860107421875, 2.5389175415039062, 1.8677520751953125, 0.5958023071289062, -1.7041397094726562, 2.0352783203125, -0.424560546875, -2.0658493041992188, 14.349517822265625], "mean_td_error": 1.2793855667114258}}, "num_steps_sampled": 43000, "num_agent_steps_sampled": 86000, "num_steps_trained": 1344256, "num_steps_trained_this_iter": 256, "num_agent_steps_trained": 2688512, "last_target_update_ts": 42832, "num_target_updates": 84}, "done": false, "episodes_total": 70, "training_iteration": 43, "trial_id": "e21e6_00000", "experiment_id": "434cd42d33fd4b638f17fc69fa01d74f", "date": "2022-06-14_19-17-01", "timestamp": 1655248621, "time_this_iter_s": 21.61277961730957, "time_total_s": 900.3781397342682, "pid": 2568314, "hostname": "lambda-dual", "node_ip": "10.104.51.19", "config": {"num_workers": 2, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.0005, "train_batch_size": 256, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "cnet", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "1v1env", "observation_space": null, "action_space": null, "env_config": {}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": true, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": null, "evaluation_duration": 10, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"num_workers": 2, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.0005, "train_batch_size": 256, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "cnet", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "1v1env", "observation_space": null, "action_space": null, "env_config": {}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": false, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": null, "evaluation_duration": 10, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"explore": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": true, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": null, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 2, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"policy_01": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]], [[[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]], (3, 64, 64), float32)", "Discrete(7)", {}], "policy_02": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]], [[[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]], (3, 64, 64), float32)", "Discrete(7)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": "<function <lambda> at 0x7f1aa2ea4cb0>", "policies_to_train": ["policy_01"], "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": 100000, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": false, "hiddens": [], "double_q": false, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": true, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": null, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 2, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"policy_01": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]], [[[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]], (3, 64, 64), float32)", "Discrete(7)", {}], "policy_02": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]], [[[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]], (3, 64, 64), float32)", "Discrete(7)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": "<function <lambda> at 0x7f1aa2ea4cb0>", "policies_to_train": ["policy_01"], "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": 100000, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": false, "hiddens": [], "double_q": false, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "time_since_restore": 900.3781397342682, "timesteps_since_restore": 11008, "iterations_since_restore": 43, "warmup_time": 45.46659183502197, "perf": {"cpu_util_percent": 24.609677419354835, "ram_util_percent": 12.532258064516135}}
{"episode_reward_max": 441.0, "episode_reward_min": 0.0, "episode_reward_mean": 27.333333333333332, "episode_len_mean": 601.0, "episode_media": {}, "episodes_this_iter": 2, "policy_reward_min": {"policy_01": 0.0, "policy_02": 0.0}, "policy_reward_max": {"policy_01": 441.0, "policy_02": 441.0}, "policy_reward_mean": {"policy_01": 15.083333333333334, "policy_02": 12.25}, "custom_metrics": {}, "hist_stats": {"episode_reward": [0.0, 204.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "episode_lengths": [601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601], "policy_policy_01_reward": [0.0, 204.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "policy_policy_02_reward": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.3356183763245626, "mean_inference_ms": 5.812576268220798, "mean_action_processing_ms": 0.08893412811436135, "mean_env_wait_ms": 8.010824004880385, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 2, "timesteps_total": 44000, "timesteps_this_iter": 256, "agent_timesteps_total": 88000, "timers": {"load_time_ms": 1.311, "load_throughput": 195325.224, "learn_time_ms": 11.827, "learn_throughput": 21644.879, "update_time_ms": 2.281}, "info": {"learner": {"policy_01": {"custom_metrics": {}, "learner_stats": {"mean_q": 100.94927978515625, "min_q": 45.96084976196289, "max_q": 619.3868408203125, "cur_lr": 0.0005}, "model": {}, "td_error": [0.6583061218261719, 53.021419525146484, -45.96632385253906, -8.562728881835938, 5.235729217529297, -2.0602684020996094, 0.002170562744140625, 0.06504440307617188, 0.17350387573242188, -0.14220809936523438, -9.04901123046875, 0.720062255859375, -2.116809844970703, 49.60121154785156, -5.554168701171875, 15.1943359375, -6.19329833984375, -28.6715087890625, -2.0713043212890625, 7.7147216796875, -0.08411407470703125, -0.8119125366210938, 0.21430587768554688, -0.05884552001953125, 0.3526802062988281, 0.917938232421875, -3.0215301513671875, -0.42891693115234375, -6.21051025390625, 0.15313720703125, 0.6746826171875, -13.80548095703125, -0.004657745361328125, -0.31325531005859375, -0.745819091796875, 2.7567367553710938, 1.9881706237792969, 0.06760025024414062, 1.3282699584960938, 0.5646133422851562, 0.5130271911621094, 52.577064514160156, 49.109100341796875, -9.260967254638672, -0.8417282104492188, -0.08988571166992188, -0.14675521850585938, 1.1419944763183594, 0.14149856567382812, -0.17369842529296875, 0.0734710693359375, -11.822273254394531, -12.630691528320312, 1.3760643005371094, 2.3849220275878906, -3.6852569580078125, 1.9265670776367188, -1.6602325439453125, -3.4473724365234375, -0.060771942138671875, -1.2152595520019531, -3.6995697021484375, 0.5614089965820312, 1.2336578369140625, -0.346771240234375, 0.5666999816894531, 1.6875953674316406, 1.7407684326171875, -0.4388847351074219, -0.246063232421875, 0.9083061218261719, 3.4489288330078125, 0.11578750610351562, 3.7040939331054688, 0.3591880798339844, 0.8833198547363281, 47.81747055053711, 0.034503936767578125, -0.3923072814941406, 1.8350143432617188, 0.6175804138183594, 0.6774635314941406, 1.2005233764648438, 0.16241836547851562, -28.3482666015625, 0.23728561401367188, 4.1175079345703125, 0.7332115173339844, -27.515731811523438, -0.3136863708496094, -0.057796478271484375, 1.5143928527832031, -1.32440185546875, -7.985931396484375, 1.543487548828125, -0.21153640747070312, 0.8211441040039062, -3.0523529052734375, -0.6352081298828125, 0.23728561401367188, -0.02727508544921875, 46.873409271240234, 0.9275856018066406, 25.2894287109375, -0.73345947265625, -0.041698455810546875, 0.6461029052734375, 0.3095207214355469, 2.859649658203125, -0.06415939331054688, -0.05096435546875, 2.3248291015625, -1.415130615234375, 0.0355987548828125, 1.1594085693359375, -0.3034019470214844, -10.558990478515625, -1.4966888427734375, -0.5741691589355469, 1.6790313720703125, -0.4192008972167969, -114.7462158203125, 1.8164482116699219, -0.2676811218261719, 0.9619407653808594, 5.740962982177734, -1.1471176147460938, 0.162109375, -17.35723114013672, 1.1040611267089844, 0.5980033874511719, 13.470458984375, -4.8205413818359375, 0.8714218139648438, 1.1617279052734375, 3.9593505859375, -0.8413352966308594, -48.37577819824219, 0.8972091674804688, -3.529632568359375, -0.99017333984375, 0.7722930908203125, 1.3563766479492188, -0.037433624267578125, 1.7311782836914062, -1.3086624145507812, -0.003314971923828125, -0.001773834228515625, -0.6323509216308594, 0.4785423278808594, 1.253204345703125, -0.038219451904296875, 0.430938720703125, -0.929901123046875, 0.6107063293457031, 6.6103515625, -0.2510871887207031, -1.0208930969238281, 1.6762924194335938, 0.014560699462890625, 2.3657455444335938, -0.09276962280273438, 46.33219909667969, 1.0062675476074219, -0.610107421875, -0.3725547790527344, 0.42638397216796875, -11.962600708007812, -0.3497734069824219, 17.87189483642578, 0.5443038940429688, -0.4655609130859375, 0.09007644653320312, 15.970703125, 2.37591552734375, 2.630138397216797, -1.4654197692871094, -0.49951171875, 1.0233268737792969, -17.11416244506836, 2.0611724853515625, -1.5172157287597656, -8.8245849609375, -12.538764953613281, 1.0946235656738281, 0.49712371826171875, 52.577064514160156, 0.5396194458007812, -7.807403564453125, -0.4763031005859375, -1.0515594482421875, 0.3527107238769531, 0.18613815307617188, 3.1016807556152344, -4.3689117431640625, 146.30868530273438, -0.450592041015625, 0.7308235168457031, -1.2493896484375, 1.5630569458007812, 1.1839866638183594, 1.8672676086425781, -1.6865196228027344, -0.11973953247070312, 0.33963775634765625, -114.02920532226562, 10.930564880371094, 1.1859588623046875, -1.2205657958984375, -26.40569305419922, 0.8060264587402344, 0.13776779174804688, -0.32546234130859375, 0.12540817260742188, -3.191802978515625, 0.7933998107910156, 0.4533233642578125, 0.6656036376953125, -2.6021728515625, 1.1026268005371094, 480.1103820800781, 4.977684020996094, -0.5313377380371094, -3.0523529052734375, -1.4616813659667969, 0.09473419189453125, 1.0278358459472656, -0.21916580200195312, 1.0952415466308594, -1.4676628112792969, -0.980194091796875, -1.136474609375, -0.17017745971679688, 1.1986083984375, -0.3838691711425781, -6.066364288330078, -0.4847412109375, -0.3215980529785156, 2.5779190063476562, -0.21562576293945312, -0.1150054931640625, -0.10311508178710938, 189.17349243164062, 0.9445152282714844, -9.24676513671875, 0.16181182861328125, -2.535633087158203, -1.6780548095703125, -0.36080169677734375, 0.6759490966796875, -0.4697418212890625, 0.021312713623046875, -11.599403381347656, 1.3159828186035156, -72.4732666015625, 9.18930435180664], "mean_td_error": 2.592437982559204}}, "num_steps_sampled": 44000, "num_agent_steps_sampled": 88000, "num_steps_trained": 1376256, "num_steps_trained_this_iter": 256, "num_agent_steps_trained": 2752512, "last_target_update_ts": 43840, "num_target_updates": 86}, "done": false, "episodes_total": 72, "training_iteration": 44, "trial_id": "e21e6_00000", "experiment_id": "434cd42d33fd4b638f17fc69fa01d74f", "date": "2022-06-14_19-17-23", "timestamp": 1655248643, "time_this_iter_s": 21.92627763748169, "time_total_s": 922.3044173717499, "pid": 2568314, "hostname": "lambda-dual", "node_ip": "10.104.51.19", "config": {"num_workers": 2, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.0005, "train_batch_size": 256, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "cnet", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "1v1env", "observation_space": null, "action_space": null, "env_config": {}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": true, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": null, "evaluation_duration": 10, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"num_workers": 2, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.0005, "train_batch_size": 256, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "cnet", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "1v1env", "observation_space": null, "action_space": null, "env_config": {}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": false, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": null, "evaluation_duration": 10, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"explore": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": true, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": null, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 2, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"policy_01": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]], [[[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]], (3, 64, 64), float32)", "Discrete(7)", {}], "policy_02": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]], [[[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]], (3, 64, 64), float32)", "Discrete(7)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": "<function <lambda> at 0x7f1aa2ea4ef0>", "policies_to_train": ["policy_01"], "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": 100000, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": false, "hiddens": [], "double_q": false, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": true, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": null, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 2, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"policy_01": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]], [[[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]], (3, 64, 64), float32)", "Discrete(7)", {}], "policy_02": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]], [[[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]], (3, 64, 64), float32)", "Discrete(7)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": "<function <lambda> at 0x7f1aa2ea4ef0>", "policies_to_train": ["policy_01"], "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": 100000, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": false, "hiddens": [], "double_q": false, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "time_since_restore": 922.3044173717499, "timesteps_since_restore": 11264, "iterations_since_restore": 44, "warmup_time": 45.46659183502197, "perf": {"cpu_util_percent": 24.7375, "ram_util_percent": 12.6125}}
{"episode_reward_max": 441.0, "episode_reward_min": 0.0, "episode_reward_mean": 26.594594594594593, "episode_len_mean": 601.0, "episode_media": {}, "episodes_this_iter": 2, "policy_reward_min": {"policy_01": 0.0, "policy_02": 0.0}, "policy_reward_max": {"policy_01": 441.0, "policy_02": 441.0}, "policy_reward_mean": {"policy_01": 14.675675675675675, "policy_02": 11.91891891891892}, "custom_metrics": {}, "hist_stats": {"episode_reward": [0.0, 204.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "episode_lengths": [601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601], "policy_policy_01_reward": [0.0, 204.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "policy_policy_02_reward": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.33552980671437693, "mean_inference_ms": 5.811445441513561, "mean_action_processing_ms": 0.08892463909697679, "mean_env_wait_ms": 8.004651774522403, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 2, "timesteps_total": 45000, "timesteps_this_iter": 256, "agent_timesteps_total": 90000, "timers": {"load_time_ms": 1.334, "load_throughput": 191948.698, "learn_time_ms": 12.046, "learn_throughput": 21251.778, "update_time_ms": 2.365}, "info": {"learner": {"policy_01": {"custom_metrics": {}, "learner_stats": {"mean_q": 119.18018341064453, "min_q": 51.415672302246094, "max_q": 769.3934326171875, "cur_lr": 0.0005}, "model": {}, "td_error": [4.157032012939453, 3.4717178344726562, 2.9986648559570312, 4.990413665771484, 3.2745628356933594, 2.432964324951172, -5.19744873046875, 5.166965484619141, 3.0161056518554688, -8.976837158203125, 6.34539794921875, 0.48818206787109375, 2.673267364501953, 3.1142196655273438, 3.2170677185058594, -1.5436553955078125, 30.1727294921875, 4.046375274658203, 6.821544647216797, 2.761180877685547, -15.8228759765625, 12.91461181640625, 2.652904510498047, 3.608917236328125, 5.314952850341797, 6.3567657470703125, 4.681980133056641, 4.128997802734375, 6.737834930419922, 3.454853057861328, 2.7386474609375, -16.398941040039062, 5.2661590576171875, 2.3671798706054688, 3.0555267333984375, 0.19480514526367188, 3.4721832275390625, 3.2057418823242188, 3.8871803283691406, 4.157600402832031, 4.694847106933594, 4.7611236572265625, 2.06634521484375, 29.46148681640625, 4.056739807128906, 30.35516357421875, 3.6531829833984375, 7.724845886230469, 2.952068328857422, 3.7912139892578125, 39.33349609375, 5.184181213378906, -0.4090614318847656, 2.9952392578125, 5.319164276123047, 3.1126251220703125, 2.3746337890625, 3.1311264038085938, 3.0952682495117188, 19.539581298828125, 15.971664428710938, 21.293914794921875, 3.3835487365722656, 2.6639175415039062, 2.788158416748047, 2.8578262329101562, 29.25323486328125, 1.3092803955078125, 5.526348114013672, 3.6648216247558594, 2.4501113891601562, 3.2210922241210938, 10.881561279296875, -9.239776611328125, 54.19343566894531, 2.9202308654785156, 4.986701965332031, 5.099884033203125, 5.411445617675781, -8.56585693359375, 4.333789825439453, -33.42926025390625, -48.0599365234375, 2.6882896423339844, -0.018310546875, 4.480682373046875, 22.54117202758789, -5.737884521484375, 13.712020874023438, 0.7186508178710938, 3.1010894775390625, 2.8703231811523438, 6.621547698974609, 3.5218772888183594, 2.6026763916015625, 4.537853240966797, 4.375823974609375, 9.474990844726562, 1.9688262939453125, 4.398532867431641, 1.8613243103027344, 0.5419120788574219, 3.276317596435547, 5.265357971191406, 4.948993682861328, 2.0901527404785156, 5.435523986816406, 5.389488220214844, 4.831535339355469, 2.176342010498047, 3.576904296875, -35.962738037109375, 3.5597076416015625, 14.946563720703125, 1.8979034423828125, 5.856655120849609, 8.503551483154297, 5.251167297363281, 2.7321929931640625, -9.10345458984375, 8.664051055908203, 11.11578369140625, 3.1641845703125, -20.689483642578125, 2.8800315856933594, 2.5113906860351562, -0.5371322631835938, 3.0870590209960938, -1.4451904296875, 2.1075363159179688, 22.54117202758789, -35.526702880859375, 198.94943237304688, 2.5783843994140625, -4.9091796875, -0.00695037841796875, 59.34441375732422, 2.8874588012695312, -4.0720672607421875, 2.6329612731933594, 4.301761627197266, 2.1167831420898438, 4.6257476806640625, 5.109977722167969, 3.4489059448242188, 3.3624839782714844, 4.021152496337891, 5.21319580078125, -30.645843505859375, -8.631561279296875, 4.165416717529297, 4.106151580810547, 4.933021545410156, 39.05712127685547, 4.882080078125, 4.553565979003906, 4.229373931884766, 9.96014404296875, -3.1411590576171875, 2.8348388671875, 4.201610565185547, 3.44232177734375, 1.8664970397949219, 13.517196655273438, 4.735126495361328, 4.820392608642578, 4.818035125732422, 4.152996063232422, 1.1403961181640625, 3.6440200805664062, -1.01727294921875, 3.8326950073242188, 18.39434814453125, 2.94024658203125, 4.071163177490234, 65.5665512084961, -2.2147216796875, 3.510974884033203, 2.578472137451172, 6.140968322753906, 2.06634521484375, 3.8800125122070312, 3.8245849609375, 4.8173065185546875, 4.066837310791016, -6.105743408203125, -35.958251953125, 1.5779228210449219, -0.7596282958984375, 2.2955970764160156, 2.1265335083007812, 5.189125061035156, 2.2710647583007812, 2.908000946044922, 4.489292144775391, 5.282390594482422, 2.6857223510742188, 2.6162796020507812, -0.630401611328125, 2.6363372802734375, 3.2009620666503906, 5.211147308349609, -17.4698486328125, 6.22186279296875, 4.027767181396484, 5.5794830322265625, 3.908172607421875, 4.602714538574219, 5.859249114990234, 4.127540588378906, 3.5471153259277344, -0.6032752990722656, 3.9027023315429688, 1.9864158630371094, 4.5522308349609375, 4.202983856201172, 3.731739044189453, 3.4981346130371094, 4.312553405761719, 8.25628662109375, 2.719860076904297, 4.701892852783203, 18.565032958984375, 3.6597251892089844, 4.315891265869141, 4.3393402099609375, 2.4324111938476562, 4.292789459228516, 4.209709167480469, 5.445941925048828, 6.079174041748047, 2.5721702575683594, 5.082664489746094, 3.6825180053710938, 5.481056213378906, 3.346435546875, 11.689743041992188, 2.3278350830078125, 4.247409820556641, 1.4436416625976562, 3.654254913330078, 4.3458404541015625, 48.523284912109375, 3.0016441345214844, 3.387561798095703, 5.968418121337891, 2.4222946166992188, 54.614662170410156, 3.9053916931152344, 4.974945068359375, 3.7737388610839844, 4.5111846923828125, -0.20875930786132812, 5.237400054931641, -0.630401611328125, 4.949047088623047], "mean_td_error": 5.067858695983887}}, "num_steps_sampled": 45000, "num_agent_steps_sampled": 90000, "num_steps_trained": 1408256, "num_steps_trained_this_iter": 256, "num_agent_steps_trained": 2816512, "last_target_update_ts": 44848, "num_target_updates": 88}, "done": false, "episodes_total": 74, "training_iteration": 45, "trial_id": "e21e6_00000", "experiment_id": "434cd42d33fd4b638f17fc69fa01d74f", "date": "2022-06-14_19-17-45", "timestamp": 1655248665, "time_this_iter_s": 21.77757501602173, "time_total_s": 944.0819923877716, "pid": 2568314, "hostname": "lambda-dual", "node_ip": "10.104.51.19", "config": {"num_workers": 2, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.0005, "train_batch_size": 256, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "cnet", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "1v1env", "observation_space": null, "action_space": null, "env_config": {}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": true, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": null, "evaluation_duration": 10, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"num_workers": 2, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.0005, "train_batch_size": 256, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "cnet", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "1v1env", "observation_space": null, "action_space": null, "env_config": {}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": false, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": null, "evaluation_duration": 10, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"explore": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": true, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": null, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 2, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"policy_01": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]], [[[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]], (3, 64, 64), float32)", "Discrete(7)", {}], "policy_02": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]], [[[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]], (3, 64, 64), float32)", "Discrete(7)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": "<function <lambda> at 0x7f1aa3764b00>", "policies_to_train": ["policy_01"], "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": 100000, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": false, "hiddens": [], "double_q": false, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": true, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": null, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 2, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"policy_01": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]], [[[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]], (3, 64, 64), float32)", "Discrete(7)", {}], "policy_02": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]], [[[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]], (3, 64, 64), float32)", "Discrete(7)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": "<function <lambda> at 0x7f1aa3764b00>", "policies_to_train": ["policy_01"], "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": 100000, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": false, "hiddens": [], "double_q": false, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "time_since_restore": 944.0819923877716, "timesteps_since_restore": 11520, "iterations_since_restore": 45, "warmup_time": 45.46659183502197, "perf": {"cpu_util_percent": 24.754838709677422, "ram_util_percent": 12.706451612903223}}
{"episode_reward_max": 441.0, "episode_reward_min": 0.0, "episode_reward_mean": 25.894736842105264, "episode_len_mean": 601.0, "episode_media": {}, "episodes_this_iter": 2, "policy_reward_min": {"policy_01": 0.0, "policy_02": 0.0}, "policy_reward_max": {"policy_01": 441.0, "policy_02": 441.0}, "policy_reward_mean": {"policy_01": 14.289473684210526, "policy_02": 11.605263157894736}, "custom_metrics": {}, "hist_stats": {"episode_reward": [0.0, 204.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "episode_lengths": [601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601], "policy_policy_01_reward": [0.0, 204.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "policy_policy_02_reward": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.33545852741750365, "mean_inference_ms": 5.810379820554288, "mean_action_processing_ms": 0.08891600763545748, "mean_env_wait_ms": 7.99877608545726, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 2, "timesteps_total": 46000, "timesteps_this_iter": 256, "agent_timesteps_total": 92000, "timers": {"load_time_ms": 1.348, "load_throughput": 189968.831, "learn_time_ms": 12.325, "learn_throughput": 20770.388, "update_time_ms": 2.325}, "info": {"learner": {"policy_01": {"custom_metrics": {}, "learner_stats": {"mean_q": 107.66543579101562, "min_q": 48.62751770019531, "max_q": 817.8641357421875, "cur_lr": 0.0005}, "model": {}, "td_error": [31.5443115234375, -87.07644653320312, -65.9268798828125, -0.9120750427246094, -10.91754150390625, 0.036468505859375, -0.4922218322753906, -0.6575546264648438, 1.7225723266601562, -1.7807159423828125, -0.7726821899414062, -0.465301513671875, -0.6166496276855469, 2.009716033935547, -0.2502784729003906, 25.47186279296875, 0.9612846374511719, 29.757843017578125, -0.050262451171875, 0.03777313232421875, -0.22600555419921875, -0.9316062927246094, 0.6743431091308594, -0.18549728393554688, -2.1808853149414062, 2.9064064025878906, -1.5699920654296875, 0.5285186767578125, 0.34159088134765625, -2.0405006408691406, -0.1751556396484375, 53.884464263916016, 0.602752685546875, -0.5150108337402344, -0.36907958984375, 0.2537040710449219, 0.25286102294921875, -3.3876113891601562, -0.033294677734375, -0.4501953125, -1.3950538635253906, -0.281341552734375, 0.628875732421875, 195.3966522216797, -0.7455406188964844, -1.2389259338378906, -0.8477020263671875, -2.397296905517578, 0.3489799499511719, -0.028759002685546875, 1.4426536560058594, -1.0596961975097656, 36.545562744140625, -0.9919090270996094, 1.9268646240234375, -1.0466995239257812, -0.5216903686523438, -0.48357391357421875, 0.02208709716796875, -6.5958251953125, -1.6353950500488281, 0.11522674560546875, 32.28875732421875, -103.54425048828125, -0.8181686401367188, 0.6377830505371094, 0.12218856811523438, -0.7965164184570312, 17.52532958984375, -0.7474021911621094, -1.4900894165039062, -0.5630302429199219, -0.6083831787109375, -0.058910369873046875, -0.5467491149902344, 0.8592948913574219, 0.24034881591796875, 0.8845062255859375, -0.3983039855957031, -0.6885108947753906, -0.1279296875, -1.0395660400390625, -0.6996498107910156, -0.5578536987304688, -0.26035308837890625, 31.866256713867188, -2.629802703857422, -0.5128822326660156, -0.9777793884277344, 21.13385009765625, -10.12066650390625, -0.8576736450195312, 0.7452545166015625, -1.2691421508789062, 2.604736328125, -0.1056671142578125, 0.685760498046875, -1.307769775390625, -0.5659065246582031, -0.4224967956542969, 2.491382598876953, -0.7946014404296875, -1.5174484252929688, 0.05968475341796875, -0.4155311584472656, -0.449676513671875, -0.8275070190429688, -1.1399688720703125, 414.3841247558594, -1.4237022399902344, -0.21991729736328125, -1.4190750122070312, 0.34259033203125, 1.6331520080566406, 0.2973480224609375, -2.7786636352539062, 1.5413856506347656, -1.9598388671875, -0.11349105834960938, 0.4982414245605469, 35.73712158203125, 0.21396255493164062, -0.299468994140625, 24.1002197265625, -0.7295417785644531, 1.6141433715820312, 0.4809989929199219, -0.5121612548828125, -1.4424247741699219, -3.553821563720703, 0.14044952392578125, -0.7582588195800781, 1.155731201171875, -15.124923706054688, -11.84283447265625, 37.16368865966797, -0.6136817932128906, 0.060909271240234375, -2.5475730895996094, 14.653450012207031, -0.2668342590332031, -0.30834197998046875, -1.3884124755859375, -0.675811767578125, 0.06473541259765625, -0.6703529357910156, -0.15270614624023438, -2.5475730895996094, 0.6423683166503906, -0.4536895751953125, 0.016796112060546875, 0.42687225341796875, 0.23455047607421875, -2.693084716796875, -1.13043212890625, -2.3649024963378906, -0.6973114013671875, -0.8665657043457031, 0.96697998046875, -0.07465744018554688, -0.19325637817382812, 0.5777397155761719, 0.43094635009765625, -1.726348876953125, 0.0554656982421875, -0.3560028076171875, -1.5445556640625, -0.3154411315917969, 0.2915306091308594, -0.2931709289550781, -0.6277427673339844, 1.5874252319335938, -1.65704345703125, -1.4848060607910156, -1.4954681396484375, -0.237152099609375, 1.4973106384277344, 27.2784423828125, -2.224987030029297, -0.6866340637207031, -0.4793357849121094, -0.8885078430175781, -1.7349891662597656, -0.2778472900390625, -1.4498863220214844, -1.2866401672363281, -1.379150390625, -9.607879638671875, -59.881072998046875, 2.223297119140625, 2.2398948669433594, -0.3638725280761719, 0.177581787109375, -4.136451721191406, -1.1055450439453125, -16.74676513671875, 48.5714111328125, 0.16990280151367188, -3.307098388671875, -0.9419784545898438, 0.8888702392578125, -1.2869148254394531, 0.7227439880371094, -1.1715545654296875, 16.784652709960938, 1.8475303649902344, -1.38201904296875, -0.4959869384765625, 0.23128128051757812, -0.14075088500976562, 0.4378662109375, -0.5850982666015625, 0.3932304382324219, 0.9338417053222656, 17.246383666992188, -1.0789947509765625, -0.6421318054199219, -0.795684814453125, 0.8964004516601562, -1.3332099914550781, -0.10186004638671875, 0.010951995849609375, 2.223297119140625, 1.3498802185058594, 0.18349075317382812, -0.8147163391113281, -0.8590202331542969, -0.20338821411132812, -1.627655029296875, -1.726348876953125, -1.0754966735839844, 0.4055824279785156, -0.8631019592285156, -1.1346969604492188, -0.9156875610351562, 1.1018486022949219, -1.24053955078125, -0.4255218505859375, -0.7894439697265625, 0.3877525329589844, -0.9865226745605469, 0.2487945556640625, 6.014404296875, -1.2494888305664062, 0.23228073120117188, 1.1989555358886719, 0.4744071960449219, -1.0370330810546875, 2.0308570861816406, 27.0306396484375, 2.4377517700195312, 0.017322540283203125, 0.4200172424316406, -0.9623184204101562, -0.00972747802734375, -1.3312721252441406], "mean_td_error": 2.6044585704803467}}, "num_steps_sampled": 46000, "num_agent_steps_sampled": 92000, "num_steps_trained": 1440256, "num_steps_trained_this_iter": 256, "num_agent_steps_trained": 2880512, "last_target_update_ts": 45856, "num_target_updates": 90}, "done": false, "episodes_total": 76, "training_iteration": 46, "trial_id": "e21e6_00000", "experiment_id": "434cd42d33fd4b638f17fc69fa01d74f", "date": "2022-06-14_19-18-07", "timestamp": 1655248687, "time_this_iter_s": 22.267149448394775, "time_total_s": 966.3491418361664, "pid": 2568314, "hostname": "lambda-dual", "node_ip": "10.104.51.19", "config": {"num_workers": 2, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.0005, "train_batch_size": 256, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "cnet", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "1v1env", "observation_space": null, "action_space": null, "env_config": {}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": true, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": null, "evaluation_duration": 10, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"num_workers": 2, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.0005, "train_batch_size": 256, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "cnet", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "1v1env", "observation_space": null, "action_space": null, "env_config": {}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": false, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": null, "evaluation_duration": 10, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"explore": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": true, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": null, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 2, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"policy_01": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]], [[[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]], (3, 64, 64), float32)", "Discrete(7)", {}], "policy_02": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]], [[[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]], (3, 64, 64), float32)", "Discrete(7)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": "<function <lambda> at 0x7f1aa372ab90>", "policies_to_train": ["policy_01"], "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": 100000, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": false, "hiddens": [], "double_q": false, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": true, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": null, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 2, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"policy_01": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]], [[[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]], (3, 64, 64), float32)", "Discrete(7)", {}], "policy_02": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]], [[[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]], (3, 64, 64), float32)", "Discrete(7)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": "<function <lambda> at 0x7f1aa372ab90>", "policies_to_train": ["policy_01"], "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": 100000, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": false, "hiddens": [], "double_q": false, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "time_since_restore": 966.3491418361664, "timesteps_since_restore": 11776, "iterations_since_restore": 46, "warmup_time": 45.46659183502197, "perf": {"cpu_util_percent": 24.796774193548387, "ram_util_percent": 12.816129032258063}}
{"episode_reward_max": 441.0, "episode_reward_min": 0.0, "episode_reward_mean": 25.23076923076923, "episode_len_mean": 601.0, "episode_media": {}, "episodes_this_iter": 2, "policy_reward_min": {"policy_01": 0.0, "policy_02": 0.0}, "policy_reward_max": {"policy_01": 441.0, "policy_02": 441.0}, "policy_reward_mean": {"policy_01": 13.923076923076923, "policy_02": 11.307692307692308}, "custom_metrics": {}, "hist_stats": {"episode_reward": [0.0, 204.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "episode_lengths": [601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601], "policy_policy_01_reward": [0.0, 204.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "policy_policy_02_reward": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.3353976916371662, "mean_inference_ms": 5.809371907971399, "mean_action_processing_ms": 0.0889081573378186, "mean_env_wait_ms": 7.993163516305562, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 2, "timesteps_total": 47000, "timesteps_this_iter": 256, "agent_timesteps_total": 94000, "timers": {"load_time_ms": 1.316, "load_throughput": 194585.423, "learn_time_ms": 12.057, "learn_throughput": 21232.867, "update_time_ms": 2.378}, "info": {"learner": {"policy_01": {"custom_metrics": {}, "learner_stats": {"mean_q": 115.21705627441406, "min_q": 50.25384521484375, "max_q": 981.762451171875, "cur_lr": 0.0005}, "model": {}, "td_error": [1.8070297241210938, -0.25803375244140625, -0.9118881225585938, 0.3644561767578125, 0.18162155151367188, 1.2263221740722656, 1.2808761596679688, 1.2335586547851562, 1.6812286376953125, 0.17206954956054688, 0.9648208618164062, 1.0017356872558594, 1.1935806274414062, 0.6551628112792969, -42.363922119140625, 19.78375244140625, 7.5507354736328125, 1.4931373596191406, -26.05322265625, 0.3918609619140625, -1.4140663146972656, -0.33062744140625, 7.6870574951171875, -0.4918365478515625, 0.6205368041992188, 1.3489875793457031, -0.46372222900390625, 0.7163200378417969, 0.9326705932617188, 9.8046875, 0.8846664428710938, 0.22819900512695312, -1.0498008728027344, -1.1793708801269531, 0.05155181884765625, -1.2447052001953125, 0.5175323486328125, 0.030101776123046875, 56.69709014892578, 2.5602340698242188, -1.9473686218261719, 50.67448043823242, 1.2600631713867188, 0.2571525573730469, -1.668121337890625, 3.5920639038085938, 0.21265029907226562, 0.6060905456542969, -7.002288818359375, 1.7761573791503906, -0.9281272888183594, 3.002574920654297, -0.4157524108886719, -77.65312194824219, 0.349029541015625, 21.419677734375, -0.513153076171875, -3.635589599609375, -4.0455322265625, -1.2776298522949219, -23.691162109375, 0.5805854797363281, 0.14298248291015625, 0.8705825805664062, -19.34099578857422, -2.3331642150878906, -0.150848388671875, -1.4254875183105469, -0.8363800048828125, 1.934356689453125, 1.3007392883300781, 1.79132080078125, 1.686737060546875, -0.037921905517578125, 0.8692169189453125, 2.4855690002441406, 0.7890625, 0.47032928466796875, -0.3406219482421875, -0.9952621459960938, 0.11508941650390625, 0.6571502685546875, -0.5020217895507812, 0.5717811584472656, 1.2217750549316406, 1.9380340576171875, -0.5698738098144531, 0.18830490112304688, -1.2517204284667969, 1.2589302062988281, 0.4688606262207031, -1.9619293212890625, -3.4964523315429688, 0.3509559631347656, -28.1419677734375, -1.3249893188476562, 0.8813934326171875, 0.5622138977050781, 0.026275634765625, -0.1573944091796875, 0.9960212707519531, -0.3428459167480469, 3.5334701538085938, 1.948089599609375, -0.14810562133789062, 1.6827392578125, 0.14167022705078125, -0.8688697814941406, 0.6851463317871094, -0.23286819458007812, -0.469329833984375, 0.7406997680664062, -15.287353515625, -1.927398681640625, -0.1874542236328125, -0.6775245666503906, -7.638507843017578, -0.6268272399902344, -0.4024543762207031, 0.4121551513671875, -1.9699859619140625, -0.016551971435546875, -0.8640403747558594, -1.5006637573242188, -13.914360046386719, 1.2585067749023438, -0.5565528869628906, -28.0966796875, -0.7304306030273438, 0.30377960205078125, 1.1375350952148438, 1.5097808837890625, -0.17838668823242188, 0.9051094055175781, 55.93120574951172, -0.17220306396484375, -0.10186767578125, -0.7996253967285156, -0.045856475830078125, -0.326904296875, 1.522125244140625, 0.3327522277832031, 1.9602241516113281, 0.4707374572753906, 0.5606765747070312, -0.6520576477050781, 2.8753814697265625, -8.90719223022461, -1.1163978576660156, -1.1947555541992188, 17.664932250976562, -0.4457511901855469, 2.0819664001464844, 0.7262687683105469, 0.8896598815917969, -53.2811279296875, 2.402233123779297, 1.717681884765625, -28.2430419921875, 1.6227226257324219, -1.0920867919921875, 1.9685173034667969, 0.037322998046875, -0.8360061645507812, 0.527679443359375, -0.9004707336425781, -28.1419677734375, 0.6146469116210938, 1.713043212890625, 1.03460693359375, -26.109603881835938, -0.3614921569824219, -0.3632698059082031, -3.747455596923828, -0.24283599853515625, -0.5278129577636719, -0.3041038513183594, -6.441925048828125, 0.6587448120117188, 0.5454902648925781, 0.32221221923828125, -20.524658203125, 6.2386474609375, 55.33588790893555, -3.8604278564453125, -17.821044921875, 0.6916275024414062, -0.246124267578125, -0.5313186645507812, 0.21077728271484375, 1.2134056091308594, 1.8780937194824219, -0.7989082336425781, -1.2355690002441406, 0.9645576477050781, 0.7917251586914062, 54.288902282714844, -1.5409965515136719, 0.255096435546875, -0.24468612670898438, -9.10467529296875, -0.16645050048828125, 1.6371879577636719, 1.4350547790527344, -0.7759819030761719, 0.2268524169921875, 9.357513427734375, 0.553436279296875, 0.4630775451660156, 1.4731674194335938, -2.051136016845703, 52.88018798828125, -0.4704742431640625, -1.41192626953125, 0.13997268676757812, 0.3215599060058594, -2.4446868896484375, 1.4904098510742188, -0.11341094970703125, -0.9551582336425781, 0.6551628112792969, 0.9616775512695312, -0.9366035461425781, -49.3966064453125, -7.6942596435546875, 0.7003173828125, 1.299896240234375, -0.6781768798828125, 51.41111755371094, 8.29461669921875, -0.21242904663085938, 0.569244384765625, 5.275184631347656, -0.6408653259277344, 1.82232666015625, -0.9430885314941406, 0.22632980346679688, 1.1553573608398438, 1.1288986206054688, 2.0933380126953125, 2.4246368408203125, 0.6910247802734375, 0.19263076782226562, 0.359649658203125, 1.0675811767578125, 0.06911468505859375, -0.8922805786132812, -0.04242706298828125, -5.13250732421875, -0.9122047424316406, 1.4852638244628906, -8.18914794921875, 15.08001708984375, 1.2898101806640625, 1.4841079711914062, 1.5902214050292969], "mean_td_error": -0.03274744749069214}}, "num_steps_sampled": 47000, "num_agent_steps_sampled": 94000, "num_steps_trained": 1472256, "num_steps_trained_this_iter": 256, "num_agent_steps_trained": 2944512, "last_target_update_ts": 46864, "num_target_updates": 92}, "done": false, "episodes_total": 78, "training_iteration": 47, "trial_id": "e21e6_00000", "experiment_id": "434cd42d33fd4b638f17fc69fa01d74f", "date": "2022-06-14_19-18-29", "timestamp": 1655248709, "time_this_iter_s": 21.968520164489746, "time_total_s": 988.3176620006561, "pid": 2568314, "hostname": "lambda-dual", "node_ip": "10.104.51.19", "config": {"num_workers": 2, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.0005, "train_batch_size": 256, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "cnet", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "1v1env", "observation_space": null, "action_space": null, "env_config": {}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": true, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": null, "evaluation_duration": 10, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"num_workers": 2, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.0005, "train_batch_size": 256, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "cnet", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "1v1env", "observation_space": null, "action_space": null, "env_config": {}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": false, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": null, "evaluation_duration": 10, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"explore": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": true, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": null, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 2, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"policy_01": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]], [[[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]], (3, 64, 64), float32)", "Discrete(7)", {}], "policy_02": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]], [[[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]], (3, 64, 64), float32)", "Discrete(7)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": "<function <lambda> at 0x7f1aa372aef0>", "policies_to_train": ["policy_01"], "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": 100000, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": false, "hiddens": [], "double_q": false, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": true, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": null, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 2, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"policy_01": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]], [[[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]], (3, 64, 64), float32)", "Discrete(7)", {}], "policy_02": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]], [[[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]], (3, 64, 64), float32)", "Discrete(7)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": "<function <lambda> at 0x7f1aa372aef0>", "policies_to_train": ["policy_01"], "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": 100000, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": false, "hiddens": [], "double_q": false, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "time_since_restore": 988.3176620006561, "timesteps_since_restore": 12032, "iterations_since_restore": 47, "warmup_time": 45.46659183502197, "perf": {"cpu_util_percent": 24.746875, "ram_util_percent": 12.9}}
{"episode_reward_max": 441.0, "episode_reward_min": 0.0, "episode_reward_mean": 25.23076923076923, "episode_len_mean": 601.0, "episode_media": {}, "episodes_this_iter": 0, "policy_reward_min": {"policy_01": 0.0, "policy_02": 0.0}, "policy_reward_max": {"policy_01": 441.0, "policy_02": 441.0}, "policy_reward_mean": {"policy_01": 13.923076923076923, "policy_02": 11.307692307692308}, "custom_metrics": {}, "hist_stats": {"episode_reward": [0.0, 204.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "episode_lengths": [601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601], "policy_policy_01_reward": [0.0, 204.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "policy_policy_02_reward": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.3353976916371662, "mean_inference_ms": 5.809371907971399, "mean_action_processing_ms": 0.0889081573378186, "mean_env_wait_ms": 7.993163516305562, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 2, "timesteps_total": 48000, "timesteps_this_iter": 256, "agent_timesteps_total": 96000, "timers": {"load_time_ms": 1.324, "load_throughput": 193345.066, "learn_time_ms": 11.173, "learn_throughput": 22912.993, "update_time_ms": 2.3}, "info": {"learner": {"policy_01": {"custom_metrics": {}, "learner_stats": {"mean_q": 139.25381469726562, "min_q": 48.83698272705078, "max_q": 1096.5831298828125, "cur_lr": 0.0005}, "model": {}, "td_error": [-0.6933631896972656, 7.444511413574219, 0.03774261474609375, 18.18658447265625, -0.21526718139648438, 47.3616943359375, 0.8323822021484375, 0.8276443481445312, 2.456073760986328, 0.5344429016113281, 1.1373748779296875, 1.1836700439453125, 1.2372779846191406, 1.3017349243164062, 0.49127197265625, 0.5768356323242188, -1.2763290405273438, 1.0074081420898438, -0.177032470703125, -0.59637451171875, 0.8244438171386719, 0.13312530517578125, 0.7785186767578125, 1.1961669921875, 1.0676002502441406, -0.22954940795898438, -0.2972679138183594, -27.19989013671875, -2.0926971435546875, 0.15029525756835938, 0.9957351684570312, -1.6345748901367188, 0.7840728759765625, -0.7209663391113281, -0.5034027099609375, 0.24882888793945312, 0.6341476440429688, 2.8809432983398438, -21.208740234375, 2.9005661010742188, 18.904586791992188, 0.9955291748046875, 26.06365966796875, 0.0207061767578125, 0.7235107421875, -4.6499481201171875, -0.4336051940917969, -14.160980224609375, 0.9887657165527344, 57.586246490478516, 1.0427513122558594, 5.3352203369140625, 0.5189323425292969, 2.2035179138183594, 1.6693878173828125, 0.927947998046875, 0.26602935791015625, 1.05712890625, 7.311309814453125, 0.5813369750976562, 0.9517898559570312, 34.033966064453125, -0.5205535888671875, -1.0570068359375, 2.3970680236816406, 0.3856048583984375, -0.4631156921386719, 0.24277877807617188, 2.1911239624023438, 74.2469482421875, 0.23706817626953125, 1.1167869567871094, -31.129638671875, -2.1967926025390625, -1.1325759887695312, 1.5984230041503906, 2.4590225219726562, -0.37050628662109375, 4.184288024902344, -0.15561676025390625, 2.1236801147460938, 57.74028396606445, 3.3205718994140625, -10.664794921875, 0.056247711181640625, 0.079193115234375, -4.126331329345703, 7.613376617431641, 54.846405029296875, -7.89874267578125, 2.504741668701172, -0.19906997680664062, 2.461017608642578, 1.474365234375, 56.615509033203125, -4.970733642578125, -0.5662689208984375, 387.829345703125, 53.74998474121094, 0.4731864929199219, 0.8818550109863281, 7.311309814453125, 0.7815933227539062, -32.09991455078125, -2.1951904296875, 1.5953025817871094, 0.7506790161132812, 28.3314208984375, 1.6173973083496094, -1.0328712463378906, 20.94403076171875, 0.604583740234375, 0.9627227783203125, 3.9979705810546875, 1.0692710876464844, 2.229328155517578, 46.73712158203125, 0.9893951416015625, 2.126781463623047, 59.05649948120117, 1.4256095886230469, 1.0279388427734375, 0.6302528381347656, 0.7429046630859375, 1.900482177734375, 1.0262947082519531, -0.4090728759765625, -7.68511962890625, -1.8513031005859375, -0.304595947265625, 25.90234375, 1.0575370788574219, 0.6006431579589844, -0.2815589904785156, 2.7087860107421875, 0.55059814453125, 0.09301376342773438, -0.5365638732910156, 1.5484237670898438, -0.9516143798828125, -0.24420166015625, 2.3021697998046875, 52.066856384277344, 0.8729782104492188, 1.2260894775390625, 0.037441253662109375, 0.854766845703125, 2.3500328063964844, 0.4370689392089844, 1.0980491638183594, -1.7220191955566406, -0.027492523193359375, 51.24734115600586, 1.0216445922851562, 0.6585884094238281, 0.7454643249511719, 1.3266067504882812, 1.1705093383789062, 7.02447509765625, 18.730987548828125, -0.04500579833984375, 0.9069252014160156, 0.809478759765625, 27.23150634765625, 2.0505104064941406, 5.899478912353516, -0.3833198547363281, 0.6688079833984375, 2.4657211303710938, 4.05645751953125, 0.5592193603515625, 0.4448585510253906, 4.685279846191406, 0.6575241088867188, -0.14020538330078125, -1.8944740295410156, 2.1930999755859375, 2.301410675048828, -3.018798828125, -0.9164695739746094, 2.1784439086914062, 58.7496223449707, -0.07659149169921875, 6.371208190917969, 1.9683380126953125, -0.8574066162109375, 1.6203079223632812, 57.79890823364258, 0.7453193664550781, 1.87518310546875, 0.553802490234375, 1.3492927551269531, -0.8787612915039062, 1.3825187683105469, 0.5880012512207031, 0.01454925537109375, 57.83201599121094, -1.3787994384765625, 1.1093482971191406, 0.08957672119140625, -0.10150146484375, 0.6282501220703125, 2.8055496215820312, 6.6035919189453125, -0.6188583374023438, 0.4723625183105469, 0.7982711791992188, -0.4936943054199219, 16.329620361328125, -0.4795265197753906, 1.9953994750976562, 1.0393905639648438, 2.436962127685547, -2.3180770874023438, -0.202667236328125, -38.87506103515625, 2.031536102294922, 18.969497680664062, 16.849609375, 0.8616828918457031, 58.39605712890625, 1.3017997741699219, 0.97943115234375, -0.18028640747070312, 20.94403076171875, -0.48455810546875, 1.4815864562988281, -0.24068832397460938, 2.071765899658203, -0.3396949768066406, 37.75335693359375, -17.88800048828125, -221.16107177734375, -66.47467041015625, 0.95538330078125, 0.3304481506347656, 0.6974067687988281, 1.7874526977539062, 0.7875442504882812, -0.3835563659667969, 1.738128662109375, 1.8866043090820312, -16.459869384765625, -0.2633819580078125, 0.3871307373046875, 0.033023834228515625, 2.0491790771484375, 0.568695068359375, 0.6519126892089844, -3.1581268310546875, -2.015544891357422, 0.8187255859375, 0.34146881103515625, -0.046398162841796875, 2.4275588989257812, 0.5490074157714844], "mean_td_error": 4.7967634201049805}}, "num_steps_sampled": 48000, "num_agent_steps_sampled": 96000, "num_steps_trained": 1504256, "num_steps_trained_this_iter": 256, "num_agent_steps_trained": 3008512, "last_target_update_ts": 47872, "num_target_updates": 94}, "done": false, "episodes_total": 78, "training_iteration": 48, "trial_id": "e21e6_00000", "experiment_id": "434cd42d33fd4b638f17fc69fa01d74f", "date": "2022-06-14_19-18-51", "timestamp": 1655248731, "time_this_iter_s": 21.890622854232788, "time_total_s": 1010.2082848548889, "pid": 2568314, "hostname": "lambda-dual", "node_ip": "10.104.51.19", "config": {"num_workers": 2, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.0005, "train_batch_size": 256, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "cnet", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "1v1env", "observation_space": null, "action_space": null, "env_config": {}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": true, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": null, "evaluation_duration": 10, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"num_workers": 2, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.0005, "train_batch_size": 256, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "cnet", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "1v1env", "observation_space": null, "action_space": null, "env_config": {}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": false, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": null, "evaluation_duration": 10, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"explore": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": true, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": null, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 2, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"policy_01": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]], [[[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]], (3, 64, 64), float32)", "Discrete(7)", {}], "policy_02": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]], [[[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]], (3, 64, 64), float32)", "Discrete(7)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": "<function <lambda> at 0x7f1aa2ea4680>", "policies_to_train": ["policy_01"], "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": 100000, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": false, "hiddens": [], "double_q": false, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": true, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": null, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 2, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"policy_01": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]], [[[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]], (3, 64, 64), float32)", "Discrete(7)", {}], "policy_02": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]], [[[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]], (3, 64, 64), float32)", "Discrete(7)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": "<function <lambda> at 0x7f1aa2ea4680>", "policies_to_train": ["policy_01"], "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": 100000, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": false, "hiddens": [], "double_q": false, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "time_since_restore": 1010.2082848548889, "timesteps_since_restore": 12288, "iterations_since_restore": 48, "warmup_time": 45.46659183502197, "perf": {"cpu_util_percent": 24.716129032258063, "ram_util_percent": 12.996774193548386}}
{"episode_reward_max": 441.0, "episode_reward_min": 0.0, "episode_reward_mean": 24.6, "episode_len_mean": 601.0, "episode_media": {}, "episodes_this_iter": 2, "policy_reward_min": {"policy_01": 0.0, "policy_02": 0.0}, "policy_reward_max": {"policy_01": 441.0, "policy_02": 441.0}, "policy_reward_mean": {"policy_01": 13.575, "policy_02": 11.025}, "custom_metrics": {}, "hist_stats": {"episode_reward": [0.0, 204.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "episode_lengths": [601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601], "policy_policy_01_reward": [0.0, 204.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "policy_policy_02_reward": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.33530650406956586, "mean_inference_ms": 5.808438637649141, "mean_action_processing_ms": 0.08890194095196938, "mean_env_wait_ms": 7.9877503268486505, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 2, "timesteps_total": 49000, "timesteps_this_iter": 256, "agent_timesteps_total": 98000, "timers": {"load_time_ms": 1.334, "load_throughput": 191972.721, "learn_time_ms": 11.711, "learn_throughput": 21860.232, "update_time_ms": 2.255}, "info": {"learner": {"policy_01": {"custom_metrics": {}, "learner_stats": {"mean_q": 133.86434936523438, "min_q": 51.17301940917969, "max_q": 1194.069091796875, "cur_lr": 0.0005}, "model": {}, "td_error": [0.5851669311523438, 20.13818359375, -5.161712646484375, -8.416854858398438, 0.031887054443359375, -0.5921974182128906, 1.4499168395996094, 4.418430328369141, -0.5974388122558594, -32.7645263671875, -9.037220001220703, 1.2737808227539062, -0.52691650390625, -2.3004226684570312, 1.23406982421875, 0.9095306396484375, 1.3737754821777344, -0.09360504150390625, 1.3037605285644531, 16.77814483642578, -0.4029655456542969, -0.1719818115234375, -0.8282814025878906, 1.2603225708007812, -4.9870147705078125, -11.9561767578125, 0.45397186279296875, -5.70428466796875, -0.8900947570800781, 0.242828369140625, 0.3247566223144531, 6.0370025634765625, 1.5247955322265625, 1.1062965393066406, 6.01300048828125, -8.060428619384766, 0.8117179870605469, 6.071418762207031, 0.7733230590820312, 1.6421966552734375, -0.2541618347167969, 10.718040466308594, 2.9541587829589844, 8.503555297851562, -1.4780998229980469, 0.5181732177734375, 5.299869537353516, 1.9456443786621094, -0.20310211181640625, 0.3933372497558594, 49.6483154296875, 0.2604179382324219, 1.2318077087402344, 0.3382911682128906, 0.8898506164550781, -5.70428466796875, -0.16394424438476562, 2.9408798217773438, 3.161334991455078, -0.003391265869140625, 1.9703483581542969, -0.2949180603027344, 56.30464172363281, -1.1491889953613281, 0.9015846252441406, -2.4615249633789062, 0.03694915771484375, -0.2806968688964844, 0.7694740295410156, -1.558807373046875, 0.4564094543457031, -9.202728271484375, 1.231201171875, 0.6024093627929688, 58.10578918457031, 2.134185791015625, 0.1229248046875, 0.7130241394042969, 0.4279136657714844, -0.5478134155273438, 0.9532356262207031, 56.99609375, 3.0902442932128906, 2.2863731384277344, -0.21655654907226562, 1.9340667724609375, -1.4170913696289062, 1.5054130554199219, 3.999095916748047, -0.9284515380859375, 2.1272048950195312, 37.2347412109375, 1.88116455078125, 1.5871200561523438, 0.3160362243652344, 0.061817169189453125, -2.1067047119140625, -0.5840797424316406, 0.9183921813964844, 0.7124481201171875, -0.24439239501953125, 1.4822769165039062, 0.08526611328125, -0.11354827880859375, 0.6443862915039062, 42.4700927734375, -1.2983856201171875, -22.343963623046875, 0.16838455200195312, 0.9099845886230469, -0.4885711669921875, 58.263980865478516, 0.024387359619140625, 1.3015518188476562, 0.4507942199707031, 0.7955818176269531, -2.3713226318359375, 0.5449943542480469, -22.5887451171875, 1.2576332092285156, -9.143562316894531, 0.15708541870117188, 18.76849365234375, -0.7342796325683594, -1.7262954711914062, 0.3854637145996094, 2.6756248474121094, 6.1822052001953125, -1.6394233703613281, 0.6303367614746094, -8.85992431640625, 49.6483154296875, -0.13161468505859375, -0.6633987426757812, -0.9051437377929688, 0.015605926513671875, 86.43869018554688, 2.5212020874023438, 0.8117179870605469, 1.1472892761230469, 2.491191864013672, -4.222503662109375, 0.6471023559570312, -1.2226829528808594, 1.6442527770996094, 1.9552001953125, 0.9939651489257812, -101.22290802001953, 0.6067771911621094, -2.640705108642578, -0.587188720703125, 0.23215103149414062, -1.1490592956542969, 0.6283035278320312, -0.2686500549316406, 1.5820503234863281, -0.29534912109375, 1.9460372924804688, 2.3794708251953125, 1.71307373046875, -1.156219482421875, 1.1054649353027344, -0.09277725219726562, 1.2863006591796875, 1.4029769897460938, 0.287506103515625, 56.4837646484375, -1.029083251953125, 26.2447509765625, 3.75201416015625, -0.3805999755859375, -0.1862640380859375, 0.822479248046875, -0.8338432312011719, 0.9003562927246094, -0.4763946533203125, 1.4744834899902344, 277.9918518066406, -20.506454467773438, 0.7404556274414062, 26.39453125, 2.7534713745117188, 0.6795082092285156, -0.3489418029785156, 1.0837516784667969, 0.4446868896484375, -22.343963623046875, -0.5194740295410156, -1.0091552734375, 17.7830810546875, 0.3682289123535156, 58.52804946899414, 6.596153259277344, 0.34764862060546875, -0.27892303466796875, 0.06678009033203125, 51.450035095214844, -2.9603347778320312, -0.7096290588378906, 0.6572952270507812, 0.94970703125, 1.4148330688476562, 3.8747825622558594, 1.876007080078125, 1.9463577270507812, 1.1641616821289062, 0.29759979248046875, -2.2611770629882812, -0.027069091796875, -0.05213165283203125, 0.5301361083984375, 0.44854736328125, -0.6803703308105469, 1.5594215393066406, 17.74407958984375, 0.3215370178222656, -0.6286582946777344, 0.653350830078125, 0.5217437744140625, 1.3316535949707031, 1.1869926452636719, 1.7428817749023438, 0.24445343017578125, -21.3441162109375, 0.6038322448730469, 0.2550315856933594, 3.0211105346679688, -22.343963623046875, 1.0748634338378906, 0.086578369140625, 1.5993232727050781, 0.5921440124511719, 0.402130126953125, 3.1510009765625, 20.601791381835938, -0.1496124267578125, -1.24566650390625, -0.7352561950683594, 0.03285980224609375, -0.1356201171875, -3.566204071044922, -0.2087554931640625, 0.07137680053710938, 1.6158905029296875, 0.4402275085449219, 5.543060302734375, 0.3358116149902344, 1.7041168212890625, 0.09470748901367188, 19.33148193359375, -9.574951171875, 0.5415229797363281, 0.36292266845703125, 2.1260414123535156, -0.2710609436035156, 0.49668121337890625], "mean_td_error": 3.5863890647888184}}, "num_steps_sampled": 49000, "num_agent_steps_sampled": 98000, "num_steps_trained": 1536256, "num_steps_trained_this_iter": 256, "num_agent_steps_trained": 3072512, "last_target_update_ts": 48880, "num_target_updates": 96}, "done": false, "episodes_total": 80, "training_iteration": 49, "trial_id": "e21e6_00000", "experiment_id": "434cd42d33fd4b638f17fc69fa01d74f", "date": "2022-06-14_19-19-13", "timestamp": 1655248753, "time_this_iter_s": 21.81571364402771, "time_total_s": 1032.0239984989166, "pid": 2568314, "hostname": "lambda-dual", "node_ip": "10.104.51.19", "config": {"num_workers": 2, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.0005, "train_batch_size": 256, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "cnet", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "1v1env", "observation_space": null, "action_space": null, "env_config": {}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": true, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": null, "evaluation_duration": 10, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"num_workers": 2, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.0005, "train_batch_size": 256, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "cnet", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "1v1env", "observation_space": null, "action_space": null, "env_config": {}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": false, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": null, "evaluation_duration": 10, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"explore": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": true, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": null, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 2, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"policy_01": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]], [[[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]], (3, 64, 64), float32)", "Discrete(7)", {}], "policy_02": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]], [[[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]], (3, 64, 64), float32)", "Discrete(7)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": "<function <lambda> at 0x7f1aa2ebe320>", "policies_to_train": ["policy_01"], "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": 100000, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": false, "hiddens": [], "double_q": false, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": true, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": null, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 2, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"policy_01": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]], [[[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]], (3, 64, 64), float32)", "Discrete(7)", {}], "policy_02": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]], [[[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]], (3, 64, 64), float32)", "Discrete(7)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": "<function <lambda> at 0x7f1aa2ebe320>", "policies_to_train": ["policy_01"], "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": 100000, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": false, "hiddens": [], "double_q": false, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "time_since_restore": 1032.0239984989166, "timesteps_since_restore": 12544, "iterations_since_restore": 49, "warmup_time": 45.46659183502197, "perf": {"cpu_util_percent": 25.032258064516128, "ram_util_percent": 13.08387096774194}}
{"episode_reward_max": 441.0, "episode_reward_min": 0.0, "episode_reward_mean": 24.0, "episode_len_mean": 601.0, "episode_media": {}, "episodes_this_iter": 2, "policy_reward_min": {"policy_01": 0.0, "policy_02": 0.0}, "policy_reward_max": {"policy_01": 441.0, "policy_02": 441.0}, "policy_reward_mean": {"policy_01": 13.24390243902439, "policy_02": 10.75609756097561}, "custom_metrics": {}, "hist_stats": {"episode_reward": [0.0, 204.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "episode_lengths": [601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601], "policy_policy_01_reward": [0.0, 204.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "policy_policy_02_reward": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.3352292367086131, "mean_inference_ms": 5.80763933768894, "mean_action_processing_ms": 0.08889671210778127, "mean_env_wait_ms": 7.982526919458055, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 2, "timesteps_total": 50000, "timesteps_this_iter": 256, "agent_timesteps_total": 100000, "timers": {"load_time_ms": 1.328, "load_throughput": 192744.637, "learn_time_ms": 12.21, "learn_throughput": 20965.787, "update_time_ms": 2.442}, "info": {"learner": {"policy_01": {"custom_metrics": {}, "learner_stats": {"mean_q": 148.95599365234375, "min_q": 54.80675506591797, "max_q": 1225.955810546875, "cur_lr": 0.0005}, "model": {}, "td_error": [0.4417762756347656, 0.5621147155761719, 8.17822265625, 1.9730148315429688, 0.7864494323730469, 2.062652587890625, 7.299324035644531, 0.5343284606933594, 1.1422271728515625, 0.24831771850585938, 0.742218017578125, -62.597900390625, 0.7030410766601562, -0.6316032409667969, -0.18010711669921875, -0.2968635559082031, 5.256053924560547, 0.7367134094238281, 1.7985877990722656, 1.76171875, -2.95867919921875, -1.0690345764160156, 0.9673233032226562, 1.5400772094726562, -78.94921875, 1.0731163024902344, 0.4259796142578125, 2.174407958984375, 0.9689064025878906, 1.0501518249511719, -0.3574371337890625, -4.2275390625, 0.3634185791015625, -2.0467567443847656, -8.860931396484375, 1.7386894226074219, 0.1788330078125, -0.21847915649414062, 0.6980209350585938, 2.1621971130371094, 1.0204391479492188, -36.1268310546875, 0.37357330322265625, 0.24745559692382812, 1.0932121276855469, 0.9933128356933594, 0.5304603576660156, 0.3305854797363281, -0.6408576965332031, 2.018085479736328, 1.5555763244628906, -2.6059112548828125, -20.74510955810547, 0.6690521240234375, -1.2395668029785156, -149.239501953125, -1.3348617553710938, 1.347564697265625, 0.16922760009765625, -1.9673500061035156, -24.6234130859375, 0.8113517761230469, 0.012950897216796875, 0.7040863037109375, 2.452728271484375, 0.5272102355957031, 0.19914627075195312, 1.7951507568359375, -1.2894668579101562, -0.6297988891601562, 16.043731689453125, 0.6514396667480469, -0.22458267211914062, 37.8729248046875, 1.1384315490722656, -19.968994140625, -1.4900245666503906, 1.7345924377441406, 1.7094955444335938, 2.0282516479492188, 4.29443359375, 0.9108047485351562, 1.1656837463378906, 0.8133621215820312, 0.4315299987792969, -3.2706031799316406, 9.5560302734375, 0.38480377197265625, 0.5065765380859375, 22.860015869140625, -0.10012054443359375, 0.72650146484375, 0.30329132080078125, 2.1149444580078125, -0.21194076538085938, -4.926761627197266, 1.4719734191894531, 0.5985794067382812, 0.345001220703125, 2.1263771057128906, 0.5769996643066406, 0.8932647705078125, 3.01165771484375, 21.984375, 0.790069580078125, -29.38934326171875, 6.957317352294922, 583.4514770507812, -2.869800567626953, 0.7421722412109375, 1.806671142578125, 0.11597442626953125, 0.7211418151855469, 1.5733184814453125, -0.3122406005859375, -4.013584136962891, 0.8543243408203125, 0.13610458374023438, 2.570018768310547, 0.1488494873046875, -0.6831207275390625, 0.24615478515625, 0.8264045715332031, 0.6103324890136719, 0.9241600036621094, 0.01128387451171875, 0.9758796691894531, 0.6379432678222656, 2.999267578125, -24.616958618164062, -1.5215110778808594, 0.8354568481445312, 0.056835174560546875, 0.379547119140625, 2.6027755737304688, -20.05419921875, -8.4539794921875, -7.83233642578125, 0.7862091064453125, 2.1621971130371094, 1.4257926940917969, 0.3445091247558594, 2.1306304931640625, 0.8201560974121094, -1.7244110107421875, -0.41352081298828125, 0.5153999328613281, -0.5557174682617188, -0.349334716796875, 3.4523849487304688, 0.709716796875, 0.3235054016113281, -35.9898681640625, 0.11242294311523438, -81.7197265625, 0.6412315368652344, -0.03527069091796875, 2.1353912353515625, 0.3672752380371094, -234.58819580078125, -0.24594879150390625, -15.203399658203125, -11.1148681640625, 0.6008644104003906, 0.166656494140625, 1.3553199768066406, 0.5106735229492188, -0.912109375, -4.396385192871094, -5.340911865234375, 1.0456924438476562, 0.7956275939941406, 0.36327362060546875, 0.711273193359375, 1.6610336303710938, -0.8542671203613281, 0.442535400390625, 29.00006103515625, 1.0108528137207031, 0.7844696044921875, -25.958343505859375, -2.541229248046875, 0.05568695068359375, -43.4520263671875, 2.850719451904297, -0.5032958984375, 0.222503662109375, 0.8624114990234375, -1.93701171875, 0.005947113037109375, 0.7589912414550781, -0.6093482971191406, 59.68506622314453, -2.466644287109375, 2.241771697998047, 0.9351387023925781, -0.23520660400390625, 1.2648277282714844, -1.6682968139648438, -21.97918701171875, -0.105133056640625, -3.1929473876953125, -0.5048637390136719, -233.124267578125, 27.453536987304688, 1.3289031982421875, 1.7852249145507812, 0.355377197265625, 1.0669174194335938, 0.6538238525390625, 1.7036247253417969, 1.0700607299804688, -0.15285110473632812, 138.4798583984375, 0.8802528381347656, 0.7509574890136719, 0.18354034423828125, 0.6791763305664062, -0.11483001708984375, 0.985992431640625, -1.2126846313476562, -1.0956573486328125, -0.6576271057128906, 0.8025894165039062, 0.020481109619140625, 11.990310668945312, -1.453399658203125, 1.6817398071289062, 0.901824951171875, 0.5223312377929688, 0.4353446960449219, 0.6762466430664062, -0.3595008850097656, 0.611785888671875, 0.5642738342285156, -2.348949432373047, -2.4878082275390625, 1.011474609375, 0.936126708984375, 0.12861251831054688, 1.9240608215332031, -17.81158447265625, 61.7381477355957, -0.6937484741210938, 1.5163383483886719, 0.78277587890625, 0.8769912719726562, 0.9348640441894531, 6.98040771484375, 0.808258056640625, 1.2324638366699219, 0.2279510498046875, 1.8255767822265625, -0.18615341186523438, -0.7959365844726562, -0.6488304138183594], "mean_td_error": -0.3246273994445801}}, "num_steps_sampled": 50000, "num_agent_steps_sampled": 100000, "num_steps_trained": 1568256, "num_steps_trained_this_iter": 256, "num_agent_steps_trained": 3136512, "last_target_update_ts": 49888, "num_target_updates": 98}, "done": false, "episodes_total": 82, "training_iteration": 50, "trial_id": "e21e6_00000", "experiment_id": "434cd42d33fd4b638f17fc69fa01d74f", "date": "2022-06-14_19-19-35", "timestamp": 1655248775, "time_this_iter_s": 21.99315619468689, "time_total_s": 1054.0171546936035, "pid": 2568314, "hostname": "lambda-dual", "node_ip": "10.104.51.19", "config": {"num_workers": 2, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.0005, "train_batch_size": 256, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "cnet", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "1v1env", "observation_space": null, "action_space": null, "env_config": {}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": true, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": null, "evaluation_duration": 10, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"num_workers": 2, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.0005, "train_batch_size": 256, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "cnet", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "1v1env", "observation_space": null, "action_space": null, "env_config": {}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": false, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": null, "evaluation_duration": 10, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"explore": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": true, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": null, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 2, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"policy_01": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]], [[[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]], (3, 64, 64), float32)", "Discrete(7)", {}], "policy_02": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]], [[[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]], (3, 64, 64), float32)", "Discrete(7)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": "<function <lambda> at 0x7f1aa372af80>", "policies_to_train": ["policy_01"], "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": 100000, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": false, "hiddens": [], "double_q": false, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": true, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": null, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 2, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"policy_01": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]], [[[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]], (3, 64, 64), float32)", "Discrete(7)", {}], "policy_02": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]], [[[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]], (3, 64, 64), float32)", "Discrete(7)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": "<function <lambda> at 0x7f1aa372af80>", "policies_to_train": ["policy_01"], "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": 100000, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": false, "hiddens": [], "double_q": false, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "time_since_restore": 1054.0171546936035, "timesteps_since_restore": 12800, "iterations_since_restore": 50, "warmup_time": 45.46659183502197, "perf": {"cpu_util_percent": 24.7875, "ram_util_percent": 13.159374999999999}}
{"episode_reward_max": 441.0, "episode_reward_min": 0.0, "episode_reward_mean": 23.428571428571427, "episode_len_mean": 601.0, "episode_media": {}, "episodes_this_iter": 2, "policy_reward_min": {"policy_01": 0.0, "policy_02": 0.0}, "policy_reward_max": {"policy_01": 441.0, "policy_02": 441.0}, "policy_reward_mean": {"policy_01": 12.928571428571429, "policy_02": 10.5}, "custom_metrics": {}, "hist_stats": {"episode_reward": [0.0, 204.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "episode_lengths": [601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601], "policy_policy_01_reward": [0.0, 204.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "policy_policy_02_reward": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.3351621817356413, "mean_inference_ms": 5.806811802318431, "mean_action_processing_ms": 0.08889085924868888, "mean_env_wait_ms": 7.9774692168726755, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 2, "timesteps_total": 51000, "timesteps_this_iter": 256, "agent_timesteps_total": 102000, "timers": {"load_time_ms": 1.321, "load_throughput": 193763.751, "learn_time_ms": 11.751, "learn_throughput": 21784.746, "update_time_ms": 2.281}, "info": {"learner": {"policy_01": {"custom_metrics": {}, "learner_stats": {"mean_q": 143.52516174316406, "min_q": 55.96558380126953, "max_q": 1146.769287109375, "cur_lr": 0.0005}, "model": {}, "td_error": [-0.004852294921875, -14.0682373046875, 1.6710205078125, -0.7836837768554688, -0.010471343994140625, 1.6102294921875, -0.005096435546875, 0.6577606201171875, 0.1189727783203125, 0.535491943359375, 1.059478759765625, 68.8001708984375, -2.5913734436035156, -3.9562530517578125, -0.8656196594238281, 1.5316162109375, 12.0491943359375, -2.2664756774902344, 1.5094528198242188, -0.10399246215820312, 0.733367919921875, 0.9762115478515625, 6.11669921875, 1.8950767517089844, -233.6851806640625, -0.7059974670410156, 1.0149803161621094, 2.361175537109375, -1.685028076171875, 15.34100341796875, 4.220794677734375, -233.6851806640625, 0.5038642883300781, 64.96031951904297, 0.07038116455078125, 65.63681030273438, 60.1065788269043, -0.9352798461914062, 2.9968795776367188, -0.2036895751953125, 1.7350387573242188, 3.9234619140625, -2.113189697265625, 0.8000679016113281, 1.8067398071289062, 1.2629165649414062, 2.338207244873047, 8.2894287109375, 1.6623077392578125, 1.5505218505859375, 2.707897186279297, 0.9887008666992188, -4.5848541259765625, -85.852783203125, 11.941925048828125, 1.648773193359375, 2.3788604736328125, 0.5943679809570312, 0.9206733703613281, 0.95989990234375, 1.0576362609863281, -14.59716796875, 1.5811958312988281, 1.3107986450195312, 1.3281631469726562, 61.48711395263672, 1.5700149536132812, 1.6769638061523438, 1.1812324523925781, -0.3661041259765625, -1.00897216796875, 1.1726722717285156, 0.10343551635742188, -0.5634689331054688, 0.11772918701171875, -13.869171142578125, 1.0737571716308594, -0.604522705078125, -0.05356597900390625, 17.523635864257812, 0.21806716918945312, -4.4105224609375, 1.7990951538085938, 0.6485137939453125, -3.9159469604492188, 1.5254783630371094, 1.0513877868652344, -0.27307891845703125, 1.5308914184570312, 0.0537109375, 4.4341888427734375, 1.5202255249023438, -0.5587997436523438, -0.061588287353515625, 0.7824554443359375, 1.5716171264648438, -2.6645660400390625, 1.8890762329101562, 12.026138305664062, 10.5858154296875, 0.17723846435546875, -37.642601013183594, 10.8984375, -25.9541015625, 0.59405517578125, 1.3223991394042969, 0.7920494079589844, 0.238067626953125, 1.5084953308105469, -6.7924041748046875, 57.345314025878906, 0.83270263671875, 0.5556526184082031, 3.4376869201660156, 46.3466796875, -0.67431640625, 0.4612312316894531, -1.5388870239257812, 1.6221847534179688, -0.15354156494140625, 1.2273979187011719, 0.6188697814941406, 19.955459594726562, 1.5378379821777344, -11.007537841796875, 0.5526008605957031, 2.3558311462402344, -0.11569595336914062, -14.926589965820312, 11.02008056640625, -0.2036895751953125, 2.17681884765625, 0.5259780883789062, 0.6993904113769531, 0.7772674560546875, -0.290557861328125, -14.531692504882812, -1.3242607116699219, -0.049468994140625, -0.1850128173828125, 58.89324951171875, -3.316864013671875, 0.8149147033691406, 0.7271537780761719, 1.2433395385742188, 0.4920005798339844, -39.65149688720703, 1.034881591796875, -0.02828216552734375, 0.16180801391601562, 0.8249740600585938, -0.5171432495117188, 28.716552734375, 1.738983154296875, 1.21221923828125, 22.22625732421875, 0.13056182861328125, 9.19140625, 0.6648521423339844, 0.0292510986328125, 0.1602325439453125, -2.3203506469726562, 1.1476898193359375, 0.8954849243164062, -0.5665512084960938, 60.21950149536133, 5.569366455078125, 0.8428421020507812, 0.027362823486328125, -0.5285568237304688, -0.293060302734375, 1.0700950622558594, -1.3211669921875, 1.4321556091308594, -23.97998046875, 0.95849609375, 0.9088859558105469, 4.3028564453125, -0.20452880859375, -0.5680427551269531, 58.009456634521484, 0.5576705932617188, -1.8225364685058594, -0.1596221923828125, 4.4584808349609375, 1.3014869689941406, 1.36187744140625, 0.8100624084472656, 0.126190185546875, -2.4788360595703125, 2.2720947265625, 0.4116325378417969, 3.2692489624023438, -0.05072784423828125, 0.6437911987304688, 11.5987548828125, 0.6166572570800781, -6.263214111328125, -0.7008514404296875, -0.48246002197265625, 1.0669403076171875, 0.6344528198242188, -1.5279121398925781, -1.1505279541015625, 1.1434288024902344, 0.4702644348144531, 0.9726028442382812, -1.3150711059570312, -1.0639610290527344, 0.537109375, 0.2649421691894531, -41.346923828125, -0.17060470581054688, 17.523635864257812, -0.7476730346679688, 1.8173828125, -23.66112518310547, 0.16292190551757812, 0.7544097900390625, 0.8618927001953125, -0.6757621765136719, 116.46688842773438, -69.0859375, 83.2801513671875, 4.309814453125, -0.7747421264648438, -1.2192802429199219, 2.425975799560547, 3.8814697265625, 0.8756523132324219, 1.2187881469726562, 1.1211967468261719, 4.3071746826171875, 7.840476989746094, 1.0727386474609375, 1.1365814208984375, 0.7736167907714844, 0.8560028076171875, 0.8856735229492188, -2.653270721435547, 0.3354644775390625, 0.3610572814941406, -0.12154006958007812, -1.8341941833496094, 0.585540771484375, 0.7014579772949219, 0.4487495422363281, -28.841766357421875, 0.6315345764160156, 1.3264732360839844, 3.4930572509765625, 0.9726028442382812, 1.01287841796875, 1.393829345703125, 3.7931976318359375, -0.9123573303222656], "mean_td_error": 0.8374176025390625}}, "num_steps_sampled": 51000, "num_agent_steps_sampled": 102000, "num_steps_trained": 1600256, "num_steps_trained_this_iter": 256, "num_agent_steps_trained": 3200512, "last_target_update_ts": 50896, "num_target_updates": 100}, "done": false, "episodes_total": 84, "training_iteration": 51, "trial_id": "e21e6_00000", "experiment_id": "434cd42d33fd4b638f17fc69fa01d74f", "date": "2022-06-14_19-19-57", "timestamp": 1655248797, "time_this_iter_s": 21.743858098983765, "time_total_s": 1075.7610127925873, "pid": 2568314, "hostname": "lambda-dual", "node_ip": "10.104.51.19", "config": {"num_workers": 2, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.0005, "train_batch_size": 256, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "cnet", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "1v1env", "observation_space": null, "action_space": null, "env_config": {}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": true, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": null, "evaluation_duration": 10, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"num_workers": 2, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.0005, "train_batch_size": 256, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "cnet", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "1v1env", "observation_space": null, "action_space": null, "env_config": {}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": false, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": null, "evaluation_duration": 10, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"explore": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": true, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": null, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 2, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"policy_01": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]], [[[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]], (3, 64, 64), float32)", "Discrete(7)", {}], "policy_02": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]], [[[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]], (3, 64, 64), float32)", "Discrete(7)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": "<function <lambda> at 0x7f1aa2ea4710>", "policies_to_train": ["policy_01"], "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": 100000, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": false, "hiddens": [], "double_q": false, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": true, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": null, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 2, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"policy_01": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]], [[[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]], (3, 64, 64), float32)", "Discrete(7)", {}], "policy_02": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]], [[[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]], (3, 64, 64), float32)", "Discrete(7)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": "<function <lambda> at 0x7f1aa2ea4710>", "policies_to_train": ["policy_01"], "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": 100000, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": false, "hiddens": [], "double_q": false, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "time_since_restore": 1075.7610127925873, "timesteps_since_restore": 13056, "iterations_since_restore": 51, "warmup_time": 45.46659183502197, "perf": {"cpu_util_percent": 24.71290322580645, "ram_util_percent": 13.245161290322583}}
{"episode_reward_max": 441.0, "episode_reward_min": 0.0, "episode_reward_mean": 22.88372093023256, "episode_len_mean": 601.0, "episode_media": {}, "episodes_this_iter": 2, "policy_reward_min": {"policy_01": 0.0, "policy_02": 0.0}, "policy_reward_max": {"policy_01": 441.0, "policy_02": 441.0}, "policy_reward_mean": {"policy_01": 12.627906976744185, "policy_02": 10.255813953488373}, "custom_metrics": {}, "hist_stats": {"episode_reward": [0.0, 204.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "episode_lengths": [601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601], "policy_policy_01_reward": [0.0, 204.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "policy_policy_02_reward": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.3351030252090421, "mean_inference_ms": 5.80598247839506, "mean_action_processing_ms": 0.08888497412210676, "mean_env_wait_ms": 7.9725715038928255, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 2, "timesteps_total": 52000, "timesteps_this_iter": 256, "agent_timesteps_total": 104000, "timers": {"load_time_ms": 1.319, "load_throughput": 194047.389, "learn_time_ms": 12.007, "learn_throughput": 21321.704, "update_time_ms": 2.234}, "info": {"learner": {"policy_01": {"custom_metrics": {}, "learner_stats": {"mean_q": 129.20263671875, "min_q": 57.21040344238281, "max_q": 1194.9212646484375, "cur_lr": 0.0005}, "model": {}, "td_error": [0.925811767578125, 0.9113922119140625, 17.21539306640625, 2.1334075927734375, 0.310455322265625, -5.6475830078125, -0.78900146484375, 0.3928375244140625, -1.1253509521484375, -0.458953857421875, 0.216705322265625, -0.43474578857421875, 0.5249404907226562, -0.4715995788574219, 1.0073928833007812, 1.60980224609375, -230.72952270507812, 0.2583351135253906, 0.22257614135742188, -0.7912368774414062, 0.8148841857910156, 0.566009521484375, 0.4358863830566406, 0.83489990234375, 0.664306640625, -1.101318359375, 0.1311798095703125, 0.657379150390625, -88.60638427734375, 0.44358062744140625, 1.9725723266601562, -0.7127838134765625, -0.39173126220703125, -0.5504531860351562, -0.6269912719726562, 0.3873863220214844, -54.36810302734375, 0.2588310241699219, -0.846282958984375, -9.6478271484375, -0.4659576416015625, -1.4775314331054688, -0.08138275146484375, 1.1445732116699219, -0.10359573364257812, 2.8304061889648438, 0.09142303466796875, 1.5987548828125, 0.21485519409179688, 0.15531158447265625, 0.9361953735351562, -0.18110275268554688, -1.1722564697265625, 1.1705589294433594, -1.1841773986816406, -0.2359161376953125, -0.07695388793945312, 0.2495269775390625, -0.3910980224609375, -229.54917907714844, 1.7377967834472656, -1.98138427734375, 25.44873046875, -0.6451263427734375, 1.8589744567871094, 0.03094482421875, -41.406707763671875, 17.3160400390625, 0.23906326293945312, -1.9783706665039062, -44.496826171875, -0.5581893920898438, 0.016021728515625, 0.4031219482421875, 1.1244621276855469, 0.4390907287597656, -0.2684326171875, 1.5266609191894531, -0.6005706787109375, -1.5363883972167969, -1.0018806457519531, -0.9672775268554688, -0.19499969482421875, -0.674072265625, -0.7599105834960938, 0.4277076721191406, 0.38855743408203125, 0.15287399291992188, -66.62646484375, -0.0302734375, 2.830425262451172, -3.6252059936523438, -5.5003814697265625, 0.01220703125, -10.663681030273438, 1.1501388549804688, -0.137603759765625, 1.208770751953125, 0.3508262634277344, -0.3838348388671875, 0.62335205078125, -30.008819580078125, -0.08711624145507812, -0.8235702514648438, -4.4665069580078125, -0.6798477172851562, 1.50860595703125, -0.52227783203125, 0.1429901123046875, 0.4385223388671875, 0.6193084716796875, -0.14544296264648438, -4.520664215087891, -0.3625335693359375, -0.637542724609375, -0.4794769287109375, -16.03729248046875, 2.3282814025878906, -0.5428848266601562, 0.10534286499023438, -1.20941162109375, 0.9344367980957031, 1.7666549682617188, 0.13756561279296875, -26.306106567382812, 0.2780036926269531, 0.7237586975097656, 0.14026641845703125, 1.1476898193359375, 0.4573936462402344, -0.6298103332519531, -0.13967132568359375, 0.5250396728515625, -5.895599365234375, -2.448211669921875, 1.1179847717285156, 0.19190216064453125, 0.0891876220703125, 31.030975341796875, -0.9852828979492188, 64.99541473388672, 0.4950141906738281, 0.5065498352050781, -0.057018280029296875, 3.9244842529296875, -1.5365676879882812, 1.1331329345703125, -0.647796630859375, 0.5835189819335938, 0.425079345703125, -59.59667205810547, -6.5067901611328125, 1.3847465515136719, -1.1502838134765625, -0.09328842163085938, 0.7118377685546875, 0.36203765869140625, -0.09973907470703125, 1.6695938110351562, -0.2859611511230469, -0.016353607177734375, 0.16024017333984375, -2.90753173828125, -0.8267784118652344, -0.08332061767578125, -0.56817626953125, -24.955215454101562, 1.1441535949707031, 31.623779296875, -1.9434928894042969, -0.05462646484375, 63.537593841552734, -1.006805419921875, -1.1427536010742188, 22.05541229248047, 0.8982162475585938, 51.60895538330078, -2.9674453735351562, 0.0473785400390625, -0.9988899230957031, -1.0570030212402344, 0.3537406921386719, -0.5045166015625, 2.018360137939453, 60.78692626953125, 0.453155517578125, -5.59063720703125, -6.609100341796875, 0.8670158386230469, 26.24560546875, -0.6030654907226562, 0.5410385131835938, -0.7197036743164062, -0.46335601806640625, 0.065673828125, -4.803314208984375, -0.36177825927734375, -2.2918701171875, 0.3306427001953125, 0.22435760498046875, -0.30619049072265625, -12.503173828125, -2.4095382690429688, 1.0066871643066406, 61.983787536621094, 0.01181793212890625, 62.597137451171875, 22.578956604003906, -0.77911376953125, 993.3742065429688, 1.0112075805664062, -1.1891555786132812, 2.4259262084960938, 0.9282989501953125, 1.7394828796386719, -2.241962432861328, 0.8259391784667969, -1.08636474609375, 0.8316230773925781, 0.42221832275390625, 0.27988433837890625, -2.5880889892578125, -0.3973846435546875, 1.3991470336914062, -0.2806282043457031, -10.866165161132812, 5.7843017578125, 0.21863937377929688, 0.2716712951660156, 27.492645263671875, -0.6259498596191406, -1.1495742797851562, 0.6575050354003906, 0.7835464477539062, 0.3051795959472656, -0.9581375122070312, 0.1505126953125, -3.889537811279297, -1.660369873046875, -230.72952270507812, -0.30628204345703125, 57.02832794189453, -0.5467071533203125, -6.81756591796875, -18.47216796875, 0.14331817626953125, -0.224517822265625, -10.836990356445312, 0.12215423583984375, 0.24633026123046875, 0.8229408264160156, -0.38555908203125, -0.58599853515625, -6.071308135986328, 0.4980125427246094, 60.78692626953125], "mean_td_error": 1.666393756866455}}, "num_steps_sampled": 52000, "num_agent_steps_sampled": 104000, "num_steps_trained": 1632256, "num_steps_trained_this_iter": 256, "num_agent_steps_trained": 3264512, "last_target_update_ts": 51904, "num_target_updates": 102}, "done": false, "episodes_total": 86, "training_iteration": 52, "trial_id": "e21e6_00000", "experiment_id": "434cd42d33fd4b638f17fc69fa01d74f", "date": "2022-06-14_19-20-19", "timestamp": 1655248819, "time_this_iter_s": 21.908825874328613, "time_total_s": 1097.669838666916, "pid": 2568314, "hostname": "lambda-dual", "node_ip": "10.104.51.19", "config": {"num_workers": 2, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.0005, "train_batch_size": 256, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "cnet", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "1v1env", "observation_space": null, "action_space": null, "env_config": {}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": true, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": null, "evaluation_duration": 10, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"num_workers": 2, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.0005, "train_batch_size": 256, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "cnet", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "1v1env", "observation_space": null, "action_space": null, "env_config": {}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": false, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": null, "evaluation_duration": 10, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"explore": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": true, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": null, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 2, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"policy_01": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]], [[[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]], (3, 64, 64), float32)", "Discrete(7)", {}], "policy_02": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]], [[[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]], (3, 64, 64), float32)", "Discrete(7)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": "<function <lambda> at 0x7f1aa2ea4b00>", "policies_to_train": ["policy_01"], "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": 100000, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": false, "hiddens": [], "double_q": false, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": true, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": null, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 2, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"policy_01": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]], [[[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]], (3, 64, 64), float32)", "Discrete(7)", {}], "policy_02": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]], [[[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]], (3, 64, 64), float32)", "Discrete(7)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": "<function <lambda> at 0x7f1aa2ea4b00>", "policies_to_train": ["policy_01"], "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": 100000, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": false, "hiddens": [], "double_q": false, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "time_since_restore": 1097.669838666916, "timesteps_since_restore": 13312, "iterations_since_restore": 52, "warmup_time": 45.46659183502197, "perf": {"cpu_util_percent": 24.754838709677426, "ram_util_percent": 13.319354838709677}}
{"episode_reward_max": 441.0, "episode_reward_min": 0.0, "episode_reward_mean": 22.363636363636363, "episode_len_mean": 601.0, "episode_media": {}, "episodes_this_iter": 2, "policy_reward_min": {"policy_01": 0.0, "policy_02": 0.0}, "policy_reward_max": {"policy_01": 441.0, "policy_02": 441.0}, "policy_reward_mean": {"policy_01": 12.340909090909092, "policy_02": 10.022727272727273}, "custom_metrics": {}, "hist_stats": {"episode_reward": [0.0, 204.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "episode_lengths": [601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601], "policy_policy_01_reward": [0.0, 204.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "policy_policy_02_reward": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.33504986428073796, "mean_inference_ms": 5.805186023506502, "mean_action_processing_ms": 0.08887939393177893, "mean_env_wait_ms": 7.967838516210701, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 2, "timesteps_total": 53000, "timesteps_this_iter": 256, "agent_timesteps_total": 106000, "timers": {"load_time_ms": 1.35, "load_throughput": 189676.875, "learn_time_ms": 11.534, "learn_throughput": 22195.342, "update_time_ms": 2.185}, "info": {"learner": {"policy_01": {"custom_metrics": {}, "learner_stats": {"mean_q": 135.73736572265625, "min_q": 61.591278076171875, "max_q": 1254.19287109375, "cur_lr": 0.0005}, "model": {}, "td_error": [0.7139091491699219, 1.9096908569335938, 11.008712768554688, -0.01798248291015625, 1.3464279174804688, 10.369331359863281, 2.2584457397460938, 2.462238311767578, 1.1720428466796875, 0.6857376098632812, -0.15372467041015625, -7.1902313232421875, 2.5313720703125, 1.871124267578125, 1.2967567443847656, 0.04741668701171875, 2.033588409423828, 37.806396484375, 4.163776397705078, 64.59454345703125, 2.3283004760742188, 2.9252700805664062, 0.9001083374023438, 6.192832946777344, 0.4278106689453125, 0.8454170227050781, -5.1312103271484375, 2.2985191345214844, 0.744140625, 2.064208984375, 1.1735801696777344, 1.7598915100097656, -2.2211837768554688, 1.4417495727539062, 1.9106216430664062, -7.657623291015625, 0.8699874877929688, 2.6416969299316406, 0.1192474365234375, 3.8656845092773438, 1.2098655700683594, 1.1512222290039062, 2.596649169921875, -6.387992858886719, 0.01465606689453125, 1.2843399047851562, -4.8740081787109375, 2.0570602416992188, 1.4686431884765625, 0.68133544921875, 1.0484619140625, 0.851348876953125, 0.6287765502929688, 1.7700958251953125, 0.9886856079101562, 1.125396728515625, -8.763519287109375, 1.8356246948242188, 0.1058807373046875, 2.8550643920898438, 1.362396240234375, 0.9000053405761719, -3.3671875, 1.1004295349121094, 1.0120697021484375, -1.9556961059570312, 0.9958572387695312, 1.2796554565429688, -4.6502227783203125, 1.5551528930664062, 1.581634521484375, 0.8162460327148438, 0.04003143310546875, 0.241455078125, 2.5053482055664062, 2.0201148986816406, 2.387298583984375, 2.0210418701171875, 2.5909881591796875, 1.8442764282226562, 0.05185699462890625, 1.0965042114257812, -0.956573486328125, 1.269134521484375, 1.0482330322265625, 0.1607666015625, -0.12964630126953125, 4.28192138671875, 1.2542076110839844, 0.3892974853515625, 0.8792228698730469, 1.0330848693847656, 1.6561775207519531, 1.353302001953125, -0.28321075439453125, 0.7637443542480469, 1.1457138061523438, 1.4132881164550781, 1.2374038696289062, 62.82191467285156, 1.8858909606933594, -5.2909393310546875, 0.7338104248046875, 2.5809669494628906, 0.7920608520507812, 2.785430908203125, 2.039764404296875, 1.9863777160644531, -0.621734619140625, 1.2862358093261719, 1.133331298828125, 63.689964294433594, 2.585681915283203, 1.2818222045898438, -0.23299407958984375, 0.922149658203125, 32.4083251953125, -89.4830322265625, 1.1782646179199219, 64.43081665039062, -14.559326171875, 2.248199462890625, 1.010101318359375, 2.3236923217773438, 0.8366661071777344, 1.1964645385742188, 1.3522071838378906, 1.0330543518066406, 0.2893829345703125, 2.8365097045898438, 0.5317916870117188, 0.9946174621582031, 1.4620628356933594, 0.23406219482421875, 3.9726486206054688, 1.9984779357910156, -10.2723388671875, 2.13641357421875, 1.3884811401367188, 1.6798820495605469, 0.7479438781738281, -45.9637451171875, 0.00698089599609375, 34.7093505859375, 0.9672164916992188, 2.2153701782226562, 1.8795318603515625, 0.9611053466796875, -0.2467041015625, 5.1151885986328125, 0.150482177734375, 1.3799972534179688, 38.7984619140625, 4.352073669433594, 2.3124618530273438, -53.571258544921875, -0.47100830078125, 1.6832122802734375, 2.2006874084472656, 1.8886833190917969, 2.3609542846679688, 0.38849639892578125, 0.160675048828125, 1.00958251953125, 0.8864212036132812, 0.7075042724609375, -0.1357421875, 1071.440185546875, 0.9018630981445312, 0.4216461181640625, 1.0778388977050781, 1.4904289245605469, 1.6157341003417969, 0.7500877380371094, 5.082023620605469, 2.5019149780273438, 0.41521453857421875, 0.34871673583984375, 2.574188232421875, 1.9670486450195312, 0.13503265380859375, 0.49884033203125, 3.8995132446289062, 0.1866302490234375, -1.578369140625, 1.489166259765625, 1.6190872192382812, -13.604034423828125, -14.3062744140625, 1.3641357421875, 0.8690681457519531, 1.0197105407714844, 2.1381683349609375, 0.03145599365234375, 0.36565399169921875, 0.82696533203125, 0.5236968994140625, 0.9453582763671875, 0.43978118896484375, 1.3383255004882812, 1.0431404113769531, 0.9176025390625, 0.5925674438476562, 2.543498992919922, 1.2621612548828125, 1.8323173522949219, 1.056671142578125, 1.4597854614257812, 1.9927902221679688, 0.0947113037109375, 2.41619873046875, 0.8905868530273438, 2.463287353515625, 0.8994216918945312, 0.9946975708007812, 0.9681396484375, 0.4134101867675781, 15.653831481933594, 19.0076904296875, 0.4502716064453125, 1.6858444213867188, 0.04388427734375, 1.77203369140625, 1.2907562255859375, 0.8406753540039062, 1.91094970703125, -10.7684326171875, 2.102001190185547, 0.46845245361328125, 2.2102737426757812, -40.17669677734375, 2.6071548461914062, 2.4779319763183594, 0.8815078735351562, 1.517822265625, 1.485504150390625, 1.3786849975585938, 1.8811531066894531, 0.3184661865234375, 1.9068756103515625, 3.795166015625, 2.9653396606445312, 0.15778350830078125, -0.2943572998046875, 2.825489044189453, -101.60951232910156, 64.88884735107422, 0.76971435546875, 0.8353614807128906, -14.50494384765625, 62.95029830932617, 2.591838836669922, 0.8001785278320312, 1.6982803344726562, 0.368682861328125, 13.060203552246094], "mean_td_error": 5.855380058288574}}, "num_steps_sampled": 53000, "num_agent_steps_sampled": 106000, "num_steps_trained": 1664256, "num_steps_trained_this_iter": 256, "num_agent_steps_trained": 3328512, "last_target_update_ts": 52912, "num_target_updates": 104}, "done": false, "episodes_total": 88, "training_iteration": 53, "trial_id": "e21e6_00000", "experiment_id": "434cd42d33fd4b638f17fc69fa01d74f", "date": "2022-06-14_19-20-41", "timestamp": 1655248841, "time_this_iter_s": 21.815910577774048, "time_total_s": 1119.48574924469, "pid": 2568314, "hostname": "lambda-dual", "node_ip": "10.104.51.19", "config": {"num_workers": 2, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.0005, "train_batch_size": 256, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "cnet", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "1v1env", "observation_space": null, "action_space": null, "env_config": {}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": true, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": null, "evaluation_duration": 10, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"num_workers": 2, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.0005, "train_batch_size": 256, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "cnet", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "1v1env", "observation_space": null, "action_space": null, "env_config": {}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": false, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": null, "evaluation_duration": 10, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"explore": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": true, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": null, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 2, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"policy_01": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]], [[[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]], (3, 64, 64), float32)", "Discrete(7)", {}], "policy_02": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]], [[[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]], (3, 64, 64), float32)", "Discrete(7)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": "<function <lambda> at 0x7f1aa3700d40>", "policies_to_train": ["policy_01"], "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": 100000, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": false, "hiddens": [], "double_q": false, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": true, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": null, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 2, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"policy_01": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]], [[[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]], (3, 64, 64), float32)", "Discrete(7)", {}], "policy_02": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]], [[[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]], (3, 64, 64), float32)", "Discrete(7)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": "<function <lambda> at 0x7f1aa3700d40>", "policies_to_train": ["policy_01"], "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": 100000, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": false, "hiddens": [], "double_q": false, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "time_since_restore": 1119.48574924469, "timesteps_since_restore": 13568, "iterations_since_restore": 53, "warmup_time": 45.46659183502197, "perf": {"cpu_util_percent": 24.79354838709677, "ram_util_percent": 13.403225806451609}}
{"episode_reward_max": 441.0, "episode_reward_min": 0.0, "episode_reward_mean": 22.363636363636363, "episode_len_mean": 601.0, "episode_media": {}, "episodes_this_iter": 0, "policy_reward_min": {"policy_01": 0.0, "policy_02": 0.0}, "policy_reward_max": {"policy_01": 441.0, "policy_02": 441.0}, "policy_reward_mean": {"policy_01": 12.340909090909092, "policy_02": 10.022727272727273}, "custom_metrics": {}, "hist_stats": {"episode_reward": [0.0, 204.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "episode_lengths": [601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601], "policy_policy_01_reward": [0.0, 204.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "policy_policy_02_reward": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.33504986428073796, "mean_inference_ms": 5.805186023506502, "mean_action_processing_ms": 0.08887939393177893, "mean_env_wait_ms": 7.967838516210701, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 2, "timesteps_total": 54000, "timesteps_this_iter": 256, "agent_timesteps_total": 108000, "timers": {"load_time_ms": 1.295, "load_throughput": 197622.407, "learn_time_ms": 11.69, "learn_throughput": 21899.958, "update_time_ms": 2.215}, "info": {"learner": {"policy_01": {"custom_metrics": {}, "learner_stats": {"mean_q": 171.1070556640625, "min_q": 61.37741470336914, "max_q": 1297.396484375, "cur_lr": 0.0005}, "model": {}, "td_error": [2.632781982421875, -232.428955078125, -0.2062225341796875, -22.7333984375, 1.7973785400390625, 0.4600372314453125, 1.0315170288085938, -1.0695762634277344, -0.00750732421875, 71.1513442993164, 1.5526657104492188, -0.39630126953125, -7.461128234863281, 0.7663955688476562, -0.4720611572265625, 1.2601470947265625, -1.40179443359375, -0.34648895263671875, 61.99140167236328, -0.20554351806640625, 0.05322265625, -1.4546127319335938, 4.112823486328125, -0.39664459228515625, -11.821258544921875, -6.975341796875, -0.6245384216308594, -2.7512969970703125, 1.3390731811523438, 0.08257293701171875, 0.06450653076171875, -4.883369445800781, 0.8510551452636719, -0.1015472412109375, -0.5560455322265625, -0.6934356689453125, -0.5447883605957031, -0.744110107421875, 0.052825927734375, 1.34210205078125, 331.63037109375, -3.1303787231445312, 2.3030166625976562, 0.5150909423828125, -9.228248596191406, 1.2858200073242188, -4.1776123046875, 1.098602294921875, -0.4650611877441406, 0.931610107421875, 0.1093597412109375, 0.3574409484863281, -1.077667236328125, 0.6341934204101562, 0.34807586669921875, 4.589504241943359, 1.9656295776367188, -0.42049407958984375, -0.5609359741210938, 1.5393753051757812, -0.16323471069335938, 0.441070556640625, -1.2976913452148438, -0.2675971984863281, -1.441375732421875, 0.6349945068359375, 1.5606689453125, 0.753692626953125, 16.0123291015625, -0.4891815185546875, 1.4899215698242188, -20.18701171875, 0.4917411804199219, 0.24234771728515625, 0.6688156127929688, -0.6765060424804688, 0.39593505859375, -0.2759513854980469, 0.16654205322265625, 1.3093032836914062, -2.2300033569335938, -0.4757499694824219, -149.39642333984375, 6.585479736328125, 0.223663330078125, -0.3326148986816406, 1.0096969604492188, -0.04276275634765625, 0.3099365234375, -0.44431304931640625, 36.6005859375, 3.798248291015625, 0.3861427307128906, 3.1367568969726562, 2.8212432861328125, 4.0816192626953125, -1.29351806640625, 0.513458251953125, 0.2601318359375, -2.0796737670898438, 4.034423828125, -0.1275482177734375, 0.9579010009765625, 1.100738525390625, 14.642120361328125, -24.19091796875, -122.110595703125, 47.114013671875, 0.0971221923828125, -0.3773193359375, 1.3135910034179688, 0.85198974609375, 0.615478515625, -2.7327804565429688, 10.921859741210938, -0.4875946044921875, 0.987548828125, 0.05724334716796875, -1.2178115844726562, 0.5981521606445312, -0.0633544921875, 0.49695587158203125, 67.48397827148438, 2.4918365478515625, -3.1232528686523438, 0.42076873779296875, 5.1492767333984375, 0.08345794677734375, 0.8551597595214844, 0.06052398681640625, -0.8896331787109375, 3.6048355102539062, 1071.068359375, 0.18712234497070312, 0.7337150573730469, -9.3455810546875, -29.572174072265625, 0.2463836669921875, -0.20069503784179688, 1.542938232421875, -0.6657218933105469, -1.2157058715820312, 10.34457778930664, 2.0983734130859375, 0.12614059448242188, 68.04943084716797, 1.4507064819335938, -0.5985374450683594, 2.1759567260742188, -0.6805953979492188, -0.024505615234375, -0.02692413330078125, 0.6961784362792969, -0.6888580322265625, 0.8202133178710938, -0.8632087707519531, 1.0344924926757812, -0.20696640014648438, -3.590545654296875, -2.8733291625976562, -37.0469970703125, 0.90216064453125, 1.4397125244140625, -0.1760406494140625, 1.8210601806640625, -1.4493598937988281, -1.1884613037109375, -0.44498443603515625, 1.2522201538085938, -1.7186355590820312, -0.9614677429199219, -1.2377281188964844, -0.3223876953125, -42.0570068359375, 1.9364089965820312, 0.3099365234375, -0.4692573547363281, 0.4117279052734375, 1.5291290283203125, -0.6617774963378906, 0.4501190185546875, 1.0690193176269531, -0.5156021118164062, -0.9149246215820312, -1.534820556640625, 0.05083465576171875, 3.8695106506347656, 1.3203125, -0.1058197021484375, 0.30060577392578125, -20.4478759765625, 0.5559158325195312, 0.667572021484375, -0.5999946594238281, -0.609649658203125, 1.0181465148925781, -0.0504913330078125, 0.445953369140625, 1.9013137817382812, 0.126556396484375, 3.27984619140625, 50.56103515625, 0.09386825561523438, -0.860748291015625, 28.022979736328125, -4.547229766845703, 0.8355560302734375, 0.17339324951171875, 11.30340576171875, 0.5115585327148438, 12.06988525390625, 0.2798271179199219, 0.5642890930175781, -0.9267807006835938, 1.02215576171875, -0.084564208984375, -37.462738037109375, 0.1734771728515625, 0.09207916259765625, 53.26416015625, -0.07587814331054688, -1.3511581420898438, 25.804100036621094, -0.19306564331054688, 1.111297607421875, 0.073211669921875, 1.409698486328125, -1.4588851928710938, -4.437355041503906, 67.99614715576172, 1.5953521728515625, -7.665500640869141, -31.1357421875, 0.504119873046875, -0.24623489379882812, -0.3927268981933594, 0.18053436279296875, 1.73138427734375, 10.34457778930664, -37.6126708984375, -0.008853912353515625, 0.5729293823242188, -0.785186767578125, -0.02276611328125, 64.88650512695312, 2.8831024169921875, 1.6372566223144531, 0.7417755126953125, 2.1147994995117188, -1.0182037353515625, -2.1696319580078125, 0.492034912109375, 2.6850967407226562, -0.13857650756835938, 1.2384414672851562, 0.41428375244140625], "mean_td_error": 5.190970420837402}}, "num_steps_sampled": 54000, "num_agent_steps_sampled": 108000, "num_steps_trained": 1696256, "num_steps_trained_this_iter": 256, "num_agent_steps_trained": 3392512, "last_target_update_ts": 53920, "num_target_updates": 106}, "done": false, "episodes_total": 88, "training_iteration": 54, "trial_id": "e21e6_00000", "experiment_id": "434cd42d33fd4b638f17fc69fa01d74f", "date": "2022-06-14_19-21-02", "timestamp": 1655248862, "time_this_iter_s": 21.787074089050293, "time_total_s": 1141.2728233337402, "pid": 2568314, "hostname": "lambda-dual", "node_ip": "10.104.51.19", "config": {"num_workers": 2, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.0005, "train_batch_size": 256, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "cnet", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "1v1env", "observation_space": null, "action_space": null, "env_config": {}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": true, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": null, "evaluation_duration": 10, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"num_workers": 2, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.0005, "train_batch_size": 256, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "cnet", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "1v1env", "observation_space": null, "action_space": null, "env_config": {}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": false, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": null, "evaluation_duration": 10, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"explore": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": true, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": null, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 2, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"policy_01": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]], [[[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]], (3, 64, 64), float32)", "Discrete(7)", {}], "policy_02": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]], [[[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]], (3, 64, 64), float32)", "Discrete(7)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": "<function <lambda> at 0x7f1aa3700710>", "policies_to_train": ["policy_01"], "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": 100000, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": false, "hiddens": [], "double_q": false, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": true, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": null, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 2, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"policy_01": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]], [[[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]], (3, 64, 64), float32)", "Discrete(7)", {}], "policy_02": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]], [[[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]], (3, 64, 64), float32)", "Discrete(7)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": "<function <lambda> at 0x7f1aa3700710>", "policies_to_train": ["policy_01"], "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": 100000, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": false, "hiddens": [], "double_q": false, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "time_since_restore": 1141.2728233337402, "timesteps_since_restore": 13824, "iterations_since_restore": 54, "warmup_time": 45.46659183502197, "perf": {"cpu_util_percent": 24.87741935483871, "ram_util_percent": 13.496774193548386}}
{"episode_reward_max": 441.0, "episode_reward_min": 0.0, "episode_reward_mean": 21.866666666666667, "episode_len_mean": 601.0, "episode_media": {}, "episodes_this_iter": 2, "policy_reward_min": {"policy_01": 0.0, "policy_02": 0.0}, "policy_reward_max": {"policy_01": 441.0, "policy_02": 441.0}, "policy_reward_mean": {"policy_01": 12.066666666666666, "policy_02": 9.8}, "custom_metrics": {}, "hist_stats": {"episode_reward": [0.0, 204.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "episode_lengths": [601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601], "policy_policy_01_reward": [0.0, 204.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "policy_policy_02_reward": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.33497467000432785, "mean_inference_ms": 5.804427020833615, "mean_action_processing_ms": 0.08887471161547185, "mean_env_wait_ms": 7.963177797263403, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 2, "timesteps_total": 55000, "timesteps_this_iter": 256, "agent_timesteps_total": 110000, "timers": {"load_time_ms": 1.303, "load_throughput": 196533.628, "learn_time_ms": 12.201, "learn_throughput": 20981.888, "update_time_ms": 2.306}, "info": {"learner": {"policy_01": {"custom_metrics": {}, "learner_stats": {"mean_q": 166.66744995117188, "min_q": 59.92670822143555, "max_q": 1233.863525390625, "cur_lr": 0.0005}, "model": {}, "td_error": [-5.9962158203125, -0.349395751953125, -1.2665023803710938, -2.651081085205078, -1.5860366821289062, -2.6760940551757812, -25.058319091796875, 2.5206680297851562, -3.0306854248046875, -1.0464401245117188, -0.7413864135742188, -2.0738525390625, -1.1704483032226562, 1.990203857421875, 0.07403945922851562, -7.3877410888671875, -64.16845703125, -1.4209060668945312, -2.0146713256835938, -15.164665222167969, 0.9258193969726562, -0.9360580444335938, -1.3602218627929688, -1.08465576171875, 16.614898681640625, 2.7661972045898438, 0.491729736328125, -6.05535888671875, 1.717864990234375, -14.885589599609375, -136.296875, -1.4866104125976562, 1.983245849609375, -0.7595329284667969, 1.5939102172851562, -0.17841339111328125, -0.6082992553710938, -33.80126953125, -1.237274169921875, -1.0119705200195312, -22.619903564453125, -1.1679153442382812, -1.8007965087890625, -32.09056091308594, -1.7347335815429688, -1.6190643310546875, -22.019256591796875, -2.1695289611816406, -0.5001754760742188, 2.5259246826171875, -0.2556610107421875, 68.9012451171875, 2.1211395263671875, -20.444610595703125, 8.82708740234375, 1.3084182739257812, -2.2429351806640625, 1.135162353515625, -1.7171134948730469, -288.7463684082031, -93.84716796875, -0.821319580078125, -1.2265396118164062, 2.9663619995117188, -1.0451278686523438, -1.294586181640625, -1.5253524780273438, 0.86541748046875, -1.6532363891601562, 0.58111572265625, -1.7798004150390625, 1.6347732543945312, 1.3528594970703125, -3.0246009826660156, -0.619476318359375, -7.040435791015625, -9.907798767089844, -1.5865020751953125, 0.995758056640625, 1.5651473999023438, -2.1521987915039062, -1.5549697875976562, -0.98162841796875, -6.8995819091796875, 0.3978309631347656, -1.5866851806640625, 0.19132614135742188, 0.0501556396484375, 0.7245712280273438, 1.7889938354492188, 1.3601531982421875, -1.3910446166992188, -0.6336135864257812, -41.291015625, 1.3527069091796875, -264.16741943359375, -0.61749267578125, -2.341796875, -0.8749771118164062, 5.783729553222656, -0.7842864990234375, -3.9456024169921875, -2.4937744140625, -0.7161102294921875, -1.3655967712402344, 1.2095718383789062, -0.7237930297851562, -3.231395721435547, 1.2421302795410156, 0.978973388671875, 1.3584747314453125, -0.5183181762695312, 59.76971435546875, -0.4867134094238281, 65.01862335205078, -14.885589599609375, 2.2312088012695312, -0.31739044189453125, -1.2947311401367188, -17.32855224609375, -1.42840576171875, -2.1321983337402344, -1.7017059326171875, -1.8380203247070312, 0.17923355102539062, 67.10777282714844, -0.4435920715332031, -0.6359100341796875, -3.0775184631347656, -4.8797607421875, 236.11044311523438, -0.7035140991210938, -1.8518295288085938, -87.7100830078125, 68.84209442138672, -37.8609619140625, -0.9522323608398438, 1.6698455810546875, 0.53570556640625, -42.56312561035156, -1.3150787353515625, -2.0023117065429688, -7.081329345703125, -0.9731597900390625, 0.5553665161132812, 4.434326171875, -3.0583839416503906, -0.5554656982421875, -6.4603729248046875, -15.164665222167969, -1.8854598999023438, -1.6969223022460938, 1.1120071411132812, -5.16387939453125, -3.0224647521972656, 0.9572372436523438, 6.021942138671875, -1.6178436279296875, 3.5161666870117188, -0.45366668701171875, 2.5403594970703125, -1.1814651489257812, -5.458549499511719, -0.504730224609375, -1.0914154052734375, -0.5875244140625, -1.1735000610351562, -1.29559326171875, 1.2254104614257812, -1.7202682495117188, -1.5113296508789062, -0.9115066528320312, 1.8579330444335938, -14.3671875, 0.6229476928710938, 0.00807952880859375, 1.0818099975585938, 1.7151031494140625, -66.3209228515625, -16.3072509765625, -1.12860107421875, -1.3087005615234375, -1.0166473388671875, 1.0044784545898438, -0.2476043701171875, 1.6698455810546875, 1.5538711547851562, -2.0141639709472656, -0.04077911376953125, 3.5924530029296875, -1.6326026916503906, 1.5307235717773438, 0.6417808532714844, 1.585723876953125, -1.4758148193359375, -1.4322891235351562, -1.2452850341796875, 1.3741989135742188, 1.3387603759765625, -0.5892333984375, 1.9549331665039062, 1.4823074340820312, -0.6346817016601562, -16.593963623046875, -2.5982284545898438, 1.4544754028320312, 3.8578414916992188, -0.09757614135742188, -0.5933303833007812, -1.2611579895019531, -47.26220703125, -1.1583404541015625, -15.9080810546875, 31.6329345703125, -1.6900100708007812, -1.1568450927734375, -2.2088623046875, -1.9383621215820312, 22.07471466064453, -2.438629150390625, -1.0489044189453125, 8.349369049072266, 1186.8765869140625, -2.2825851440429688, -2.16595458984375, -7.605682373046875, -0.162506103515625, 0.9431228637695312, -0.9909896850585938, -0.4983406066894531, -1.1276702880859375, -1.4093246459960938, -1.6361083984375, 1.7571792602539062, -3.171234130859375, -1.1337661743164062, -0.49391937255859375, 1.55029296875, 62.36613845825195, -1.6114349365234375, 26.303390502929688, 1.5661239624023438, -3.3115005493164062, -210.78369140625, -1.198516845703125, -13.271942138671875, -6.085395812988281, -14.411376953125, -2.20166015625, -2.67303466796875, -0.9921798706054688, -0.937652587890625, -2.4997634887695312, -13.339630126953125, 22.237770080566406, -2.5853195190429688], "mean_td_error": 0.2545185089111328}}, "num_steps_sampled": 55000, "num_agent_steps_sampled": 110000, "num_steps_trained": 1728256, "num_steps_trained_this_iter": 256, "num_agent_steps_trained": 3456512, "last_target_update_ts": 54928, "num_target_updates": 108}, "done": false, "episodes_total": 90, "training_iteration": 55, "trial_id": "e21e6_00000", "experiment_id": "434cd42d33fd4b638f17fc69fa01d74f", "date": "2022-06-14_19-21-25", "timestamp": 1655248885, "time_this_iter_s": 22.112353086471558, "time_total_s": 1163.3851764202118, "pid": 2568314, "hostname": "lambda-dual", "node_ip": "10.104.51.19", "config": {"num_workers": 2, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.0005, "train_batch_size": 256, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "cnet", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "1v1env", "observation_space": null, "action_space": null, "env_config": {}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": true, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": null, "evaluation_duration": 10, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"num_workers": 2, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.0005, "train_batch_size": 256, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "cnet", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "1v1env", "observation_space": null, "action_space": null, "env_config": {}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": false, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": null, "evaluation_duration": 10, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"explore": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": true, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": null, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 2, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"policy_01": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]], [[[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]], (3, 64, 64), float32)", "Discrete(7)", {}], "policy_02": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]], [[[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]], (3, 64, 64), float32)", "Discrete(7)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": "<function <lambda> at 0x7f1aa2ea4a70>", "policies_to_train": ["policy_01"], "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": 100000, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": false, "hiddens": [], "double_q": false, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": true, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": null, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 2, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"policy_01": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]], [[[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]], (3, 64, 64), float32)", "Discrete(7)", {}], "policy_02": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]], [[[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]], (3, 64, 64), float32)", "Discrete(7)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": "<function <lambda> at 0x7f1aa2ea4a70>", "policies_to_train": ["policy_01"], "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": 100000, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": false, "hiddens": [], "double_q": false, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "time_since_restore": 1163.3851764202118, "timesteps_since_restore": 14080, "iterations_since_restore": 55, "warmup_time": 45.46659183502197, "perf": {"cpu_util_percent": 24.73548387096774, "ram_util_percent": 13.577419354838716}}
{"episode_reward_max": 441.0, "episode_reward_min": 0.0, "episode_reward_mean": 21.391304347826086, "episode_len_mean": 601.0, "episode_media": {}, "episodes_this_iter": 2, "policy_reward_min": {"policy_01": 0.0, "policy_02": 0.0}, "policy_reward_max": {"policy_01": 441.0, "policy_02": 441.0}, "policy_reward_mean": {"policy_01": 11.804347826086957, "policy_02": 9.58695652173913}, "custom_metrics": {}, "hist_stats": {"episode_reward": [0.0, 204.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "episode_lengths": [601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601], "policy_policy_01_reward": [0.0, 204.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "policy_policy_02_reward": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.334908285529302, "mean_inference_ms": 5.803678962754425, "mean_action_processing_ms": 0.08887009336383427, "mean_env_wait_ms": 7.95866105479794, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 2, "timesteps_total": 56000, "timesteps_this_iter": 256, "agent_timesteps_total": 112000, "timers": {"load_time_ms": 1.382, "load_throughput": 185300.422, "learn_time_ms": 12.175, "learn_throughput": 21026.633, "update_time_ms": 2.448}, "info": {"learner": {"policy_01": {"custom_metrics": {}, "learner_stats": {"mean_q": 121.32669067382812, "min_q": 63.452552795410156, "max_q": 1281.3203125, "cur_lr": 0.0005}, "model": {}, "td_error": [-0.421875, -1.9904556274414062, -15.514739990234375, -1.235015869140625, -1.6947708129882812, -1.2489471435546875, 0.03369140625, -0.13260650634765625, -0.9621810913085938, -0.18556976318359375, -17.80303955078125, -1.5024261474609375, -0.07757568359375, 0.7733154296875, -5.211883544921875, -1.1941299438476562, -1.53399658203125, -0.8926773071289062, -1.1888809204101562, -8.4683837890625, -1.6456375122070312, -8.37744140625, -0.40183258056640625, 139.79933166503906, -139.20654296875, 15.480804443359375, 2.2567214965820312, -0.29285430908203125, -1.8339080810546875, 33.9302978515625, -4.221336364746094, 0.02317047119140625, -0.854766845703125, 0.24774932861328125, 0.9147186279296875, -2.1421661376953125, -1.9889984130859375, -1.3458709716796875, -2.3849411010742188, -1.8302688598632812, 8.986968994140625, 0.199554443359375, -9.22491455078125, -0.9396209716796875, -0.5486068725585938, -1.2447891235351562, -1.2685394287109375, -2.3309707641601562, -0.396087646484375, -0.1527862548828125, -1.51275634765625, -6.504707336425781, -1.1208267211914062, -54.56103515625, -0.04538726806640625, 0.323089599609375, -3.1894302368164062, -1.58447265625, -1.8453826904296875, -1.0983734130859375, -0.09143829345703125, -1.1437149047851562, -1.3582077026367188, -1.6222915649414062, -1.8506698608398438, -1.759674072265625, 0.33245849609375, 0.02997589111328125, -1.8120651245117188, 0.9368743896484375, -0.733551025390625, -24.0777587890625, 1.9037933349609375, 31.55902099609375, -0.6086807250976562, -0.11034393310546875, -1.3526611328125, -0.8876724243164062, -0.2157440185546875, -1.0963897705078125, -1.5923309326171875, -0.7488632202148438, -3.107421875, -1.13055419921875, -1.0514678955078125, -1.0068817138671875, -0.892974853515625, -0.7570571899414062, 0.8830032348632812, 68.7134017944336, -1.0159835815429688, -1.68402099609375, -1.4536285400390625, -2.1339569091796875, -3.5106735229492188, -0.8410873413085938, -1.3905181884765625, 0.417236328125, 1.4954986572265625, -0.29850006103515625, -3.6556930541992188, -69.866943359375, -2.1402053833007812, 0.8354034423828125, -0.25615692138671875, -3.0239715576171875, -0.5655059814453125, -0.0936126708984375, -2.9644088745117188, -6.6428680419921875, -0.16570281982421875, -2.0495681762695312, -1.694610595703125, -1.1329803466796875, -0.317626953125, 2.307647705078125, -2.4024085998535156, -0.9252471923828125, -0.6846771240234375, -10.60345458984375, 0.5550537109375, 2.4119720458984375, -2.36529541015625, -2.106475830078125, 1.2654876708984375, -1.3504486083984375, -0.669219970703125, 1.1864776611328125, -0.9014511108398438, -2.3047561645507812, 3.1899185180664062, -0.8685150146484375, -3.89215087890625, 0.09218597412109375, -0.9881362915039062, 1.1296920776367188, -1.1014785766601562, -0.8486328125, -1.8418197631835938, -1.133331298828125, -0.1270599365234375, 2.3967514038085938, 0.32508087158203125, -12.5709228515625, 10.030387878417969, -1.3298416137695312, -1.535491943359375, -1.568603515625, -0.252532958984375, -30.709075927734375, 10.972900390625, -0.23439788818359375, -0.26720428466796875, 0.27787017822265625, -1.5926055908203125, 11.94305419921875, -0.7188034057617188, -1.0604705810546875, -1.6181297302246094, -5.808563232421875, -1.743621826171875, 0.44770050048828125, -1.5173110961914062, 1.4029006958007812, -1.7332077026367188, -1.1920166015625, 1.82830810546875, 14.71466064453125, -0.8986434936523438, -0.6346511840820312, -1.584503173828125, 65.16644287109375, -1.46234130859375, -1.3865203857421875, 41.400238037109375, -1.2539138793945312, -1.7098388671875, -0.90228271484375, -0.8786773681640625, 0.41303253173828125, -0.8339309692382812, -1.3967666625976562, -0.7884445190429688, 0.16274261474609375, 0.20361328125, 1.4166107177734375, -1.9835891723632812, -21.0701904296875, -0.7619400024414062, -2.6553802490234375, -2.36383056640625, -2.051727294921875, 0.048095703125, 0.13301849365234375, 8.43902587890625, -0.07977294921875, -3.3922195434570312, -1.9700927734375, -1.3186988830566406, -1.0860061645507812, -0.8223495483398438, -1.3824615478515625, -1.8911209106445312, -1.7818450927734375, -0.3785858154296875, 1.0453872680664062, 0.089324951171875, -1.8912734985351562, -1.1466293334960938, -1.2907028198242188, 66.06324005126953, 1.5779953002929688, -0.16831207275390625, -0.638458251953125, 0.4595947265625, -0.26395416259765625, -1.2104415893554688, -1.3250350952148438, -2.9217300415039062, -0.929656982421875, -2.902557373046875, -0.581756591796875, -0.17185211181640625, -1.6339569091796875, -8.58447265625, -2.0374908447265625, -0.877838134765625, -1.1927261352539062, -2.48712158203125, -1.3476638793945312, -6.91925048828125, -3.279937744140625, 0.7983016967773438, -4.924934387207031, -1.8555145263671875, -1.3431777954101562, -0.10385894775390625, -1.4102249145507812, -0.2930908203125, -0.3042755126953125, -2.0304794311523438, -8.606773376464844, -0.5007400512695312, -2.18023681640625, -0.3364715576171875, -1.6411819458007812, -0.9609527587890625, -1.7532577514648438, -1.4665985107421875, -0.8161773681640625, -0.7608108520507812, -5.892242431640625, 27.438140869140625, -5.8671417236328125, -14.812080383300781, -0.7193069458007812], "mean_td_error": -0.5528208017349243}}, "num_steps_sampled": 56000, "num_agent_steps_sampled": 112000, "num_steps_trained": 1760256, "num_steps_trained_this_iter": 256, "num_agent_steps_trained": 3520512, "last_target_update_ts": 55936, "num_target_updates": 110}, "done": false, "episodes_total": 92, "training_iteration": 56, "trial_id": "e21e6_00000", "experiment_id": "434cd42d33fd4b638f17fc69fa01d74f", "date": "2022-06-14_19-21-46", "timestamp": 1655248906, "time_this_iter_s": 21.76555871963501, "time_total_s": 1185.1507351398468, "pid": 2568314, "hostname": "lambda-dual", "node_ip": "10.104.51.19", "config": {"num_workers": 2, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.0005, "train_batch_size": 256, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "cnet", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "1v1env", "observation_space": null, "action_space": null, "env_config": {}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": true, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": null, "evaluation_duration": 10, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"num_workers": 2, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.0005, "train_batch_size": 256, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "cnet", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "1v1env", "observation_space": null, "action_space": null, "env_config": {}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": false, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": null, "evaluation_duration": 10, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"explore": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": true, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": null, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 2, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"policy_01": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]], [[[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]], (3, 64, 64), float32)", "Discrete(7)", {}], "policy_02": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]], [[[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]], (3, 64, 64), float32)", "Discrete(7)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": "<function <lambda> at 0x7f1aa2ecb0e0>", "policies_to_train": ["policy_01"], "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": 100000, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": false, "hiddens": [], "double_q": false, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": true, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": null, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 2, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"policy_01": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]], [[[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]], (3, 64, 64), float32)", "Discrete(7)", {}], "policy_02": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]], [[[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]], (3, 64, 64), float32)", "Discrete(7)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": "<function <lambda> at 0x7f1aa2ecb0e0>", "policies_to_train": ["policy_01"], "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": 100000, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": false, "hiddens": [], "double_q": false, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "time_since_restore": 1185.1507351398468, "timesteps_since_restore": 14336, "iterations_since_restore": 56, "warmup_time": 45.46659183502197, "perf": {"cpu_util_percent": 24.8, "ram_util_percent": 13.638709677419351}}
{"episode_reward_max": 441.0, "episode_reward_min": 0.0, "episode_reward_mean": 20.93617021276596, "episode_len_mean": 601.0, "episode_media": {}, "episodes_this_iter": 2, "policy_reward_min": {"policy_01": 0.0, "policy_02": 0.0}, "policy_reward_max": {"policy_01": 441.0, "policy_02": 441.0}, "policy_reward_mean": {"policy_01": 11.553191489361701, "policy_02": 9.382978723404255}, "custom_metrics": {}, "hist_stats": {"episode_reward": [0.0, 204.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "episode_lengths": [601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601], "policy_policy_01_reward": [0.0, 204.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "policy_policy_02_reward": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.33485073463614995, "mean_inference_ms": 5.802949943529179, "mean_action_processing_ms": 0.0888653063629079, "mean_env_wait_ms": 7.954318080971742, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 2, "timesteps_total": 57000, "timesteps_this_iter": 256, "agent_timesteps_total": 114000, "timers": {"load_time_ms": 1.353, "load_throughput": 189142.283, "learn_time_ms": 11.917, "learn_throughput": 21482.484, "update_time_ms": 2.233}, "info": {"learner": {"policy_01": {"custom_metrics": {}, "learner_stats": {"mean_q": 174.36349487304688, "min_q": 66.4878158569336, "max_q": 1454.08056640625, "cur_lr": 0.0005}, "model": {}, "td_error": [0.7095870971679688, 0.5870513916015625, 72.59827423095703, -14.39263916015625, 1.6856689453125, 1.7200393676757812, 70.07364654541016, -1.0560836791992188, -38.887847900390625, 11.8426513671875, 2.5334014892578125, 1.5811386108398438, -85.6695556640625, 1.0564727783203125, 0.24553680419921875, 1.330078125, 254.33584594726562, 2.7163009643554688, 0.34226226806640625, 1.07037353515625, 2.308074951171875, 6.578033447265625, -0.1976318359375, -1.0117263793945312, 3.036041259765625, -9.954010009765625, -0.521148681640625, 254.33584594726562, 2.7802581787109375, 1.2903900146484375, 1.2586669921875, 14.004425048828125, 2.912689208984375, 2.4348526000976562, -26.894775390625, 1.6002044677734375, -1.91229248046875, 0.19187164306640625, 3.8332061767578125, -3.6153793334960938, -2.7989883422851562, 0.33819580078125, 28.85882568359375, 71.56980895996094, 2.002899169921875, 0.73193359375, -0.29180908203125, 2.3319778442382812, -0.5445175170898438, 32.118133544921875, -0.11412811279296875, -20.8885498046875, 22.588134765625, 1.4488601684570312, 0.6484527587890625, -0.9551925659179688, 69.8976821899414, 0.5834503173828125, 0.3431396484375, 1.6313858032226562, -20.4654541015625, 0.43807220458984375, -0.0345458984375, 0.09407806396484375, 1.8521881103515625, 2.2580108642578125, 1.2575759887695312, 0.8190155029296875, -33.7860107421875, 24.307037353515625, 1.4831008911132812, 1.4584579467773438, 1.4967193603515625, -1.479278564453125, 1.860137939453125, 1.3079605102539062, 1.7508163452148438, -85.2138671875, 0.46491241455078125, 0.47800445556640625, 3.241851806640625, 12.667510986328125, -138.8681640625, 0.1712188720703125, 2.5613632202148438, 0.9796218872070312, 0.442535400390625, -1.1844024658203125, -0.065155029296875, 0.8098526000976562, 0.36959075927734375, 0.43840789794921875, 1.2792282104492188, 0.19481658935546875, 0.064422607421875, 1.3332061767578125, -0.6310348510742188, 1.6060714721679688, 0.11965179443359375, -9.542510986328125, 0.8887405395507812, -0.3824005126953125, -0.7360916137695312, 71.87519073486328, 2.3726348876953125, -8.731353759765625, 0.9454803466796875, -10.908355712890625, 4.48040771484375, -0.2069244384765625, 0.4622802734375, 2.544189453125, 2.66815185546875, 1.2899093627929688, -0.06465911865234375, 0.6831893920898438, 3.0801925659179688, 1.1656875610351562, 1.98834228515625, 19.442047119140625, 10.8878173828125, -3.15625, 70.26862335205078, 2.0714950561523438, -45.7462158203125, 2.2449569702148438, 2.9521865844726562, 23.655548095703125, 0.27398681640625, 0.9352951049804688, 1.2851943969726562, 1.5256805419921875, 1.1453628540039062, 1.835693359375, 1.3627243041992188, 1.715850830078125, -0.2947540283203125, 2.347686767578125, 1.49212646484375, 35.9901123046875, 9.6602783203125, 0.6703643798828125, 1.8521041870117188, 1.6504135131835938, 0.04953765869140625, 0.47383880615234375, -74.4105224609375, 1.1650161743164062, 1.6861343383789062, 1.5227127075195312, 1.6467971801757812, 1.0220565795898438, 2.0400466918945312, 2.1626434326171875, 0.8747177124023438, 2.1220703125, 0.6265182495117188, 1.165863037109375, 2.5577392578125, 74.1070327758789, 2.32550048828125, -261.01654052734375, 0.33077239990234375, -3.6545486450195312, 0.8769607543945312, -0.854888916015625, 0.8603439331054688, -3.7296142578125, 2.2305831909179688, 0.935028076171875, -0.269500732421875, 1.0279083251953125, -2.2288436889648438, 0.9466781616210938, -1.7001419067382812, 0.205352783203125, 73.55709075927734, -2.302215576171875, -1.962738037109375, 0.24544525146484375, -76.44902801513672, -16.1558837890625, -1.6752166748046875, 73.46611785888672, 3.3312835693359375, -0.17917633056640625, 0.3819427490234375, 1.4218597412109375, -6.7683258056640625, 2.5068893432617188, -0.648529052734375, 2.3543777465820312, 0.5022964477539062, -1.4007797241210938, 0.911346435546875, -1.2840194702148438, 0.5022964477539062, 0.16489410400390625, -3.8043594360351562, 10.433631896972656, 1.4219512939453125, 0.5553665161132812, -2.7999191284179688, 2.6444854736328125, 73.55709075927734, 1.6727447509765625, 1.5023574829101562, -0.14844512939453125, 1.3438034057617188, 1.1595611572265625, 1.1193313598632812, -0.5312881469726562, 0.887603759765625, 74.11296081542969, 1.2553863525390625, -2.4290847778320312, 1.163360595703125, 0.900054931640625, 0.70367431640625, -0.35823822021484375, 0.5926742553710938, 0.43799591064453125, 1.2415084838867188, 1.2385482788085938, 0.33287811279296875, -0.8091583251953125, 0.07051849365234375, 71.5292739868164, 0.01242828369140625, 1.1364898681640625, 1.353302001953125, 0.48339080810546875, 16.6229248046875, 1.10247802734375, 402.0088806152344, 1.3581466674804688, 2.086761474609375, 0.3907928466796875, -2.2402572631835938, 2.6489639282226562, 0.13411712646484375, 70.24172973632812, 2.2441177368164062, 1.84716796875, 1.3519363403320312, -0.083221435546875, 1.1953582763671875, -43.931396484375, -0.29644012451171875, -2.3922882080078125, -1.301605224609375, 0.3871002197265625, 5.3048095703125, 3.217010498046875, 14.40496826171875, -0.6378860473632812], "mean_td_error": 4.943205833435059}}, "num_steps_sampled": 57000, "num_agent_steps_sampled": 114000, "num_steps_trained": 1792256, "num_steps_trained_this_iter": 256, "num_agent_steps_trained": 3584512, "last_target_update_ts": 56944, "num_target_updates": 112}, "done": false, "episodes_total": 94, "training_iteration": 57, "trial_id": "e21e6_00000", "experiment_id": "434cd42d33fd4b638f17fc69fa01d74f", "date": "2022-06-14_19-22-08", "timestamp": 1655248928, "time_this_iter_s": 22.018961429595947, "time_total_s": 1207.1696965694427, "pid": 2568314, "hostname": "lambda-dual", "node_ip": "10.104.51.19", "config": {"num_workers": 2, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.0005, "train_batch_size": 256, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "cnet", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "1v1env", "observation_space": null, "action_space": null, "env_config": {}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": true, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": null, "evaluation_duration": 10, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"num_workers": 2, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.0005, "train_batch_size": 256, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "cnet", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "1v1env", "observation_space": null, "action_space": null, "env_config": {}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": false, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": null, "evaluation_duration": 10, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"explore": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": true, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": null, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 2, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"policy_01": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]], [[[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]], (3, 64, 64), float32)", "Discrete(7)", {}], "policy_02": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]], [[[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]], (3, 64, 64), float32)", "Discrete(7)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": "<function <lambda> at 0x7f1aa37643b0>", "policies_to_train": ["policy_01"], "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": 100000, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": false, "hiddens": [], "double_q": false, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": true, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": null, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 2, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"policy_01": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]], [[[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]], (3, 64, 64), float32)", "Discrete(7)", {}], "policy_02": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]], [[[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]], (3, 64, 64), float32)", "Discrete(7)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": "<function <lambda> at 0x7f1aa37643b0>", "policies_to_train": ["policy_01"], "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": 100000, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": false, "hiddens": [], "double_q": false, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "time_since_restore": 1207.1696965694427, "timesteps_since_restore": 14592, "iterations_since_restore": 57, "warmup_time": 45.46659183502197, "perf": {"cpu_util_percent": 24.78125, "ram_util_percent": 13.728124999999999}}
{"episode_reward_max": 441.0, "episode_reward_min": 0.0, "episode_reward_mean": 20.5, "episode_len_mean": 601.0, "episode_media": {}, "episodes_this_iter": 2, "policy_reward_min": {"policy_01": 0.0, "policy_02": 0.0}, "policy_reward_max": {"policy_01": 441.0, "policy_02": 441.0}, "policy_reward_mean": {"policy_01": 11.3125, "policy_02": 9.1875}, "custom_metrics": {}, "hist_stats": {"episode_reward": [0.0, 204.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "episode_lengths": [601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601], "policy_policy_01_reward": [0.0, 204.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "policy_policy_02_reward": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.3347987227343024, "mean_inference_ms": 5.8022086770035175, "mean_action_processing_ms": 0.08886021943323796, "mean_env_wait_ms": 7.950070985328751, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 2, "timesteps_total": 58000, "timesteps_this_iter": 256, "agent_timesteps_total": 116000, "timers": {"load_time_ms": 1.315, "load_throughput": 194659.504, "learn_time_ms": 11.837, "learn_throughput": 21626.394, "update_time_ms": 2.34}, "info": {"learner": {"policy_01": {"custom_metrics": {}, "learner_stats": {"mean_q": 213.15667724609375, "min_q": 67.47468566894531, "max_q": 1487.788330078125, "cur_lr": 0.0005}, "model": {}, "td_error": [37.473114013671875, 0.8061065673828125, 0.8151321411132812, 0.503204345703125, 1.694915771484375, 0.2085723876953125, 69.58575439453125, 0.8621978759765625, 5.371177673339844, -1.4588623046875, -0.6096649169921875, -17.7774658203125, 0.2352142333984375, -1.7833175659179688, -0.39606475830078125, 1.886138916015625, 0.15261077880859375, 1.1630172729492188, 1.3529281616210938, -12.001007080078125, 31.134979248046875, 0.3377685546875, 1.322784423828125, 0.9419631958007812, -6.3472900390625, -1.1057357788085938, 1.3338470458984375, 0.6576080322265625, 1.8539581298828125, 0.5700912475585938, 1.0922698974609375, -0.116668701171875, -0.5531997680664062, -5.259033203125, 0.7760696411132812, 2.3293914794921875, 2.008087158203125, 1.7695159912109375, -0.5628433227539062, -0.032135009765625, 2.0777130126953125, 42.6214599609375, 2.3095703125, 3.0242156982421875, -0.084136962890625, 0.30236053466796875, -0.14891815185546875, 1.4927291870117188, 26.498779296875, 42.36181640625, 0.3088531494140625, 1.7013397216796875, 1.1038055419921875, -0.783843994140625, 6.507598876953125, 0.48767852783203125, 0.230316162109375, 72.26519012451172, 1.0132064819335938, 9.641510009765625, -0.7461090087890625, -0.04367828369140625, -4.1595916748046875, 0.8132476806640625, 1.0745162963867188, 1.8965988159179688, 0.8853683471679688, 87.13720703125, 4.531219482421875, -0.000213623046875, 0.5662612915039062, 0.4194488525390625, 0.0287322998046875, -0.03240966796875, -5.506378173828125, 0.14122772216796875, 0.09410858154296875, 0.0939178466796875, 1.2766952514648438, 0.2452239990234375, -3.165313720703125, 1.8313140869140625, 4.078460693359375, 1.2862014770507812, 3.4101104736328125, 13.717193603515625, 1.3891830444335938, 11.0030517578125, 0.41742706298828125, 1.8375167846679688, -1.2459564208984375, -0.8965225219726562, 0.89068603515625, -1.4074325561523438, 0.9084014892578125, 0.30323028564453125, 14.286598205566406, 3.3972244262695312, 12.267822265625, 0.8742828369140625, 1.0002899169921875, 0.874908447265625, 2.22149658203125, 1.1884613037109375, 1.7652740478515625, 1.34674072265625, 12.949462890625, 1.3411483764648438, 70.96708679199219, 7.714263916015625, 2.4020538330078125, -0.6778488159179688, 1.941802978515625, -15.915908813476562, 0.199981689453125, 0.12082672119140625, 1.6261978149414062, 56.8299560546875, 1.18426513671875, 0.00844573974609375, 55.214927673339844, 3.054779052734375, -0.4327239990234375, 59.16650390625, 2.0964126586914062, 5.42291259765625, 68.4145736694336, 3.239501953125, -1.4610977172851562, 1.4077987670898438, -1.4285888671875, 2.003387451171875, -0.52740478515625, -0.874725341796875, 1.1250991821289062, 1.3255386352539062, 0.12335205078125, -0.6123886108398438, 1.2180099487304688, -1.4542770385742188, -11.036224365234375, -2.9534454345703125, -0.00463104248046875, 0.7876510620117188, 0.2953338623046875, 1.3797683715820312, 0.8378219604492188, 3.9500732421875, 10.982421875, 1.5953903198242188, -23.668212890625, -1.5948944091796875, 0.820159912109375, 0.3798370361328125, -1.1329193115234375, 0.9461441040039062, -1.3484954833984375, 38.6513671875, -1.511627197265625, 6.74224853515625, 59.16650390625, 4.741363525390625, 0.21266937255859375, 0.29691314697265625, 35.4906005859375, 1.3537521362304688, -33.006103515625, -0.6077957153320312, -0.3742218017578125, 28.284454345703125, 5.468536376953125, 424.86151123046875, 1.5820846557617188, -0.4815826416015625, 12.798828125, 4.523529052734375, 1.5720748901367188, 1.729339599609375, -11.446250915527344, 0.7759933471679688, 2.5462265014648438, 25.57196044921875, -4.64013671875, 1.7731704711914062, 1.5460281372070312, 0.05712127685546875, 1.3116836547851562, 0.11565399169921875, 0.31604766845703125, -4.163810729980469, 1.7207183837890625, 1.7802963256835938, 0.6006546020507812, -0.03830718994140625, 0.03833770751953125, 0.9125518798828125, 0.9846954345703125, 39.1964111328125, 69.62394714355469, 0.6439285278320312, 0.4198760986328125, 14.288970947265625, 1.4577484130859375, -0.6397705078125, 1.7647552490234375, 2.178131103515625, 0.15045166015625, 2.4342880249023438, 4.713165283203125, 3.05804443359375, 4.861015319824219, 0.33382415771484375, -28.94482421875, 2.2792205810546875, 4.110984802246094, 0.7557601928710938, 35.91180419921875, -0.7247390747070312, 0.203765869140625, 14.5733642578125, 71.10345458984375, 1.9079666137695312, 1.5340805053710938, 1.7942962646484375, -0.6813812255859375, -1.939605712890625, 0.33379364013671875, -0.01406097412109375, -11.462158203125, 1.302734375, 2.0729827880859375, 0.9366912841796875, 2.1040267944335938, 14.478012084960938, 0.8403854370117188, 1.3395233154296875, 2.5182342529296875, 0.8221282958984375, 1.4047088623046875, 3.7989120483398438, 0.6076431274414062, 38.989501953125, -1.227874755859375, 1.5622329711914062, 0.827178955078125, 0.773223876953125, 1.0622177124023438, 0.27312469482421875, 4.4449462890625, -0.5575027465820312, 1.5551223754882812, 0.5163345336914062, -55.3599853515625, 0.8908615112304688, 0.652252197265625, 0.38336944580078125], "mean_td_error": 6.5894775390625}}, "num_steps_sampled": 58000, "num_agent_steps_sampled": 116000, "num_steps_trained": 1824256, "num_steps_trained_this_iter": 256, "num_agent_steps_trained": 3648512, "last_target_update_ts": 57952, "num_target_updates": 114}, "done": false, "episodes_total": 96, "training_iteration": 58, "trial_id": "e21e6_00000", "experiment_id": "434cd42d33fd4b638f17fc69fa01d74f", "date": "2022-06-14_19-22-30", "timestamp": 1655248950, "time_this_iter_s": 21.735210418701172, "time_total_s": 1228.904906988144, "pid": 2568314, "hostname": "lambda-dual", "node_ip": "10.104.51.19", "config": {"num_workers": 2, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.0005, "train_batch_size": 256, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "cnet", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "1v1env", "observation_space": null, "action_space": null, "env_config": {}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": true, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": null, "evaluation_duration": 10, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"num_workers": 2, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.0005, "train_batch_size": 256, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "cnet", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "1v1env", "observation_space": null, "action_space": null, "env_config": {}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": false, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": null, "evaluation_duration": 10, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"explore": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": true, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": null, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 2, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"policy_01": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]], [[[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]], (3, 64, 64), float32)", "Discrete(7)", {}], "policy_02": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]], [[[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]], (3, 64, 64), float32)", "Discrete(7)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": "<function <lambda> at 0x7f1aa2ebedd0>", "policies_to_train": ["policy_01"], "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": 100000, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": false, "hiddens": [], "double_q": false, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": true, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": null, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 2, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"policy_01": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]], [[[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]], (3, 64, 64), float32)", "Discrete(7)", {}], "policy_02": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]], [[[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]], (3, 64, 64), float32)", "Discrete(7)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": "<function <lambda> at 0x7f1aa2ebedd0>", "policies_to_train": ["policy_01"], "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": 100000, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": false, "hiddens": [], "double_q": false, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "time_since_restore": 1228.904906988144, "timesteps_since_restore": 14848, "iterations_since_restore": 58, "warmup_time": 45.46659183502197, "perf": {"cpu_util_percent": 24.848387096774196, "ram_util_percent": 13.819354838709675}}
{"episode_reward_max": 441.0, "episode_reward_min": 0.0, "episode_reward_mean": 20.081632653061224, "episode_len_mean": 601.0, "episode_media": {}, "episodes_this_iter": 2, "policy_reward_min": {"policy_01": 0.0, "policy_02": 0.0}, "policy_reward_max": {"policy_01": 441.0, "policy_02": 441.0}, "policy_reward_mean": {"policy_01": 11.081632653061224, "policy_02": 9.0}, "custom_metrics": {}, "hist_stats": {"episode_reward": [0.0, 204.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "episode_lengths": [601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601], "policy_policy_01_reward": [0.0, 204.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "policy_policy_02_reward": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.3347512928839464, "mean_inference_ms": 5.801471242314613, "mean_action_processing_ms": 0.08885513858735661, "mean_env_wait_ms": 7.945954277418491, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 2, "timesteps_total": 59000, "timesteps_this_iter": 256, "agent_timesteps_total": 118000, "timers": {"load_time_ms": 1.308, "load_throughput": 195720.425, "learn_time_ms": 11.375, "learn_throughput": 22506.395, "update_time_ms": 2.293}, "info": {"learner": {"policy_01": {"custom_metrics": {}, "learner_stats": {"mean_q": 220.49081420898438, "min_q": 69.25364685058594, "max_q": 1986.70263671875, "cur_lr": 0.0005}, "model": {}, "td_error": [-3.300506591796875, -32.375144958496094, 50.9725341796875, 1327.550537109375, -2.4544525146484375, -2.9165878295898438, -0.370269775390625, -67.777099609375, -5.2523651123046875, -1.9670639038085938, -3.7388839721679688, 0.23752593994140625, -4.32958984375, -3.1384811401367188, -1.391082763671875, -0.6771011352539062, 19.468841552734375, -5.207023620605469, -1.3684539794921875, -2.7740554809570312, 80.59686279296875, 77.3179931640625, -2.521881103515625, -44.480560302734375, 21.86029052734375, -5.095222473144531, -3.7347564697265625, -1.0541000366210938, -3.3394622802734375, 0.32208251953125, -1.8619308471679688, -7.3468017578125, -2.67999267578125, -0.12055206298828125, -35.6539306640625, -1.3257217407226562, -3.6613006591796875, 17.755035400390625, -1.2722320556640625, -2.674468994140625, -1.4510116577148438, -4.570610046386719, -0.7870864868164062, -1.7453460693359375, -29.410812377929688, -1.4544448852539062, -3.5520706176757812, -297.3812561035156, -2.7585067749023438, 0.239013671875, -24.756179809570312, -1.3250732421875, -21.048782348632812, -2.0167999267578125, 0.764984130859375, -2.8416671752929688, -2.4821319580078125, -1.0829544067382812, 6.040397644042969, 0.9528579711914062, -1.0805282592773438, -5.0555267333984375, 7.967521667480469, 1.04400634765625, 0.04662322998046875, -1.11029052734375, -17.19482421875, -5.105384826660156, -1.8541641235351562, -0.775482177734375, -2.249786376953125, -2.4335174560546875, -3.405853271484375, 4.259307861328125, 0.026519775390625, -6.0008697509765625, -2.0957107543945312, -1.5829086303710938, -1.3786087036132812, -0.7927017211914062, 3.7034149169921875, -2.965179443359375, -1.336761474609375, 0.11038970947265625, -1.8205108642578125, 0.49953460693359375, -3.0334548950195312, -1.2410430908203125, -1.0796966552734375, -2.261016845703125, -3.022796630859375, -143.9405517578125, 8.482574462890625, -1.6538009643554688, 0.117401123046875, -3.5774078369140625, 1.0585174560546875, -3.0371551513671875, -2.4546737670898438, -9.6575927734375, 72.2789306640625, 1.5208053588867188, -1.5301055908203125, -0.794708251953125, 1.9405517578125, 31.034576416015625, -2.9624176025390625, -1.6952362060546875, -2.5481033325195312, -4.906974792480469, -2.8402557373046875, -0.4416351318359375, 4.332244873046875, -1.781463623046875, 0.1968841552734375, 0.46044921875, -0.04815673828125, 70.17412567138672, -1.6613235473632812, 3.8761444091796875, 4.15789794921875, -0.94696044921875, -2.2731781005859375, 51.1182861328125, -3.0120010375976562, 1.0213623046875, -2.8307113647460938, 1.2962646484375, -2.439544677734375, -0.9027252197265625, -1.7977218627929688, -0.8971328735351562, -0.7910537719726562, 0.9281005859375, 6.072021484375, 0.17383575439453125, -1.2191009521484375, -2.5268936157226562, -1.7563247680664062, -1.0863571166992188, 4.118736267089844, -3.8304977416992188, -1.5209579467773438, -0.8673019409179688, -1.73455810546875, -77.833740234375, -3.9478836059570312, 4.18865966796875, 55.304534912109375, 19.33306884765625, -17.91595458984375, -1.0231475830078125, 23.06695556640625, -2.5961227416992188, 0.5334930419921875, -38.3162841796875, 2.621978759765625, -3.8349990844726562, -2.8475570678710938, -2.4416656494140625, -0.39923858642578125, 3.5972900390625, 25.689453125, -0.34589385986328125, -1.3434524536132812, -2.9514999389648438, -2.0718154907226562, -3.1145401000976562, -0.619720458984375, -0.38134765625, 1.5506820678710938, 22.880126953125, -0.6246109008789062, 71.34319305419922, 1.3068923950195312, -1.7739715576171875, -3.1912765502929688, -0.540008544921875, -2.0065460205078125, -1.40716552734375, -1.8277053833007812, 0.41736602783203125, -0.9293441772460938, 1.5607757568359375, -0.48171234130859375, -2.6830368041992188, -2.7021636962890625, -0.7558364868164062, -16.7535400390625, -2.794830322265625, -1.4833831787109375, -0.708740234375, 21.25958251953125, -2.528289794921875, -3.7903976440429688, 4.15631103515625, -1.5874176025390625, -1.47259521484375, 0.9928131103515625, -3.592803955078125, -5.32275390625, -4.278472900390625, -0.6305999755859375, -1.7233734130859375, -0.2508697509765625, -1.4501571655273438, -0.8683242797851562, -1.0921783447265625, -2.5413894653320312, 12.126129150390625, -3.2585983276367188, -2.5166397094726562, -1.243133544921875, -37.9364013671875, 0.1621856689453125, -1.4017257690429688, -3.8729248046875, 0.16829681396484375, 13.406982421875, 1.9219589233398438, 40.095458984375, 142.083251953125, -2.9590225219726562, -1.329132080078125, -0.4409027099609375, -17.19482421875, 12.536041259765625, -24.8331298828125, -1.5637359619140625, -17.263168334960938, -0.6373367309570312, -100.5013427734375, -2.2194747924804688, -0.7945938110351562, -3.8122100830078125, -16.740997314453125, 30.27978515625, 1.4842529296875, 9.076904296875, -0.4289398193359375, -1.2133102416992188, -1.4350509643554688, -4.9204254150390625, -1.4630813598632812, -1.2567062377929688, -3.0589370727539062, -3.2432327270507812, -2.3743820190429688, -1.6164093017578125, -2.647369384765625, -1.259429931640625, 75.65286254882812, 1.089202880859375, -2.4828567504882812, -2.7863388061523438, -2.8884811401367188], "mean_td_error": 4.059542655944824}}, "num_steps_sampled": 59000, "num_agent_steps_sampled": 118000, "num_steps_trained": 1856256, "num_steps_trained_this_iter": 256, "num_agent_steps_trained": 3712512, "last_target_update_ts": 58960, "num_target_updates": 116}, "done": false, "episodes_total": 98, "training_iteration": 59, "trial_id": "e21e6_00000", "experiment_id": "434cd42d33fd4b638f17fc69fa01d74f", "date": "2022-06-14_19-22-52", "timestamp": 1655248972, "time_this_iter_s": 21.903143167495728, "time_total_s": 1250.8080501556396, "pid": 2568314, "hostname": "lambda-dual", "node_ip": "10.104.51.19", "config": {"num_workers": 2, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.0005, "train_batch_size": 256, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "cnet", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "1v1env", "observation_space": null, "action_space": null, "env_config": {}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": true, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": null, "evaluation_duration": 10, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"num_workers": 2, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.0005, "train_batch_size": 256, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "cnet", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "1v1env", "observation_space": null, "action_space": null, "env_config": {}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": false, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": null, "evaluation_duration": 10, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"explore": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": true, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": null, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 2, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"policy_01": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]], [[[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]], (3, 64, 64), float32)", "Discrete(7)", {}], "policy_02": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]], [[[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]], (3, 64, 64), float32)", "Discrete(7)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": "<function <lambda> at 0x7f1aa2ea43b0>", "policies_to_train": ["policy_01"], "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": 100000, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": false, "hiddens": [], "double_q": false, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": true, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": null, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 2, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"policy_01": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]], [[[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]], (3, 64, 64), float32)", "Discrete(7)", {}], "policy_02": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]], [[[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]], (3, 64, 64), float32)", "Discrete(7)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": "<function <lambda> at 0x7f1aa2ea43b0>", "policies_to_train": ["policy_01"], "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": 100000, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": false, "hiddens": [], "double_q": false, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "time_since_restore": 1250.8080501556396, "timesteps_since_restore": 15104, "iterations_since_restore": 59, "warmup_time": 45.46659183502197, "perf": {"cpu_util_percent": 24.690322580645162, "ram_util_percent": 13.899999999999995}}
{"episode_reward_max": 441.0, "episode_reward_min": 0.0, "episode_reward_mean": 20.081632653061224, "episode_len_mean": 601.0, "episode_media": {}, "episodes_this_iter": 0, "policy_reward_min": {"policy_01": 0.0, "policy_02": 0.0}, "policy_reward_max": {"policy_01": 441.0, "policy_02": 441.0}, "policy_reward_mean": {"policy_01": 11.081632653061224, "policy_02": 9.0}, "custom_metrics": {}, "hist_stats": {"episode_reward": [0.0, 204.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "episode_lengths": [601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601], "policy_policy_01_reward": [0.0, 204.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "policy_policy_02_reward": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.3347512928839464, "mean_inference_ms": 5.801471242314613, "mean_action_processing_ms": 0.08885513858735661, "mean_env_wait_ms": 7.945954277418491, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 2, "timesteps_total": 60000, "timesteps_this_iter": 256, "agent_timesteps_total": 120000, "timers": {"load_time_ms": 1.321, "load_throughput": 193767.247, "learn_time_ms": 12.126, "learn_throughput": 21111.923, "update_time_ms": 2.369}, "info": {"learner": {"policy_01": {"custom_metrics": {}, "learner_stats": {"mean_q": 255.16085815429688, "min_q": 68.87605285644531, "max_q": 1956.1268310546875, "cur_lr": 0.0005}, "model": {}, "td_error": [-1.0612640380859375, 0.874176025390625, -3.5103759765625, 0.5891647338867188, -1.0623397827148438, -1.0584564208984375, -0.1306915283203125, 13.49609375, -0.5267868041992188, -1.2037887573242188, 1.5694656372070312, 17.348373413085938, -0.7362289428710938, -0.19672393798828125, 0.25377655029296875, 90.0875244140625, -1.0879898071289062, -0.8985214233398438, -0.8230361938476562, 0.887115478515625, -0.44518280029296875, -0.7439727783203125, -0.62103271484375, 0.3250732421875, 0.48388671875, 49.34471130371094, 0.6172561645507812, -0.5183258056640625, -0.19504547119140625, 19.57147216796875, -0.20809173583984375, -0.6722259521484375, -0.855560302734375, -0.6997299194335938, 1.2732467651367188, 0.4039306640625, 80.2384033203125, -0.8712692260742188, 0.574981689453125, 70.83980560302734, -1.1151199340820312, -26.42724609375, 0.9787063598632812, -17.3023681640625, 51.7535400390625, -56.79595947265625, 0.35431671142578125, -0.452056884765625, -1.32257080078125, 1.3344192504882812, 22.017578125, -0.7814483642578125, 4.3789215087890625, -267.8701171875, -2.3917465209960938, -0.6387176513671875, 3.9038009643554688, -0.11142730712890625, 0.3563079833984375, -0.8154830932617188, -0.8159713745117188, -0.5796585083007812, 31.273193359375, -1.0659942626953125, 33.9744873046875, -12.177459716796875, -72.31063842773438, -0.4984893798828125, -0.9697265625, 0.3676605224609375, -0.765350341796875, -0.5044479370117188, 1.5352249145507812, 1.0645980834960938, 1.1792526245117188, -102.72021484375, 0.9783096313476562, -38.777099609375, -0.9155807495117188, 3.6015625, -5.89947509765625, -0.9450912475585938, 71.69686126708984, -210.9150390625, -0.91302490234375, -0.6046829223632812, 0.5508499145507812, -3.9170608520507812, -0.582550048828125, 104.58328247070312, -0.6044692993164062, -0.7181854248046875, 0.18456268310546875, 0.7761459350585938, -0.1624908447265625, -1.01910400390625, 0.14049530029296875, -0.7404022216796875, -0.7599945068359375, -0.194580078125, 0.27344512939453125, 75.60574340820312, 71.03129577636719, 8.85577392578125, -42.3892822265625, -0.5620346069335938, -0.1416015625, 0.49745941162109375, -0.8580169677734375, 0.6363372802734375, -0.87847900390625, -0.8459014892578125, -320.066650390625, 13.795425415039062, 2.8202896118164062, 0.25113677978515625, 0.01392364501953125, 0.7147445678710938, -0.9588470458984375, -0.5532455444335938, 0.240875244140625, -0.237396240234375, -5.399375915527344, 0.11479949951171875, -279.6336364746094, 1.5789413452148438, 0.8285751342773438, -1.3133773803710938, -9.597900390625, -1.1653671264648438, -33.720245361328125, -0.9931640625, -0.950408935546875, -0.25926971435546875, -83.5277099609375, -0.47776031494140625, 8.826736450195312, -0.9268951416015625, -0.27437591552734375, -57.923095703125, 0.595733642578125, 73.09880065917969, -0.2003631591796875, -273.357177734375, 1.2026214599609375, -0.6387176513671875, -0.13817596435546875, -1.1777725219726562, -1.4473495483398438, -0.9512786865234375, 0.5845565795898438, 13.492912292480469, -79.510009765625, -18.1517333984375, -0.8684463500976562, -0.7935867309570312, -65.52734375, -1.059722900390625, 0.4981536865234375, 35.299560546875, 0.5806427001953125, -1.1584930419921875, 1.5435104370117188, -1.552703857421875, 16.167335510253906, -1.0029754638671875, 0.35477447509765625, -2.53277587890625, -4.6532440185546875, -0.7038192749023438, -4.849700927734375, 3.01092529296875, 0.17547607421875, -0.533111572265625, -4.8280029296875, 0.8225173950195312, -0.615142822265625, 22.333694458007812, -0.483734130859375, 53.407318115234375, -0.3189697265625, -0.8237457275390625, -1.0005950927734375, -0.8958511352539062, -0.30712890625, -1.0848770141601562, 0.2434539794921875, -0.7083740234375, -9.067779541015625, -1.0907669067382812, -1.8295669555664062, -0.5878067016601562, 0.2103424072265625, -1.026519775390625, 3.089508056640625, -0.41631317138671875, -60.4249267578125, -0.8432388305664062, 1.6708526611328125, -1.96673583984375, -0.35865020751953125, 1478.72607421875, -1.1352310180664062, -1.0503387451171875, 1.2010345458984375, 8.83935546875, -1.0192489624023438, -0.7251129150390625, 0.6031570434570312, -1.0398788452148438, 0.5300369262695312, 0.3460693359375, 3.8833770751953125, -1.1961898803710938, -11.055206298828125, 19.71044921875, -0.46240997314453125, 14.072509765625, -8.635162353515625, 1.4472732543945312, 71.000244140625, 0.6471405029296875, -0.7545394897460938, 0.869842529296875, -2.2898406982421875, -0.13250732421875, -0.1747894287109375, -0.07196044921875, -1.0046844482421875, -1.172637939453125, -1.0028457641601562, -284.1402587890625, -6.20361328125, -0.71832275390625, -0.8376922607421875, -47.608184814453125, 13.924041748046875, -0.27011871337890625, 2.982513427734375, 2.24462890625, -1.4189300537109375, -0.7962188720703125, 10.082321166992188, 0.6442108154296875, -1.1340103149414062, -27.248573303222656, -0.1555023193359375, 73.3396224975586, 0.19549560546875, 1.9672012329101562, -1.145965576171875, -0.335723876953125, 4.6268310546875, -0.6144485473632812, 1.1695480346679688, 0.173583984375], "mean_td_error": 0.6271028518676758}}, "num_steps_sampled": 60000, "num_agent_steps_sampled": 120000, "num_steps_trained": 1888256, "num_steps_trained_this_iter": 256, "num_agent_steps_trained": 3776512, "last_target_update_ts": 59968, "num_target_updates": 118}, "done": false, "episodes_total": 98, "training_iteration": 60, "trial_id": "e21e6_00000", "experiment_id": "434cd42d33fd4b638f17fc69fa01d74f", "date": "2022-06-14_19-23-14", "timestamp": 1655248994, "time_this_iter_s": 21.773189544677734, "time_total_s": 1272.5812397003174, "pid": 2568314, "hostname": "lambda-dual", "node_ip": "10.104.51.19", "config": {"num_workers": 2, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.0005, "train_batch_size": 256, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "cnet", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "1v1env", "observation_space": null, "action_space": null, "env_config": {}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": true, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": null, "evaluation_duration": 10, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"num_workers": 2, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.0005, "train_batch_size": 256, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "cnet", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "1v1env", "observation_space": null, "action_space": null, "env_config": {}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": false, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": null, "evaluation_duration": 10, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"explore": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": true, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": null, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 2, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"policy_01": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]], [[[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]], (3, 64, 64), float32)", "Discrete(7)", {}], "policy_02": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]], [[[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]], (3, 64, 64), float32)", "Discrete(7)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": "<function <lambda> at 0x7f1aa37470e0>", "policies_to_train": ["policy_01"], "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": 100000, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": false, "hiddens": [], "double_q": false, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": true, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": null, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 2, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"policy_01": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]], [[[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]], (3, 64, 64), float32)", "Discrete(7)", {}], "policy_02": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]], [[[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]], (3, 64, 64), float32)", "Discrete(7)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": "<function <lambda> at 0x7f1aa37470e0>", "policies_to_train": ["policy_01"], "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": 100000, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": false, "hiddens": [], "double_q": false, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "time_since_restore": 1272.5812397003174, "timesteps_since_restore": 15360, "iterations_since_restore": 60, "warmup_time": 45.46659183502197, "perf": {"cpu_util_percent": 24.851612903225803, "ram_util_percent": 13.993548387096775}}
{"episode_reward_max": 441.0, "episode_reward_min": 0.0, "episode_reward_mean": 24.09, "episode_len_mean": 601.0, "episode_media": {}, "episodes_this_iter": 2, "policy_reward_min": {"policy_01": 0.0, "policy_02": 0.0}, "policy_reward_max": {"policy_01": 441.0, "policy_02": 441.0}, "policy_reward_mean": {"policy_01": 10.86, "policy_02": 13.23}, "custom_metrics": {}, "hist_stats": {"episode_reward": [0.0, 204.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0], "episode_lengths": [601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601], "policy_policy_01_reward": [0.0, 204.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "policy_policy_02_reward": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.3346847680745239, "mean_inference_ms": 5.8007408226712665, "mean_action_processing_ms": 0.08884974381096221, "mean_env_wait_ms": 7.941914150983089, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 2, "timesteps_total": 61000, "timesteps_this_iter": 256, "agent_timesteps_total": 122000, "timers": {"load_time_ms": 1.3, "load_throughput": 196850.699, "learn_time_ms": 11.997, "learn_throughput": 21338.441, "update_time_ms": 2.334}, "info": {"learner": {"policy_01": {"custom_metrics": {}, "learner_stats": {"mean_q": 257.4609069824219, "min_q": 69.33770751953125, "max_q": 2145.07080078125, "cur_lr": 0.0005}, "model": {}, "td_error": [-0.08220672607421875, -0.6159515380859375, -0.2193603515625, -3.7324676513671875, -12.859283447265625, -58.541290283203125, 2.1488037109375, 0.0778045654296875, 74.71391296386719, -1.1360015869140625, -1.3239974975585938, 0.198883056640625, 22.60089111328125, -0.8385009765625, -0.36651611328125, -0.09728240966796875, -10.863494873046875, -0.4298553466796875, -0.5037841796875, -214.007568359375, 0.4012451171875, 9.358154296875, -0.00726318359375, -0.2971038818359375, -1.3680801391601562, -1.5305633544921875, -1.0674362182617188, -1.3420181274414062, -0.7913970947265625, -0.04924774169921875, 0.19585418701171875, -0.6602554321289062, 12.385688781738281, 66.6417236328125, -0.8714599609375, 30.333755493164062, -47.83697509765625, -0.071441650390625, 0.8386154174804688, -0.864166259765625, -214.007568359375, -1.0890655517578125, 0.6689910888671875, 19.532394409179688, 9.422523498535156, 64.58807373046875, -55.60919189453125, -0.7003631591796875, 13.58123779296875, -1.266754150390625, -0.46163177490234375, 6.088935852050781, -27.1849365234375, -126.835205078125, -3.896728515625, -2.8932876586914062, -0.471710205078125, -0.866912841796875, -1.1137161254882812, -0.527557373046875, 0.3091888427734375, -0.6719970703125, -1.2187423706054688, 0.47898101806640625, 0.03623199462890625, -1.5437164306640625, -1.36260986328125, 0.7048187255859375, -0.5063095092773438, -27.667236328125, -2.45513916015625, 27.9619140625, -1.2007827758789062, 8.882965087890625, -24.536216735839844, -0.7737350463867188, -1.7896652221679688, -0.06231689453125, 0.5455398559570312, -3.4286117553710938, -15.28497314453125, 8.015586853027344, -0.3998565673828125, -0.5936965942382812, 90.79022216796875, 5.0625152587890625, -0.8668060302734375, -0.8932418823242188, -0.2891693115234375, 354.3460388183594, 66.8604736328125, 9.693603515625, -0.75018310546875, -8.008697509765625, 0.5305328369140625, -0.57733154296875, 6.5860137939453125, -0.8084945678710938, 0.31690216064453125, 0.34766387939453125, 0.01806640625, -1.0472488403320312, -0.15581512451171875, 4.7386016845703125, 0.5296859741210938, -1.0125808715820312, 79.42489624023438, -0.554656982421875, 12.36297607421875, -3.447113037109375, -1.2630157470703125, -0.14604949951171875, -0.7911224365234375, 33.0889892578125, 0.4335784912109375, -0.9758529663085938, -0.15084075927734375, -0.853271484375, -0.1912689208984375, -1.201995849609375, 0.6896209716796875, 0.4322509765625, -2.2709884643554688, -26.8380126953125, -0.008636474609375, -0.96337890625, 1.1811752319335938, -1.3702926635742188, -0.43328094482421875, -0.4556121826171875, 0.1420745849609375, 0.04837799072265625, -33.0555419921875, 0.6994857788085938, -192.8243408203125, 0.4153289794921875, 75.2723388671875, -68.924072265625, -0.1482086181640625, 0.24925994873046875, 0.25699615478515625, -0.5833892822265625, -11.246955871582031, -0.6841506958007812, -0.6372528076171875, -78.9151611328125, 0.067474365234375, 0.7034988403320312, -0.131591796875, -1.97027587890625, -0.2769317626953125, 0.45706939697265625, 0.47589111328125, 0.32916259765625, -0.10828399658203125, -9.420883178710938, 0.6806640625, -0.3480987548828125, 74.96228790283203, 31.7607421875, -0.7646331787109375, -1.985504150390625, -0.16384124755859375, 41.4390869140625, -0.27646636962890625, -1.2259750366210938, -0.3654937744140625, -0.8532485961914062, 0.15032196044921875, -0.25232696533203125, 67.05337524414062, -1.5389480590820312, -1.1656265258789062, -69.8631591796875, 0.42449188232421875, 23.38372802734375, -0.20719146728515625, 0.412200927734375, 3.034149169921875, -0.8028793334960938, -0.290313720703125, -1.12384033203125, 4.32830810546875, 7.38372802734375, -0.8961181640625, 1.4604339599609375, -1.1158981323242188, 0.12891387939453125, -0.14983367919921875, 8.83404541015625, -70.2451171875, -0.22821807861328125, -2.0927581787109375, 0.5397415161132812, 0.7850723266601562, -119.6947021484375, 3.5585098266601562, 0.049835205078125, -0.8769378662109375, -0.1739959716796875, -0.17067718505859375, -0.29924774169921875, -1.33441162109375, -0.8394775390625, -2.1634979248046875, -16.16754150390625, 2.3898849487304688, -1.4214248657226562, 1.4673614501953125, 19.06097412109375, -1.0093460083007812, -0.9801712036132812, -0.6371383666992188, 21.05144500732422, -0.00726318359375, -0.7963409423828125, -0.24776458740234375, 0.4691009521484375, 0.021575927734375, 43.1407470703125, 21.87054443359375, -0.076568603515625, 0.7119827270507812, -7.345977783203125, -0.4904937744140625, 0.865264892578125, -1.91632080078125, 2.1097183227539062, -1.1511001586914062, 0.04752349853515625, -0.20177459716796875, 0.6159286499023438, -0.03614044189453125, 0.13367462158203125, -3.4313278198242188, -9.52154541015625, -1.0561141967773438, 1.6762924194335938, 0.66387939453125, 0.5300216674804688, 0.7425918579101562, -1.1098709106445312, -0.2377777099609375, 0.2496185302734375, 0.26462554931640625, 78.32148742675781, -0.1302337646484375, -1.2726593017578125, -0.8282089233398438, -1.3636016845703125, 18.37872314453125, 3.4399795532226562, -0.0769500732421875, -0.8304977416992188, 12.696395874023438, 46.9395751953125], "mean_td_error": 0.014032125473022461}}, "num_steps_sampled": 61000, "num_agent_steps_sampled": 122000, "num_steps_trained": 1920256, "num_steps_trained_this_iter": 256, "num_agent_steps_trained": 3840512, "last_target_update_ts": 60976, "num_target_updates": 120}, "done": false, "episodes_total": 100, "training_iteration": 61, "trial_id": "e21e6_00000", "experiment_id": "434cd42d33fd4b638f17fc69fa01d74f", "date": "2022-06-14_19-23-36", "timestamp": 1655249016, "time_this_iter_s": 21.857831716537476, "time_total_s": 1294.4390714168549, "pid": 2568314, "hostname": "lambda-dual", "node_ip": "10.104.51.19", "config": {"num_workers": 2, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.0005, "train_batch_size": 256, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "cnet", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "1v1env", "observation_space": null, "action_space": null, "env_config": {}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": true, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": null, "evaluation_duration": 10, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"num_workers": 2, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.0005, "train_batch_size": 256, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "cnet", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "1v1env", "observation_space": null, "action_space": null, "env_config": {}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": false, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": null, "evaluation_duration": 10, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"explore": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": true, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": null, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 2, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"policy_01": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]], [[[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]], (3, 64, 64), float32)", "Discrete(7)", {}], "policy_02": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]], [[[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]], (3, 64, 64), float32)", "Discrete(7)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": "<function <lambda> at 0x7f1aa3764320>", "policies_to_train": ["policy_01"], "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": 100000, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": false, "hiddens": [], "double_q": false, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": true, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": null, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 2, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"policy_01": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]], [[[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]], (3, 64, 64), float32)", "Discrete(7)", {}], "policy_02": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]], [[[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]], (3, 64, 64), float32)", "Discrete(7)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": "<function <lambda> at 0x7f1aa3764320>", "policies_to_train": ["policy_01"], "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": 100000, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": false, "hiddens": [], "double_q": false, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "time_since_restore": 1294.4390714168549, "timesteps_since_restore": 15616, "iterations_since_restore": 61, "warmup_time": 45.46659183502197, "perf": {"cpu_util_percent": 24.85483870967742, "ram_util_percent": 14.074193548387102}}
{"episode_reward_max": 441.0, "episode_reward_min": 0.0, "episode_reward_mean": 22.05, "episode_len_mean": 601.0, "episode_media": {}, "episodes_this_iter": 2, "policy_reward_min": {"policy_01": 0.0, "policy_02": 0.0}, "policy_reward_max": {"policy_01": 441.0, "policy_02": 441.0}, "policy_reward_mean": {"policy_01": 8.82, "policy_02": 13.23}, "custom_metrics": {}, "hist_stats": {"episode_reward": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0], "episode_lengths": [601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601], "policy_policy_01_reward": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "policy_policy_02_reward": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.3348024262926907, "mean_inference_ms": 5.798472155523536, "mean_action_processing_ms": 0.08885660315206105, "mean_env_wait_ms": 7.91707399752787, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 2, "timesteps_total": 62000, "timesteps_this_iter": 256, "agent_timesteps_total": 124000, "timers": {"load_time_ms": 1.356, "load_throughput": 188736.676, "learn_time_ms": 12.226, "learn_throughput": 20938.762, "update_time_ms": 2.525}, "info": {"learner": {"policy_01": {"custom_metrics": {}, "learner_stats": {"mean_q": 309.6852722167969, "min_q": 71.75218200683594, "max_q": 2180.74853515625, "cur_lr": 0.0005}, "model": {}, "td_error": [2.5109100341796875, 9.266273498535156, 2.93560791015625, 4.353424072265625, 3.488677978515625, 2.6275787353515625, 3.3872909545898438, 1.2626876831054688, -45.318603515625, 3.4892196655273438, 77.86141967773438, 1.892974853515625, 3.7686920166015625, 3.4641571044921875, 3.50885009765625, 5.898773193359375, 4.022315979003906, -29.9954833984375, 2.8526763916015625, 3.8849258422851562, 3.2247161865234375, 3.1944732666015625, 8.09912109375, 2.9086532592773438, -43.7476806640625, 3.5228347778320312, 3.9949874877929688, 2.0294418334960938, 2.8883056640625, 1.1656341552734375, 3.5529632568359375, 3.0227203369140625, 2.98187255859375, 14.940185546875, 3.1368637084960938, 0.9553451538085938, 1.5786285400390625, 4.11700439453125, 77.00750732421875, 8.72796630859375, 9.012451171875, -163.00048828125, 3.0982208251953125, 6.026824951171875, 2.796875, 9.565345764160156, 2.8538894653320312, 80.3214111328125, 4.261436462402344, 85.7564697265625, 3.4677276611328125, 2.8895034790039062, 4.5872955322265625, 2.8092575073242188, 53.72923278808594, 6.011505126953125, 4.701515197753906, 4.269187927246094, 2.8822021484375, 3.649261474609375, 3.9630889892578125, -26.23193359375, 2.3515167236328125, 3.2404861450195312, 2.97259521484375, 1.8575897216796875, 1.3475875854492188, 3.4300994873046875, 2.760528564453125, 2.29095458984375, 3.12530517578125, 3.40008544921875, 3.7649993896484375, 3.4710617065429688, 4.019371032714844, 3.8780288696289062, 2.502593994140625, -118.6531982421875, 91.2779541015625, -27.5625, 2.9366531372070312, 4.346343994140625, 7.204071044921875, 2.8859710693359375, 3.607086181640625, 3.0204086303710938, 7.513702392578125, 4.7469024658203125, -55.066162109375, -137.048583984375, 3.6954574584960938, 5.152549743652344, -76.604736328125, -156.7803955078125, 3.4274978637695312, -1.66448974609375, 4.571563720703125, -184.70179748535156, 3.6703567504882812, 0.9093475341796875, 4.137153625488281, -1.1378173828125, 481.0610656738281, 6.112358093261719, 2.4527587890625, -30.57000732421875, 3.8591690063476562, 3.4181442260742188, 4.322715759277344, -3.86590576171875, 2.7964019775390625, 3.1990509033203125, -67.5621337890625, 2.9305496215820312, 1.951995849609375, 4.112113952636719, 3.719696044921875, 4.08099365234375, -84.1702880859375, -0.5350341796875, 61.58161926269531, 3.3217544555664062, 2.8690109252929688, 1.6186065673828125, -1.5999755859375, 3.423797607421875, 3.543121337890625, 2.828521728515625, 12.630615234375, 4.18280029296875, 3.2690200805664062, 2.8657073974609375, 3.2534942626953125, 3.8584442138671875, 3.9773712158203125, 4.108329772949219, 3.6687545776367188, 2.583892822265625, 2.3551483154296875, 1.9900054931640625, 4.035980224609375, 3.3492431640625, 3.1076126098632812, 4.107231140136719, 2.663970947265625, 46.12115478515625, 2.890838623046875, 4.0814361572265625, 0.9775009155273438, 0.6246414184570312, 2.2516632080078125, 3.0624237060546875, 3.649078369140625, 5.022331237792969, 7.9188690185546875, 2.70416259765625, -62.29291534423828, 2.8048629760742188, 2.8524246215820312, 13.2738037109375, 74.5110092163086, 2.976837158203125, 2.9755630493164062, -2.5976028442382812, 2.701202392578125, 3.4263458251953125, 2.8981246948242188, 3.4207077026367188, 1.559295654296875, 2.8981704711914062, 2.938995361328125, 2.7788543701171875, 4.654594421386719, 6.769325256347656, 3.809173583984375, 6.01190185546875, -63.338134765625, 3.4722900390625, -12.7528076171875, 2.64361572265625, 3.4932479858398438, 6.916007995605469, -120.37548828125, 2.9073104858398438, 4.0447235107421875, -9.11376953125, 2.8242034912109375, 1.8271865844726562, 3.0409698486328125, 4.733184814453125, 2.87646484375, 4.6987152099609375, 3.6104888916015625, 2.998077392578125, 3.8537445068359375, 3.5847702026367188, 2.647979736328125, 2.7154083251953125, -90.36468505859375, 2.881072998046875, 3.8362808227539062, 3.6107254028320312, 1.6067276000976562, 4.832466125488281, 3.3914947509765625, 3.6058731079101562, -23.9591064453125, 3.6306381225585938, 5.262542724609375, 79.49864959716797, 5.968048095703125, 3.1583328247070312, 15.29443359375, 12.351654052734375, 3.2344436645507812, 3.3041534423828125, 4.1746063232421875, 5.6128997802734375, 3.5658416748046875, 3.45916748046875, 2.90997314453125, -12.074951171875, 1.82293701171875, 3.9079132080078125, -22.2625732421875, 1.4776611328125, 4.189506530761719, 3.330657958984375, 1.3760299682617188, 2.308929443359375, -63.796630859375, -0.8273086547851562, 5.97869873046875, 79.10839080810547, -0.6508255004882812, 5.747772216796875, 1816.5284423828125, 2.9672775268554688, 3.4657821655273438, 1.0313491821289062, 3.0593490600585938, 91.3734130859375, 3.2514877319335938, 1.5613937377929688, 60.991943359375, 73.64825439453125, 3.7590103149414062, -148.94384765625, 3.4230728149414062, 5.6965484619140625, 0.768310546875, -2.03515625, 3.2769622802734375, 2.2986831665039062, 3.9694671630859375, 5.993927001953125], "mean_td_error": 8.674450874328613}}, "num_steps_sampled": 62000, "num_agent_steps_sampled": 124000, "num_steps_trained": 1952256, "num_steps_trained_this_iter": 256, "num_agent_steps_trained": 3904512, "last_target_update_ts": 61984, "num_target_updates": 122}, "done": false, "episodes_total": 102, "training_iteration": 62, "trial_id": "e21e6_00000", "experiment_id": "434cd42d33fd4b638f17fc69fa01d74f", "date": "2022-06-14_19-23-58", "timestamp": 1655249038, "time_this_iter_s": 21.71773362159729, "time_total_s": 1316.1568050384521, "pid": 2568314, "hostname": "lambda-dual", "node_ip": "10.104.51.19", "config": {"num_workers": 2, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.0005, "train_batch_size": 256, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "cnet", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "1v1env", "observation_space": null, "action_space": null, "env_config": {}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": true, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": null, "evaluation_duration": 10, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"num_workers": 2, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.0005, "train_batch_size": 256, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "cnet", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "1v1env", "observation_space": null, "action_space": null, "env_config": {}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": false, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": null, "evaluation_duration": 10, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"explore": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": true, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": null, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 2, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"policy_01": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]], [[[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]], (3, 64, 64), float32)", "Discrete(7)", {}], "policy_02": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]], [[[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]], (3, 64, 64), float32)", "Discrete(7)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": "<function <lambda> at 0x7f1aa3764d40>", "policies_to_train": ["policy_01"], "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": 100000, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": false, "hiddens": [], "double_q": false, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": true, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": null, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 2, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"policy_01": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]], [[[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]], (3, 64, 64), float32)", "Discrete(7)", {}], "policy_02": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]], [[[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]], (3, 64, 64), float32)", "Discrete(7)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": "<function <lambda> at 0x7f1aa3764d40>", "policies_to_train": ["policy_01"], "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": 100000, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": false, "hiddens": [], "double_q": false, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "time_since_restore": 1316.1568050384521, "timesteps_since_restore": 15872, "iterations_since_restore": 62, "warmup_time": 45.46659183502197, "perf": {"cpu_util_percent": 24.899999999999995, "ram_util_percent": 14.141935483870965}}
{"episode_reward_max": 441.0, "episode_reward_min": 0.0, "episode_reward_mean": 22.05, "episode_len_mean": 601.0, "episode_media": {}, "episodes_this_iter": 2, "policy_reward_min": {"policy_01": 0.0, "policy_02": 0.0}, "policy_reward_max": {"policy_01": 441.0, "policy_02": 441.0}, "policy_reward_mean": {"policy_01": 8.82, "policy_02": 13.23}, "custom_metrics": {}, "hist_stats": {"episode_reward": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0], "episode_lengths": [601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601], "policy_policy_01_reward": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "policy_policy_02_reward": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.3346427877221297, "mean_inference_ms": 5.795341752168189, "mean_action_processing_ms": 0.08883732066320955, "mean_env_wait_ms": 7.8967485743658345, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 2, "timesteps_total": 63000, "timesteps_this_iter": 256, "agent_timesteps_total": 126000, "timers": {"load_time_ms": 1.332, "load_throughput": 192189.197, "learn_time_ms": 12.264, "learn_throughput": 20874.243, "update_time_ms": 2.485}, "info": {"learner": {"policy_01": {"custom_metrics": {}, "learner_stats": {"mean_q": 281.3974609375, "min_q": 71.44883728027344, "max_q": 2174.0146484375, "cur_lr": 0.0005}, "model": {}, "td_error": [-14.90667724609375, 0.1790924072265625, -0.7565460205078125, -114.457275390625, 1.9913101196289062, 75.9669418334961, -0.60321044921875, -1.6296539306640625, 2.180999755859375, 2.5185012817382812, -1.3873062133789062, -0.45238494873046875, -1.5990753173828125, 0.2352294921875, 4.0816650390625, 0.768524169921875, -2.8580169677734375, 0.7809295654296875, 2.112640380859375, 0.49973297119140625, -0.33453369140625, -0.623931884765625, -1.725799560546875, 76.19371795654297, 9.884681701660156, 0.06963348388671875, -0.5954132080078125, -137.9537353515625, 0.7980499267578125, -0.314727783203125, 1.5523300170898438, 0.3494720458984375, 3.05975341796875, -16.04388427734375, 0.9177093505859375, -16.9486083984375, -20.503265380859375, -1.8681411743164062, -2.2064056396484375, -1.8090744018554688, -0.13034820556640625, 8.6519775390625, 0.6636199951171875, 74.30057525634766, -40.573974609375, -0.297332763671875, -1.8870849609375, 0.772064208984375, 0.0344390869140625, -0.9613189697265625, 0.03975677490234375, -43.53515625, 1.9033203125, -0.26192474365234375, 0.6097946166992188, -4.186744689941406, -1.6856536865234375, 0.2579803466796875, 8.41497802734375, 0.1022491455078125, -1.2042465209960938, 1.4475555419921875, 0.15880584716796875, -0.7069931030273438, -108.78701782226562, -0.1953277587890625, -2.1343841552734375, -2.6774673461914062, -4.365898132324219, -0.41584014892578125, -1.4821014404296875, -1.340057373046875, -0.45337677001953125, -0.6253585815429688, 0.7264862060546875, -1.166748046875, -2.6892166137695312, -50.294097900390625, -2.3499221801757812, 0.33768463134765625, -30.693923950195312, 0.78192138671875, -0.8234634399414062, 0.720123291015625, -1.72869873046875, 123.827880859375, -0.5034255981445312, -0.6752548217773438, 0.371063232421875, -0.8980331420898438, -1.378875732421875, -1.9039459228515625, 0.7651596069335938, 1.4316253662109375, 0.6623916625976562, -2.3385391235351562, -0.6330642700195312, 17.325927734375, -0.2296600341796875, 0.05931854248046875, 0.6188125610351562, -0.217559814453125, 0.7810745239257812, -0.10538482666015625, -1.8537826538085938, -0.7936782836914062, -1.2282257080078125, 77.51990509033203, -2.400146484375, -1.1926727294921875, -0.0371551513671875, -0.39430999755859375, -0.28411865234375, -3.201629638671875, -0.81298828125, -1.2712554931640625, 0.49819183349609375, 7.0454559326171875, -0.7392196655273438, 0.5178298950195312, 3.6705322265625, -0.2971343994140625, -2.171722412109375, 75.93877410888672, -12.35931396484375, -26.0242919921875, -2.9138412475585938, 0.75335693359375, -3.996429443359375, 10.0986328125, 78.7191162109375, 0.28563690185546875, -0.7994308471679688, -42.529205322265625, -2.6709747314453125, 0.2996063232421875, 0.5409469604492188, -1.111236572265625, 0.7727813720703125, -1.3907470703125, -1.1231231689453125, -0.35919189453125, 0.48954010009765625, 1.439971923828125, 0.44146728515625, -24.260345458984375, -1.4890594482421875, 0.4505615234375, -2.3763046264648438, -2.09075927734375, -0.7203521728515625, 0.13359832763671875, 74.13924407958984, -75.4625244140625, -0.32940673828125, -1.0392303466796875, -0.5132980346679688, -0.6561431884765625, -1.315032958984375, -0.22132110595703125, -0.44602203369140625, -1.8774032592773438, -0.5431289672851562, -21.36492919921875, 0.7857284545898438, -20.53765869140625, -20.73858642578125, -1.0789642333984375, -1.8626327514648438, -23.421630859375, -3.536163330078125, -1.088287353515625, 0.29186248779296875, -0.26263427734375, -0.44826507568359375, 0.7750244140625, -0.8236541748046875, -0.8010482788085938, -10.63812255859375, -3.354461669921875, -1.6351242065429688, -251.8233642578125, -0.029998779296875, -1.9919662475585938, 0.1222076416015625, 0.785247802734375, 0.2196502685546875, 1.481353759765625, -0.5431289672851562, 0.32342529296875, -1.5145721435546875, -2.7840652465820312, -1.1944427490234375, -0.46588134765625, -31.5147705078125, 48.62060546875, -1.9527969360351562, -150.30859375, 78.17344665527344, 0.5165328979492188, -0.40384674072265625, 0.21562957763671875, -1.8440704345703125, 0.6373825073242188, -2.3372955322265625, -0.59393310546875, -2.4550704956054688, -2.115234375, -8.14404296875, -12.35931396484375, 28.3092041015625, -1.0450439453125, -0.9246826171875, 0.3338775634765625, -1.4660186767578125, -1.6241989135742188, 0.2426910400390625, -109.31005859375, 0.1287994384765625, 0.792510986328125, -1.1281356811523438, -2.8779067993164062, 2.997802734375, 0.4384918212890625, 3.6705322265625, -0.01703643798828125, -72.2713623046875, 0.562408447265625, -144.16796875, -0.18791961669921875, -2.73822021484375, -0.7666015625, -2.598907470703125, 0.23053741455078125, -41.292510986328125, -1.4255752563476562, -2.4592819213867188, 74.8564453125, -0.7538604736328125, -4.153831481933594, -119.3011474609375, 1.107757568359375, -1.5119552612304688, 0.1222076416015625, -2.831146240234375, -15.056427001953125, 0.6305007934570312, -1.7427291870117188, 1.1642074584960938, 2.4858932495117188, 0.483123779296875, -3.2375411987304688, -0.534423828125, 0.7636337280273438, 79.32350158691406, -0.39605712890625], "mean_td_error": -3.546661853790283}}, "num_steps_sampled": 63000, "num_agent_steps_sampled": 126000, "num_steps_trained": 1984256, "num_steps_trained_this_iter": 256, "num_agent_steps_trained": 3968512, "last_target_update_ts": 62992, "num_target_updates": 124}, "done": false, "episodes_total": 104, "training_iteration": 63, "trial_id": "e21e6_00000", "experiment_id": "434cd42d33fd4b638f17fc69fa01d74f", "date": "2022-06-14_19-24-20", "timestamp": 1655249060, "time_this_iter_s": 21.949179649353027, "time_total_s": 1338.1059846878052, "pid": 2568314, "hostname": "lambda-dual", "node_ip": "10.104.51.19", "config": {"num_workers": 2, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.0005, "train_batch_size": 256, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "cnet", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "1v1env", "observation_space": null, "action_space": null, "env_config": {}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": true, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": null, "evaluation_duration": 10, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"num_workers": 2, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.0005, "train_batch_size": 256, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "cnet", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "1v1env", "observation_space": null, "action_space": null, "env_config": {}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": false, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": null, "evaluation_duration": 10, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"explore": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": true, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": null, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 2, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"policy_01": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]], [[[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]], (3, 64, 64), float32)", "Discrete(7)", {}], "policy_02": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]], [[[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]], (3, 64, 64), float32)", "Discrete(7)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": "<function <lambda> at 0x7f1aa375ec20>", "policies_to_train": ["policy_01"], "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": 100000, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": false, "hiddens": [], "double_q": false, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": true, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": null, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 2, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"policy_01": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]], [[[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]], (3, 64, 64), float32)", "Discrete(7)", {}], "policy_02": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]], [[[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]], (3, 64, 64), float32)", "Discrete(7)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": "<function <lambda> at 0x7f1aa375ec20>", "policies_to_train": ["policy_01"], "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": 100000, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": false, "hiddens": [], "double_q": false, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "time_since_restore": 1338.1059846878052, "timesteps_since_restore": 16128, "iterations_since_restore": 63, "warmup_time": 45.46659183502197, "perf": {"cpu_util_percent": 24.86875, "ram_util_percent": 14.215625}}
{"episode_reward_max": 441.0, "episode_reward_min": 0.0, "episode_reward_mean": 22.05, "episode_len_mean": 601.0, "episode_media": {}, "episodes_this_iter": 2, "policy_reward_min": {"policy_01": 0.0, "policy_02": 0.0}, "policy_reward_max": {"policy_01": 441.0, "policy_02": 441.0}, "policy_reward_mean": {"policy_01": 8.82, "policy_02": 13.23}, "custom_metrics": {}, "hist_stats": {"episode_reward": [0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "episode_lengths": [601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601], "policy_policy_01_reward": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "policy_policy_02_reward": [0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.3343572191989408, "mean_inference_ms": 5.792380699800122, "mean_action_processing_ms": 0.08881603636392237, "mean_env_wait_ms": 7.8797480468161485, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 2, "timesteps_total": 64000, "timesteps_this_iter": 256, "agent_timesteps_total": 128000, "timers": {"load_time_ms": 1.32, "load_throughput": 193949.247, "learn_time_ms": 11.831, "learn_throughput": 21637.158, "update_time_ms": 2.262}, "info": {"learner": {"policy_01": {"custom_metrics": {}, "learner_stats": {"mean_q": 248.89605712890625, "min_q": 73.27436828613281, "max_q": 2311.18310546875, "cur_lr": 0.0005}, "model": {}, "td_error": [-1.8826141357421875, 2.13885498046875, -1.2750015258789062, 21.914642333984375, 5.442138671875, 76.80083465576172, 1.29144287109375, 0.41826629638671875, 8.917938232421875, 0.39066314697265625, -2.541748046875, 0.37496185302734375, 5.519134521484375, 0.443359375, 322.94049072265625, 25.79656982421875, 0.5374984741210938, 1.330596923828125, 0.7801361083984375, -1.36126708984375, 0.2226715087890625, -0.8011627197265625, 8.62725830078125, -0.257293701171875, 0.7920150756835938, 0.5535202026367188, -0.32794952392578125, -0.97686767578125, 0.4435882568359375, -1.2168502807617188, 7.34698486328125, 0.0751495361328125, 1.2439498901367188, 1.8127288818359375, -0.08663177490234375, 0.6176910400390625, 2.0957794189453125, -1.1713104248046875, 0.956634521484375, -27.477294921875, 77.18584442138672, 9.879058837890625, 0.3871612548828125, 0.327972412109375, 18.672119140625, 1.036041259765625, 0.0460357666015625, 1.274566650390625, 2.831573486328125, -0.38672637939453125, -0.663665771484375, 1.56512451171875, -0.8206329345703125, 0.176300048828125, 2.1332778930664062, -0.5839996337890625, 31.2039794921875, -1.1316070556640625, 0.6511077880859375, 0.8662948608398438, 0.17786407470703125, 0.7656936645507812, -1.2819900512695312, -2.3696746826171875, 0.38525390625, 0.8157196044921875, 0.1074371337890625, -0.6857681274414062, -0.135986328125, 1.0143966674804688, 64.3050537109375, -1.272308349609375, 2.4767532348632812, 1.20257568359375, 0.800201416015625, 0.5441818237304688, -6.72412109375, 0.6720352172851562, 2425.95166015625, 0.4398193359375, 1.799560546875, 78.12216186523438, 0.10424041748046875, -0.436737060546875, 86.6912841796875, -1.9150848388671875, -0.36846923828125, -1.21649169921875, -4.86822509765625, 35.64447021484375, 75.61298370361328, -0.436737060546875, 0.4600982666015625, 0.188262939453125, 1.4864273071289062, 80.42682647705078, -0.298431396484375, -0.1104736328125, 1.1935272216796875, 0.7426223754882812, 37.286376953125, 0.21569061279296875, 0.30977630615234375, 3.23291015625, -3.1948776245117188, 0.804443359375, 0.37833404541015625, -0.28748321533203125, -10.78643798828125, 35.92620849609375, 1.0049514770507812, 0.49645233154296875, 0.9483413696289062, -1.702880859375, -71.102294921875, 1.7044677734375, -0.08005523681640625, 10.207420349121094, -3.2137374877929688, -0.16131591796875, -0.16684722900390625, 0.0386810302734375, 1.3865814208984375, -0.8492660522460938, 7.9715576171875, 2.0247879028320312, -0.5689010620117188, -15.0625, 0.0653533935546875, -0.14844512939453125, 0.37044525146484375, 56.761962890625, 0.23474884033203125, -0.3864593505859375, 64.3050537109375, 0.310638427734375, 0.879150390625, 0.95257568359375, -0.0354461669921875, 77.83900451660156, 2.2410659790039062, -0.7674713134765625, 2.83160400390625, 0.5377273559570312, 1.8305511474609375, 0.37835693359375, -0.42385101318359375, 3.1992874145507812, 0.6365966796875, -0.8580322265625, 0.6833343505859375, -0.02342987060546875, 2.78125, -0.41326141357421875, 0.41419219970703125, 32.62821960449219, -0.5781784057617188, 0.2078399658203125, 0.324432373046875, 1.0632781982421875, 0.5706787109375, -61.5062255859375, 1.2134628295898438, 0.097900390625, -24.89990234375, 0.285308837890625, 0.0706329345703125, 0.4803009033203125, 1.3297271728515625, 0.0449066162109375, -0.479583740234375, 0.43320465087890625, -1.7271347045898438, 0.2470245361328125, 78.85203552246094, -7.192222595214844, 0.84063720703125, 27.2672119140625, -0.0889129638671875, 0.08914947509765625, 0.4738311767578125, 0.4563140869140625, 0.1134796142578125, 0.2641448974609375, 0.518402099609375, 9.31024169921875, -5.622314453125, 1.7947463989257812, 0.8544921875, 76.56645202636719, 2.393463134765625, 29.0416259765625, 0.03667449951171875, 0.11861419677734375, -0.35869598388671875, -1.68560791015625, 1.839752197265625, 10.736846923828125, 0.9691238403320312, -0.8424606323242188, -0.6113433837890625, -0.9872665405273438, 0.8326187133789062, 2.16046142578125, -1.0610733032226562, -0.0182037353515625, 0.5909652709960938, 3.7543716430664062, 78.62677001953125, 0.5807266235351562, -0.12293243408203125, 1.5049896240234375, 76.62178039550781, -0.012725830078125, 112.159423828125, -0.106964111328125, -0.178375244140625, -1.2534027099609375, -0.7187423706054688, -2.343292236328125, 0.8830184936523438, 2.0317153930664062, 1.6694564819335938, 0.69256591796875, -0.9705963134765625, 0.6409225463867188, 6.99560546875, 0.85028076171875, 30.66326904296875, 0.13288116455078125, -0.6973953247070312, -3.4221038818359375, 6.628509521484375, 0.42293548583984375, 0.8166122436523438, 1.0582046508789062, 0.478851318359375, -2.6187896728515625, 12.08856201171875, 0.7623291015625, 0.508056640625, 9.39208984375, -0.173614501953125, 0.6545639038085938, -0.225860595703125, -50.4251708984375, -0.28021240234375, -0.5895462036132812, 1.5681228637695312, 27.2672119140625, 4.888786315917969, 0.599395751953125, 0.1153411865234375, 0.7137680053710938, 1.4161605834960938, 1.1735000610351562], "mean_td_error": 16.241199493408203}}, "num_steps_sampled": 64000, "num_agent_steps_sampled": 128000, "num_steps_trained": 2016256, "num_steps_trained_this_iter": 256, "num_agent_steps_trained": 4032512, "last_target_update_ts": 64000, "num_target_updates": 126}, "done": false, "episodes_total": 106, "training_iteration": 64, "trial_id": "e21e6_00000", "experiment_id": "434cd42d33fd4b638f17fc69fa01d74f", "date": "2022-06-14_19-24-42", "timestamp": 1655249082, "time_this_iter_s": 22.10756230354309, "time_total_s": 1360.2135469913483, "pid": 2568314, "hostname": "lambda-dual", "node_ip": "10.104.51.19", "config": {"num_workers": 2, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.0005, "train_batch_size": 256, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "cnet", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "1v1env", "observation_space": null, "action_space": null, "env_config": {}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": true, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": null, "evaluation_duration": 10, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"num_workers": 2, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.0005, "train_batch_size": 256, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "cnet", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "1v1env", "observation_space": null, "action_space": null, "env_config": {}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": false, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": null, "evaluation_duration": 10, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"explore": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": true, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": null, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 2, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"policy_01": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]], [[[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]], (3, 64, 64), float32)", "Discrete(7)", {}], "policy_02": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]], [[[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]], (3, 64, 64), float32)", "Discrete(7)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": "<function <lambda> at 0x7f1aa375e7a0>", "policies_to_train": ["policy_01"], "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": 100000, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": false, "hiddens": [], "double_q": false, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": true, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": null, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 2, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"policy_01": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]], [[[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]], (3, 64, 64), float32)", "Discrete(7)", {}], "policy_02": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]], [[[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]], (3, 64, 64), float32)", "Discrete(7)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": "<function <lambda> at 0x7f1aa375e7a0>", "policies_to_train": ["policy_01"], "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": 100000, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": false, "hiddens": [], "double_q": false, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "time_since_restore": 1360.2135469913483, "timesteps_since_restore": 16384, "iterations_since_restore": 64, "warmup_time": 45.46659183502197, "perf": {"cpu_util_percent": 24.970967741935485, "ram_util_percent": 14.306451612903228}}
{"episode_reward_max": 441.0, "episode_reward_min": 0.0, "episode_reward_mean": 22.05, "episode_len_mean": 601.0, "episode_media": {}, "episodes_this_iter": 2, "policy_reward_min": {"policy_01": 0.0, "policy_02": 0.0}, "policy_reward_max": {"policy_01": 441.0, "policy_02": 441.0}, "policy_reward_mean": {"policy_01": 8.82, "policy_02": 13.23}, "custom_metrics": {}, "hist_stats": {"episode_reward": [0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "episode_lengths": [601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601], "policy_policy_01_reward": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "policy_policy_02_reward": [0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.3340536795897713, "mean_inference_ms": 5.789736688916263, "mean_action_processing_ms": 0.08878935920391182, "mean_env_wait_ms": 7.8658233377864715, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 2, "timesteps_total": 65000, "timesteps_this_iter": 256, "agent_timesteps_total": 130000, "timers": {"load_time_ms": 1.353, "load_throughput": 189198.94, "learn_time_ms": 12.239, "learn_throughput": 20917.551, "update_time_ms": 2.503}, "info": {"learner": {"policy_01": {"custom_metrics": {}, "learner_stats": {"mean_q": 223.8628387451172, "min_q": 69.70457458496094, "max_q": 2127.49365234375, "cur_lr": 0.0005}, "model": {}, "td_error": [-48.500732421875, -1.0616607666015625, -2.4148406982421875, -0.21270751953125, -1.8179397583007812, -2.001007080078125, -6.67144775390625, -2.230987548828125, -0.36699676513671875, -1.7980804443359375, -0.3570404052734375, 2.2281951904296875, -0.8542938232421875, 0.4011993408203125, -1.9978561401367188, -0.12613677978515625, -1.36041259765625, 0.27947235107421875, -6.537506103515625, -3.8347396850585938, 3.179229736328125, -1.58502197265625, -0.34310150146484375, -0.8045654296875, -0.19931793212890625, -3.292022705078125, -7.511810302734375, 82.14825439453125, -0.06424713134765625, -1.5003204345703125, -1.4364547729492188, -0.35832977294921875, -0.22873687744140625, 0.17653656005859375, -2.4367446899414062, 1.592681884765625, -0.15155792236328125, -0.586761474609375, -1.2625198364257812, -36.31103515625, 0.5876541137695312, -0.9746170043945312, -1.1776123046875, -0.4159698486328125, 0.508819580078125, -1.9387969970703125, -1.7771682739257812, -0.8580474853515625, -0.49347686767578125, -1.5033035278320312, -0.7634963989257812, -13.112503051757812, -1.3136672973632812, -0.8627548217773438, -1.1231307983398438, -0.17206573486328125, 51.340576171875, 0.391082763671875, -0.9274368286132812, -11.85504150390625, 75.87427520751953, -1.0363922119140625, -0.195159912109375, 3.499908447265625, -0.8246002197265625, -1.9474029541015625, -1.2975692749023438, 38.30126953125, -1.0368118286132812, -2.7346343994140625, 0.02977752685546875, -0.459136962890625, -0.48235321044921875, -19.85358428955078, 0.2392730712890625, 0.06536102294921875, -1.1305465698242188, -0.8754959106445312, -1.2930526733398438, 7.23291015625, -0.9581222534179688, -45.108642578125, -0.8746795654296875, -2.1753616333007812, 3.06195068359375, 0.3326416015625, -15.664581298828125, -0.414154052734375, -1.7195281982421875, 0.255615234375, -8.081382751464844, -1.8636474609375, 64.37969970703125, -1.4567413330078125, 2.59283447265625, -0.8183975219726562, -0.9357070922851562, 4.604034423828125, -0.32163238525390625, -1.3469161987304688, 1.4976043701171875, -1.750823974609375, -1.0143508911132812, 0.11508941650390625, -0.6392669677734375, -0.0794219970703125, -0.23175811767578125, -0.7859649658203125, -0.8257217407226562, 53.20947265625, -9.62286376953125, -1.336578369140625, -1.6342086791992188, -0.9390640258789062, 76.4810791015625, -20.65966796875, 328.1154479980469, 38.30126953125, 14.7696533203125, -0.02515411376953125, -2.1266326904296875, -1.7334823608398438, 0.04465484619140625, 0.58087158203125, 0.41761016845703125, 80.18098449707031, -3.49578857421875, -1.2143707275390625, -1.4334793090820312, -0.8789901733398438, 0.151824951171875, -1.23004150390625, -4.293617248535156, -2.56231689453125, -1.9820709228515625, -3.686553955078125, -75.31298828125, -0.44622802734375, -6.7215576171875, -0.8245697021484375, 7.192085266113281, 0.0914459228515625, -0.66845703125, 371.7003479003906, 0.9036483764648438, 74.33509826660156, -0.255950927734375, 0.39893341064453125, -173.9232177734375, -1.5326919555664062, 0.3214111328125, 73.78401947021484, 27.238311767578125, -1.033172607421875, -0.7990264892578125, -22.58831787109375, 0.513916015625, 0.7676620483398438, -1.9556427001953125, -0.23757171630859375, 0.396759033203125, -0.44049072265625, 75.81824493408203, -2.7742156982421875, -2.2837600708007812, -0.6786041259765625, -0.928741455078125, 1.10308837890625, -7.57879638671875, -1.8284378051757812, -0.9048004150390625, -3.4573287963867188, -2.8849411010742188, -0.7091064453125, 0.398468017578125, -1.5201797485351562, -0.7700271606445312, 0.161590576171875, -23.3173828125, -0.6905364990234375, -3.41937255859375, -3.67584228515625, -0.26103973388671875, 0.08123016357421875, -0.886077880859375, 0.16254425048828125, 5.221221923828125, 0.10713958740234375, -32.793304443359375, -1.5022430419921875, -1.7401275634765625, -2.98150634765625, -0.7308120727539062, -2.5164260864257812, 3.857696533203125, -0.03514862060546875, -173.9232177734375, -0.30149078369140625, -0.6392669677734375, -2.26336669921875, -1.1410903930664062, 0.2127685546875, -1.4836959838867188, 10.69146728515625, -0.41104888916015625, -1.142608642578125, 30.208969116210938, -0.7309417724609375, -4.1370697021484375, -2.4860610961914062, -1.437835693359375, -144.1505126953125, -0.726593017578125, -0.7231597900390625, 0.4885406494140625, -1.6152114868164062, -0.6786651611328125, -12.229705810546875, 0.6158294677734375, 0.29978179931640625, -0.7737579345703125, 0.0019378662109375, -1.5866317749023438, 48.6375732421875, -1.2393112182617188, -0.29103851318359375, -1.6427154541015625, -1.5487518310546875, -0.765960693359375, -0.7055130004882812, -4.329795837402344, -0.76629638671875, 0.567535400390625, -2.08984375, -22.91729736328125, -0.7052154541015625, -46.3909912109375, 4.683502197265625, -1.7917556762695312, -1.3631591796875, 0.87652587890625, -1.1010360717773438, -2.34222412109375, -3.45166015625, -1.1862030029296875, -0.5244827270507812, -0.8527679443359375, 77.7483139038086, -0.49250030517578125, -1.3178329467773438, -2.0633392333984375, -0.6108551025390625, -0.922210693359375, -0.6558456420898438, -1.7983016967773438, -0.955657958984375], "mean_td_error": 2.160815715789795}}, "num_steps_sampled": 65000, "num_agent_steps_sampled": 130000, "num_steps_trained": 2048256, "num_steps_trained_this_iter": 256, "num_agent_steps_trained": 4096512, "last_target_update_ts": 64504, "num_target_updates": 127}, "done": false, "episodes_total": 108, "training_iteration": 65, "trial_id": "e21e6_00000", "experiment_id": "434cd42d33fd4b638f17fc69fa01d74f", "date": "2022-06-14_19-25-04", "timestamp": 1655249104, "time_this_iter_s": 22.28496789932251, "time_total_s": 1382.4985148906708, "pid": 2568314, "hostname": "lambda-dual", "node_ip": "10.104.51.19", "config": {"num_workers": 2, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.0005, "train_batch_size": 256, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "cnet", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "1v1env", "observation_space": null, "action_space": null, "env_config": {}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": true, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": null, "evaluation_duration": 10, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"num_workers": 2, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.0005, "train_batch_size": 256, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "cnet", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "1v1env", "observation_space": null, "action_space": null, "env_config": {}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": false, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": null, "evaluation_duration": 10, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"explore": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": true, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": null, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 2, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"policy_01": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]], [[[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]], (3, 64, 64), float32)", "Discrete(7)", {}], "policy_02": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]], [[[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]], (3, 64, 64), float32)", "Discrete(7)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": "<function <lambda> at 0x7f1aa3700680>", "policies_to_train": ["policy_01"], "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": 100000, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": false, "hiddens": [], "double_q": false, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": true, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": null, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 2, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"policy_01": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]], [[[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]], (3, 64, 64), float32)", "Discrete(7)", {}], "policy_02": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]], [[[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]], (3, 64, 64), float32)", "Discrete(7)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": "<function <lambda> at 0x7f1aa3700680>", "policies_to_train": ["policy_01"], "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": 100000, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": false, "hiddens": [], "double_q": false, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "time_since_restore": 1382.4985148906708, "timesteps_since_restore": 16640, "iterations_since_restore": 65, "warmup_time": 45.46659183502197, "perf": {"cpu_util_percent": 25.037499999999998, "ram_util_percent": 14.4}}
{"episode_reward_max": 441.0, "episode_reward_min": 0.0, "episode_reward_mean": 22.05, "episode_len_mean": 601.0, "episode_media": {}, "episodes_this_iter": 0, "policy_reward_min": {"policy_01": 0.0, "policy_02": 0.0}, "policy_reward_max": {"policy_01": 441.0, "policy_02": 441.0}, "policy_reward_mean": {"policy_01": 8.82, "policy_02": 13.23}, "custom_metrics": {}, "hist_stats": {"episode_reward": [0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "episode_lengths": [601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601], "policy_policy_01_reward": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "policy_policy_02_reward": [0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.3340536795897713, "mean_inference_ms": 5.789736688916263, "mean_action_processing_ms": 0.08878935920391182, "mean_env_wait_ms": 7.8658233377864715, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 2, "timesteps_total": 66000, "timesteps_this_iter": 256, "agent_timesteps_total": 132000, "timers": {"load_time_ms": 1.313, "load_throughput": 195041.383, "learn_time_ms": 11.871, "learn_throughput": 21564.762, "update_time_ms": 2.357}, "info": {"learner": {"policy_01": {"custom_metrics": {}, "learner_stats": {"mean_q": 214.7493896484375, "min_q": 77.50434112548828, "max_q": 2404.54833984375, "cur_lr": 0.0005}, "model": {}, "td_error": [-0.097259521484375, 10.575729370117188, -3.023956298828125, -104.28456115722656, 0.7684783935546875, 0.6951751708984375, -0.7128753662109375, 3.852691650390625, 20.2105712890625, -4.152496337890625, 0.06695556640625, 3.4529647827148438, -0.34555816650390625, 19.99700927734375, -0.75201416015625, 3.858612060546875, 1.3566360473632812, -48.39857482910156, 8.374130249023438, 24.646728515625, 0.56475830078125, 1.0282440185546875, -4.5524139404296875, 0.52569580078125, -0.539581298828125, 0.7954559326171875, 0.393463134765625, -0.4833984375, -0.8718185424804688, 81.29975128173828, -3.475189208984375, 0.6399154663085938, -2.9048080444335938, -0.0944671630859375, 0.56927490234375, 7.869232177734375, 1.0372772216796875, -11.494720458984375, -1.5773086547851562, 78.61228942871094, 1.3867034912109375, -1.5104751586914062, 1.0692062377929688, 0.129669189453125, -0.05467987060546875, 2.10565185546875, 1.4563980102539062, 0.4833221435546875, -0.42990875244140625, -0.3936309814453125, -0.23105621337890625, -0.6288986206054688, -0.02523040771484375, 96.6212158203125, -0.6835250854492188, 2.3979873657226562, 0.1594696044921875, -85.269287109375, 1.15545654296875, 3.81085205078125, 0.4485626220703125, 0.9079360961914062, 84.16802215576172, 0.15064239501953125, -0.132110595703125, -2.1238555908203125, 0.256134033203125, -0.29250335693359375, -1.186309814453125, 0.7716827392578125, -0.492218017578125, -3.083740234375, 1.6433486938476562, -21.25701904296875, 1.0137176513671875, 0.9158401489257812, 0.17209625244140625, -104.28456115722656, -1.4133071899414062, -61.95416259765625, -0.10530853271484375, -1.8333663940429688, 2521.67626953125, 0.33040618896484375, 0.9434356689453125, -0.41413116455078125, -0.2138519287109375, 140.42327880859375, -3.1239166259765625, 4.594322204589844, -0.717071533203125, 1.4599151611328125, -0.6477127075195312, 4.263397216796875, 0.30741119384765625, -1.4064102172851562, 0.35379791259765625, -1.578155517578125, -0.9822006225585938, -15.584014892578125, -0.2251739501953125, -0.4681396484375, -2.36627197265625, 0.17730712890625, -0.7260055541992188, -0.5317764282226562, -0.8888092041015625, 0.2381591796875, 1.0606842041015625, -0.6757354736328125, 1.0176925659179688, -0.34613037109375, -1.31939697265625, -0.2033538818359375, -7.854522705078125, -1.689605712890625, -2.20526123046875, 1.1536407470703125, -0.12497711181640625, 0.443023681640625, -2.442718505859375, 2.2324676513671875, -0.2772064208984375, -13.72857666015625, 0.9215240478515625, -1.4051437377929688, -0.08190155029296875, -2.655853271484375, 0.40445709228515625, 78.12405395507812, 0.7306900024414062, 7.989494323730469, 5.510887145996094, 1.8118896484375, -1.6385269165039062, 2.7841339111328125, -4.51690673828125, -43.8818359375, 0.33467864990234375, -0.6219024658203125, 0.6889495849609375, -0.45450592041015625, 2.6266708374023438, -1.2776260375976562, 10.145919799804688, 4.569488525390625, -1.9488677978515625, -0.131072998046875, 1.119873046875, -0.6757125854492188, 0.7001953125, -4.508575439453125, -1.1963882446289062, 0.01676177978515625, 0.4101104736328125, 78.75039672851562, -1.4914932250976562, -0.2833251953125, 28.859893798828125, 1.3451156616210938, -0.3642425537109375, -4.6109619140625, 4.1943817138671875, -1.6105728149414062, 0.12067413330078125, 2.4939422607421875, -0.11249542236328125, 0.27138519287109375, 2.5082626342773438, 0.4244537353515625, 3.298614501953125, -1.690277099609375, 0.630096435546875, 1.384185791015625, 0.47846221923828125, -2.446563720703125, -0.544952392578125, 40.60107421875, 0.1485443115234375, 0.749359130859375, 1.50762939453125, -1.0513229370117188, 7.04815673828125, 1.02728271484375, -42.9229736328125, 1.201019287109375, -2.8154449462890625, -7.584228515625, 0.6108932495117188, 0.06330108642578125, -1.8960647583007812, 11.7760009765625, -1.8509521484375, -1.8541641235351562, -6.21124267578125, -0.498779296875, -0.42195892333984375, 0.05028533935546875, 185.9930419921875, 0.15526580810546875, 0.17950439453125, 0.14650726318359375, 2.8270263671875, 0.38362884521484375, 0.6807479858398438, -0.5949859619140625, 78.298828125, 1.3020248413085938, -1.8797836303710938, 0.941497802734375, -8.772010803222656, 0.27782440185546875, 0.204833984375, -9.072334289550781, 0.2274169921875, 0.9233627319335938, 1.0793991088867188, 1.1512603759765625, 1.6946258544921875, 10.10491943359375, -7.854522705078125, -0.665374755859375, -2.368621826171875, -0.08086395263671875, 0.8629531860351562, -1.1275177001953125, -1.6677169799804688, -1.6842193603515625, -0.24282073974609375, 1.3184738159179688, -4.2366943359375, 0.7419204711914062, 78.83284759521484, 0.15468597412109375, -1.5521163940429688, -0.3117523193359375, 0.379486083984375, 0.11087799072265625, 1.3596649169921875, 0.2984619140625, 1.26251220703125, -31.93670654296875, -2.0295639038085938, -1.7988128662109375, -0.9173660278320312, 1.3473052978515625, -0.382049560546875, 0.6919937133789062, 0.9158401489257812, 16.644561767578125, -66.26123046875, 1.1327896118164062, 0.0128173828125, -0.764892578125, 1.1058197021484375, 0.0844879150390625], "mean_td_error": 11.828149795532227}}, "num_steps_sampled": 66000, "num_agent_steps_sampled": 132000, "num_steps_trained": 2080256, "num_steps_trained_this_iter": 256, "num_agent_steps_trained": 4160512, "last_target_update_ts": 65512, "num_target_updates": 129}, "done": false, "episodes_total": 108, "training_iteration": 66, "trial_id": "e21e6_00000", "experiment_id": "434cd42d33fd4b638f17fc69fa01d74f", "date": "2022-06-14_19-25-26", "timestamp": 1655249126, "time_this_iter_s": 22.239515781402588, "time_total_s": 1404.7380306720734, "pid": 2568314, "hostname": "lambda-dual", "node_ip": "10.104.51.19", "config": {"num_workers": 2, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.0005, "train_batch_size": 256, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "cnet", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "1v1env", "observation_space": null, "action_space": null, "env_config": {}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": true, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": null, "evaluation_duration": 10, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"num_workers": 2, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.0005, "train_batch_size": 256, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "cnet", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "1v1env", "observation_space": null, "action_space": null, "env_config": {}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": false, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": null, "evaluation_duration": 10, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"explore": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": true, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": null, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 2, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"policy_01": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]], [[[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]], (3, 64, 64), float32)", "Discrete(7)", {}], "policy_02": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]], [[[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]], (3, 64, 64), float32)", "Discrete(7)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": "<function <lambda> at 0x7f1aa375ed40>", "policies_to_train": ["policy_01"], "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": 100000, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": false, "hiddens": [], "double_q": false, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": true, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": null, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 2, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"policy_01": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]], [[[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]], (3, 64, 64), float32)", "Discrete(7)", {}], "policy_02": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]], [[[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]], (3, 64, 64), float32)", "Discrete(7)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": "<function <lambda> at 0x7f1aa375ed40>", "policies_to_train": ["policy_01"], "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": 100000, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": false, "hiddens": [], "double_q": false, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "time_since_restore": 1404.7380306720734, "timesteps_since_restore": 16896, "iterations_since_restore": 66, "warmup_time": 45.46659183502197, "perf": {"cpu_util_percent": 25.8375, "ram_util_percent": 14.4875}}
{"episode_reward_max": 441.0, "episode_reward_min": 0.0, "episode_reward_mean": 22.05, "episode_len_mean": 601.0, "episode_media": {}, "episodes_this_iter": 2, "policy_reward_min": {"policy_01": 0.0, "policy_02": 0.0}, "policy_reward_max": {"policy_01": 441.0, "policy_02": 441.0}, "policy_reward_mean": {"policy_01": 8.82, "policy_02": 13.23}, "custom_metrics": {}, "hist_stats": {"episode_reward": [0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "episode_lengths": [601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601], "policy_policy_01_reward": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "policy_policy_02_reward": [0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.33399214544144384, "mean_inference_ms": 5.7877627699067435, "mean_action_processing_ms": 0.08876719655462667, "mean_env_wait_ms": 7.8557227119303334, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 2, "timesteps_total": 67000, "timesteps_this_iter": 256, "agent_timesteps_total": 134000, "timers": {"load_time_ms": 1.334, "load_throughput": 191859.524, "learn_time_ms": 11.872, "learn_throughput": 21562.856, "update_time_ms": 2.291}, "info": {"learner": {"policy_01": {"custom_metrics": {}, "learner_stats": {"mean_q": 291.95831298828125, "min_q": 76.81512451171875, "max_q": 2857.736328125, "cur_lr": 0.0005}, "model": {}, "td_error": [10.314010620117188, 0.7157821655273438, -0.11740875244140625, -0.4372711181640625, 0.9735946655273438, 1.1720199584960938, 1.0128173828125, 0.2212677001953125, -0.6792068481445312, 2.8700408935546875, 1.003692626953125, 0.7219772338867188, 0.5139923095703125, 1.7446060180664062, 0.6876449584960938, -1.814605712890625, 165.9322509765625, 1.7887115478515625, 2.3311996459960938, 194.36181640625, -1.6555557250976562, -49.431243896484375, -0.1823272705078125, -28.4822998046875, -0.09500885009765625, 1.31390380859375, 3.492584228515625, 0.056060791015625, -5.2950286865234375, 0.1810302734375, 1.2651138305664062, 0.7701873779296875, 80.81869506835938, -0.2193145751953125, 55.4415283203125, 1.0479583740234375, 0.17107391357421875, 2.2269363403320312, -0.05371856689453125, -1.0086517333984375, -1.534576416015625, 3.725067138671875, 1.2993087768554688, 0.69805908203125, 0.6754837036132812, 0.2250518798828125, 36.3651123046875, 0.122589111328125, 9.105278015136719, 127.82080078125, 0.243682861328125, -0.6584930419921875, 1.4488449096679688, 1.7730255126953125, 0.3036041259765625, 6.5535888671875, 0.7127151489257812, -0.683013916015625, -0.4404449462890625, -0.090484619140625, 13.77532958984375, -137.85137939453125, 0.7958831787109375, -1.4139251708984375, -16.534820556640625, -0.36646270751953125, 29.864013671875, 0.7354507446289062, 80.51981353759766, 0.4637603759765625, 5.472984313964844, 39.144287109375, 55.85333251953125, 0.6494979858398438, 4.822334289550781, 0.2736358642578125, 1.78076171875, 0.8828811645507812, 0.14173126220703125, -0.572540283203125, -0.01811981201171875, 1.4061203002929688, 25.752273559570312, 1.3627243041992188, 0.02193450927734375, 1.0424652099609375, 17.0860595703125, 1.2936019897460938, -0.262908935546875, 26.5712890625, 1.6584548950195312, 0.6220169067382812, 4.771331787109375, -1.353485107421875, -11.072265625, -5.6715545654296875, 0.30487060546875, 1.0220794677734375, -7.656242370605469, 2.058441162109375, 14.03900146484375, 2.6043930053710938, -1.5703125, -2.6609115600585938, -0.5193099975585938, 5.28167724609375, 0.7081527709960938, 0.1552734375, 1.5185775756835938, 80.07190704345703, 0.69891357421875, -6.657135009765625, 32.780517578125, -0.926177978515625, 0.43126678466796875, -40.787872314453125, 2.2067947387695312, 0.42071533203125, 0.8158721923828125, -0.995574951171875, 0.07791900634765625, 0.9543991088867188, -1.1494064331054688, -42.81895446777344, 0.262664794921875, 0.19525146484375, 2.0620574951171875, -1.2021102905273438, 0.09002685546875, 0.4784698486328125, 0.7613067626953125, -1.4062652587890625, -3.3278579711914062, 3.2015151977539062, -12.947067260742188, 1.7296295166015625, 381.2213134765625, 0.07759857177734375, 0.9646530151367188, 1.22198486328125, -1.0462570190429688, -1.2968063354492188, -0.43456268310546875, -0.36418914794921875, 60.33935546875, 1.733062744140625, 80.62814331054688, 0.9376754760742188, 54.56695556640625, 8.613426208496094, -0.99615478515625, 0.06977081298828125, 1.1506805419921875, -0.9536209106445312, 0.02042388916015625, 0.8900146484375, 2.8611984252929688, 4.754661560058594, 2.5327606201171875, -137.85137939453125, 0.6533126831054688, -1.134552001953125, 241.19873046875, 62.581298828125, 1.0183944702148438, 0.3625640869140625, -1.6265869140625, -0.37935638427734375, -2.8677215576171875, -0.3204498291015625, 0.8897781372070312, -7.656242370605469, 2.3120498657226562, 1.1906280517578125, -1.6932830810546875, -1.385009765625, 0.5610504150390625, 0.21533966064453125, 1.4236373901367188, -0.8814773559570312, 1.1045074462890625, 189.169189453125, -1.288787841796875, 0.22803497314453125, -6.92291259765625, 0.14199066162109375, 53.5960693359375, 1.802001953125, -14.9752197265625, -18.92510986328125, 1.1029205322265625, 0.2313232421875, -0.002044677734375, 44.310791015625, 0.7331314086914062, -0.6007080078125, 0.430816650390625, 2.0879974365234375, 2.4495620727539062, 51.79541015625, 1.535614013671875, -1.7279434204101562, 1.3597030639648438, 1.5625076293945312, -1.3291549682617188, 1.22186279296875, 2.507476806640625, 29.724838256835938, 12.3599853515625, -0.4372711181640625, 0.24279022216796875, -0.9211502075195312, 0.3997344970703125, 82.11009216308594, 0.6664581298828125, -0.43874359130859375, 148.47412109375, 1.459716796875, -0.45584869384765625, -4.06671142578125, 0.38457489013671875, 2.3663558959960938, 1.7949981689453125, -0.07196044921875, 0.2520599365234375, 26.525390625, 0.0672607421875, 61.23089599609375, 1.2845077514648438, 3.8535995483398438, 0.0866241455078125, -1.628204345703125, 1.2267303466796875, 93.27001953125, -0.45584869384765625, 1.2641677856445312, 84.27366638183594, -0.7681045532226562, -0.0330963134765625, 1.1996002197265625, 50.9415283203125, 2.70147705078125, -1.50006103515625, -0.7059555053710938, 1.892547607421875, 0.7514266967773438, 2.638824462890625, 4.092369079589844, 1.466522216796875, 0.2341156005859375, 0.8779983520507812, 1.458740234375, 0.25580596923828125, 0.03893280029296875, 1.3585968017578125, -27.2158203125], "mean_td_error": 9.584091186523438}}, "num_steps_sampled": 67000, "num_agent_steps_sampled": 134000, "num_steps_trained": 2112256, "num_steps_trained_this_iter": 256, "num_agent_steps_trained": 4224512, "last_target_update_ts": 66520, "num_target_updates": 131}, "done": false, "episodes_total": 110, "training_iteration": 67, "trial_id": "e21e6_00000", "experiment_id": "434cd42d33fd4b638f17fc69fa01d74f", "date": "2022-06-14_19-25-49", "timestamp": 1655249149, "time_this_iter_s": 22.013945817947388, "time_total_s": 1426.7519764900208, "pid": 2568314, "hostname": "lambda-dual", "node_ip": "10.104.51.19", "config": {"num_workers": 2, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.0005, "train_batch_size": 256, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "cnet", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "1v1env", "observation_space": null, "action_space": null, "env_config": {}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": true, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": null, "evaluation_duration": 10, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"num_workers": 2, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.0005, "train_batch_size": 256, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "cnet", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "1v1env", "observation_space": null, "action_space": null, "env_config": {}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": false, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": null, "evaluation_duration": 10, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"explore": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": true, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": null, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 2, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"policy_01": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]], [[[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]], (3, 64, 64), float32)", "Discrete(7)", {}], "policy_02": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]], [[[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]], (3, 64, 64), float32)", "Discrete(7)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": "<function <lambda> at 0x7f1aa3700710>", "policies_to_train": ["policy_01"], "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": 100000, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": false, "hiddens": [], "double_q": false, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": true, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": null, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 2, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"policy_01": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]], [[[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]], (3, 64, 64), float32)", "Discrete(7)", {}], "policy_02": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]], [[[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]], (3, 64, 64), float32)", "Discrete(7)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": "<function <lambda> at 0x7f1aa3700710>", "policies_to_train": ["policy_01"], "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": 100000, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": false, "hiddens": [], "double_q": false, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "time_since_restore": 1426.7519764900208, "timesteps_since_restore": 17152, "iterations_since_restore": 67, "warmup_time": 45.46659183502197, "perf": {"cpu_util_percent": 26.567741935483877, "ram_util_percent": 14.570967741935489}}
{"episode_reward_max": 441.0, "episode_reward_min": 0.0, "episode_reward_mean": 17.64, "episode_len_mean": 601.0, "episode_media": {}, "episodes_this_iter": 2, "policy_reward_min": {"policy_01": 0.0, "policy_02": 0.0}, "policy_reward_max": {"policy_01": 441.0, "policy_02": 441.0}, "policy_reward_mean": {"policy_01": 8.82, "policy_02": 8.82}, "custom_metrics": {}, "hist_stats": {"episode_reward": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "episode_lengths": [601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601], "policy_policy_01_reward": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "policy_policy_02_reward": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.33389258936899835, "mean_inference_ms": 5.78574155892365, "mean_action_processing_ms": 0.08874553011682669, "mean_env_wait_ms": 7.846682268854414, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 2, "timesteps_total": 68000, "timesteps_this_iter": 256, "agent_timesteps_total": 136000, "timers": {"load_time_ms": 1.314, "load_throughput": 194846.721, "learn_time_ms": 11.636, "learn_throughput": 21999.841, "update_time_ms": 2.21}, "info": {"learner": {"policy_01": {"custom_metrics": {}, "learner_stats": {"mean_q": 246.7194061279297, "min_q": 74.23129272460938, "max_q": 2928.28662109375, "cur_lr": 0.0005}, "model": {}, "td_error": [-0.5540237426757812, -3.6070098876953125, -1.4946365356445312, -1.5682601928710938, -0.9790725708007812, -5.75927734375, -1.115692138671875, 8.0579833984375, -0.126190185546875, -20.704742431640625, -0.0679168701171875, 119.593505859375, -1.5414199829101562, -2.1170883178710938, -2.28118896484375, -3.4401397705078125, -0.43132781982421875, -0.0463714599609375, 46.09649658203125, -0.735198974609375, -4.5055084228515625, 0.0702056884765625, -0.7188644409179688, -1.7027587890625, -0.37651824951171875, 1.560302734375, -1.5772857666015625, 3.47613525390625, -1.8790206909179688, -1.5514678955078125, -2.0527267456054688, -2.9112014770507812, 0.1644744873046875, 2.13177490234375, -1.0909194946289062, -0.2991180419921875, -1.0612564086914062, -2.5062332153320312, -9.8289794921875, -0.7463836669921875, -1.457763671875, -1.8546371459960938, -3.3774032592773438, -0.143157958984375, -1.7333450317382812, -1.3246994018554688, 11.156837463378906, 82.5206527709961, -1.4827117919921875, -11.929878234863281, -2.7238082885742188, -2.4377822875976562, 0.12259674072265625, -0.7288360595703125, 81.12724304199219, -0.5881729125976562, -1.8375167846679688, -1.3601150512695312, -1.1655960083007812, 1.20025634765625, 2.9189453125, -43.723602294921875, -1.019012451171875, -1.032806396484375, -1.211029052734375, -1.6114959716796875, 0.7759246826171875, -0.8611068725585938, -0.7207489013671875, -0.3188323974609375, -0.32065582275390625, -1.8179168701171875, -1.8218917846679688, -0.872772216796875, -0.8710556030273438, -2.1056594848632812, -0.6566696166992188, -2.29083251953125, 1.3490371704101562, -2.4245147705078125, -3.6363525390625, 0.20227813720703125, -1.1530532836914062, -1.8382797241210938, -0.30628204345703125, -1.4857025146484375, -2.5501327514648438, -0.6521148681640625, -0.6191253662109375, -15.723274230957031, 56.44561767578125, 0.3289031982421875, 30.24285888671875, -1.4230728149414062, -1.8056182861328125, -2.0137710571289062, 7.56341552734375, 1.7618026733398438, -25.162925720214844, -1.9470291137695312, -4.045166015625, -1.5741119384765625, -1.7311172485351562, -0.42140960693359375, -101.444580078125, 3.728179931640625, 0.082489013671875, -9.4979248046875, -1.9189071655273438, -2.4477081298828125, -1.9400634765625, -1.3076324462890625, 0.09409332275390625, 0.22454833984375, -1.5264511108398438, -3.8442916870117188, -2.1260757446289062, -1.6327285766601562, -1.9870834350585938, -1.6644134521484375, -0.4927215576171875, -2.1341781616210938, 5.81146240234375, -2.3758392333984375, 495.94439697265625, -3.3698501586914062, -3.5933837890625, -2.0904922485351562, -6.332977294921875, -2.17437744140625, 0.6251068115234375, 3.6761932373046875, -0.9077606201171875, -11.841888427734375, -3.093841552734375, -1.7224349975585938, -4.356224060058594, -9.3563232421875, -2.9252090454101562, -3.3128738403320312, 2.369873046875, -1.0402450561523438, 4.662193298339844, 79.50199890136719, -2.6242294311523438, -0.705780029296875, 1.096435546875, -4.947601318359375, -2.0515899658203125, 9.86767578125, -2.5157623291015625, -0.721038818359375, -1.7171630859375, -2.7192459106445312, -1.0956878662109375, 9.997550964355469, -30.135833740234375, 15.020416259765625, -1.23919677734375, -2.1600494384765625, -2.3618392944335938, 0.758544921875, -2.0609664916992188, -1.1097946166992188, 4.78814697265625, -1.1210708618164062, -0.8198623657226562, -1.661651611328125, -1.804656982421875, 40.593017578125, -5.041557312011719, 77.16300964355469, 6.1116943359375, -2.9473419189453125, 28.22503662109375, -3.4036483764648438, -0.667724609375, -5.994087219238281, 2.6795730590820312, -8.24896240234375, 1.1867523193359375, 82.40062713623047, 0.089813232421875, -0.26129150390625, -2.30426025390625, -2.2708740234375, -0.9970855712890625, -1.7723922729492188, 45.466552734375, 3.8344879150390625, 61.462738037109375, -0.161865234375, -0.5637435913085938, 18.365699768066406, 0.9153823852539062, -3.2430496215820312, -2.558563232421875, 34.6956787109375, -1.4509048461914062, -1.9387435913085938, 28.95703125, -1.9189071655273438, -37.457763671875, -0.44188690185546875, -2.2129974365234375, -1.089691162109375, -35.7940673828125, -1.22265625, 14.354644775390625, -1.7688827514648438, -1.4741058349609375, -67.37353515625, -2.4959259033203125, -4.5055084228515625, -0.6057968139648438, -1.560821533203125, -0.76092529296875, 1.511444091796875, -2.13079833984375, -0.04335784912109375, -2.1231002807617188, -1.5444717407226562, -0.576568603515625, 17.932373046875, -0.9227523803710938, -1.4657058715820312, -1.6148147583007812, -4.73516845703125, -36.190185546875, 0.25600433349609375, -29.313751220703125, -0.2458648681640625, -1.275665283203125, -2.751953125, -1.8636245727539062, 1.8454971313476562, -2.6147079467773438, -1.60552978515625, -32.85791015625, 2.115325927734375, -0.265777587890625, -0.7509994506835938, -1.6679840087890625, 25.80364990234375, -2.8066864013671875, -2.4488754272460938, 55.13690185546875, -1.775146484375, -0.20378875732421875, -0.7036819458007812, -1.4517593383789062, -6.11785888671875, -2.9502716064453125, -0.6224746704101562, -1.4832000732421875, -2.8427047729492188], "mean_td_error": 3.095386505126953}}, "num_steps_sampled": 68000, "num_agent_steps_sampled": 136000, "num_steps_trained": 2144256, "num_steps_trained_this_iter": 256, "num_agent_steps_trained": 4288512, "last_target_update_ts": 67528, "num_target_updates": 133}, "done": false, "episodes_total": 112, "training_iteration": 68, "trial_id": "e21e6_00000", "experiment_id": "434cd42d33fd4b638f17fc69fa01d74f", "date": "2022-06-14_19-26-11", "timestamp": 1655249171, "time_this_iter_s": 22.296679496765137, "time_total_s": 1449.048655986786, "pid": 2568314, "hostname": "lambda-dual", "node_ip": "10.104.51.19", "config": {"num_workers": 2, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.0005, "train_batch_size": 256, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "cnet", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "1v1env", "observation_space": null, "action_space": null, "env_config": {}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": true, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": null, "evaluation_duration": 10, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"num_workers": 2, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.0005, "train_batch_size": 256, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "cnet", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "1v1env", "observation_space": null, "action_space": null, "env_config": {}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": false, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": null, "evaluation_duration": 10, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"explore": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": true, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": null, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 2, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"policy_01": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]], [[[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]], (3, 64, 64), float32)", "Discrete(7)", {}], "policy_02": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]], [[[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]], (3, 64, 64), float32)", "Discrete(7)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": "<function <lambda> at 0x7f1aa37008c0>", "policies_to_train": ["policy_01"], "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": 100000, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": false, "hiddens": [], "double_q": false, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": true, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": null, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 2, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"policy_01": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]], [[[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]], (3, 64, 64), float32)", "Discrete(7)", {}], "policy_02": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]], [[[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]], (3, 64, 64), float32)", "Discrete(7)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": "<function <lambda> at 0x7f1aa37008c0>", "policies_to_train": ["policy_01"], "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": 100000, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": false, "hiddens": [], "double_q": false, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "time_since_restore": 1449.048655986786, "timesteps_since_restore": 17408, "iterations_since_restore": 68, "warmup_time": 45.46659183502197, "perf": {"cpu_util_percent": 25.75, "ram_util_percent": 14.706249999999999}}
{"episode_reward_max": 441.0, "episode_reward_min": 0.0, "episode_reward_mean": 17.64, "episode_len_mean": 601.0, "episode_media": {}, "episodes_this_iter": 2, "policy_reward_min": {"policy_01": 0.0, "policy_02": 0.0}, "policy_reward_max": {"policy_01": 441.0, "policy_02": 441.0}, "policy_reward_mean": {"policy_01": 8.82, "policy_02": 8.82}, "custom_metrics": {}, "hist_stats": {"episode_reward": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "episode_lengths": [601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601], "policy_policy_01_reward": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "policy_policy_02_reward": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.3337790426930814, "mean_inference_ms": 5.783710104064089, "mean_action_processing_ms": 0.08872336526436804, "mean_env_wait_ms": 7.8384802766573625, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 2, "timesteps_total": 69000, "timesteps_this_iter": 256, "agent_timesteps_total": 138000, "timers": {"load_time_ms": 1.336, "load_throughput": 191619.849, "learn_time_ms": 12.178, "learn_throughput": 21020.705, "update_time_ms": 2.484}, "info": {"learner": {"policy_01": {"custom_metrics": {}, "learner_stats": {"mean_q": 329.59075927734375, "min_q": 77.80979919433594, "max_q": 2800.1669921875, "cur_lr": 0.0005}, "model": {}, "td_error": [-1.0950546264648438, 0.5797119140625, -10.155517578125, -1.7203826904296875, -1.5641708374023438, 0.9387969970703125, -1.1359176635742188, 1.3417510986328125, -1.5171737670898438, -0.02565765380859375, -0.41504669189453125, 6.8655242919921875, 0.37273406982421875, -1.38623046875, -2.283447265625, -0.5307846069335938, -50.09504699707031, -0.0331268310546875, 1.5691299438476562, -1.1389083862304688, -8.518325805664062, -0.593017578125, -1.1555023193359375, 1.0264739990234375, -5.4139404296875, -1.0531539916992188, 0.09627532958984375, 0.0059051513671875, 0.24898529052734375, 0.7354507446289062, 1.2166213989257812, -0.1258544921875, 6.65667724609375, -1.09515380859375, 0.083282470703125, -0.415863037109375, -0.9914627075195312, 11.733367919921875, -3.848541259765625, -14.357421875, 1.6212310791015625, -51.15107727050781, -1.9078903198242188, -23.5196533203125, 0.5424118041992188, -3.3221435546875, 0.1187591552734375, 0.1553497314453125, -0.1077728271484375, -0.39300537109375, 0.16640472412109375, -0.00867462158203125, 2.0261688232421875, 1.3776092529296875, -1.36541748046875, 0.5355682373046875, 2.187957763671875, -1.0571746826171875, 0.7966232299804688, -29.47119140625, 0.8976669311523438, -4.314208984375, 0.5425872802734375, 2.7762680053710938, 0.38953399658203125, -1.056243896484375, -0.85614013671875, -31.05615234375, -1.6345291137695312, -149.253662109375, 0.8377532958984375, -1.02142333984375, -1.3746414184570312, 0.7561111450195312, -4.0203857421875, 0.18276214599609375, -17.546722412109375, 1.0806350708007812, 5.225830078125, 1.3024368286132812, 1.567779541015625, -0.5618133544921875, -0.48146820068359375, 0.6123580932617188, -0.30828094482421875, 0.41300201416015625, 81.62622833251953, 0.575958251953125, 0.428436279296875, 18.9449462890625, -0.46329498291015625, 27.253173828125, -19.65594482421875, -0.21184539794921875, -3.2417221069335938, 0.4229583740234375, -1.0374374389648438, 1.6572494506835938, -1.02142333984375, 0.501129150390625, -0.14878082275390625, 0.25766754150390625, -0.1107635498046875, -5.6253662109375, 2.329681396484375, -1.0648269653320312, 0.5040435791015625, -1.3786468505859375, 1.5753250122070312, 0.5031967163085938, -1.1470565795898438, -0.6390151977539062, 79.77420806884766, 2.0427169799804688, -0.30716705322265625, 35.63568115234375, 28.158538818359375, 4.385368347167969, 1.1934890747070312, 1.27294921875, -7.9953155517578125, -0.25962066650390625, 0.6995468139648438, -1.09014892578125, -0.36345672607421875, -1.199859619140625, -2.7440872192382812, 0.4190826416015625, 1.384185791015625, -20.57134246826172, -2.475067138671875, -33.1552734375, 2.2995223999023438, -7.821014404296875, -1.341583251953125, -0.5099258422851562, 84.59375, -1.7470169067382812, -2.4213409423828125, 1.6937789916992188, 8.878662109375, 0.6139602661132812, 14.024826049804688, 38.585205078125, 36.642578125, 0.465576171875, -0.14907073974609375, 0.5505294799804688, 10.390350341796875, -1.9111251831054688, -0.97760009765625, 0.6650009155273438, -0.5466537475585938, -1.779693603515625, 1.9637832641601562, 0.5777130126953125, -0.24112701416015625, 14.117477416992188, -0.3822784423828125, -4.448974609375, -38.07763671875, -0.2538604736328125, -28.615966796875, 0.18845367431640625, 1.265045166015625, 1.3989410400390625, -15.6541748046875, -18.7381591796875, 0.40749359130859375, -28.55517578125, 0.239654541015625, -0.30883026123046875, -18.347320556640625, -2.3554763793945312, -0.6364822387695312, -1.1150741577148438, -1.01898193359375, -0.19397735595703125, 1.294647216796875, 0.32477569580078125, 0.9723358154296875, 0.0620880126953125, 2.215728759765625, 1.7521514892578125, -1.3121337890625, 0.1369171142578125, 36.43115234375, 134.85302734375, -1.6839828491210938, -0.0392608642578125, -0.23722076416015625, 1.9035797119140625, -18.687255859375, 0.726226806640625, 2.0011367797851562, -1.2792434692382812, 0.190032958984375, -1.7066116333007812, 1.5991439819335938, 1.9458465576171875, 0.4264984130859375, -16.453842163085938, 31.16119384765625, -0.4051361083984375, -0.31398773193359375, 0.0194091796875, -0.953369140625, 2.150360107421875, 1.2827987670898438, 13.455047607421875, -0.9054489135742188, -55.7833251953125, -0.289276123046875, -2.7464981079101562, -0.5532913208007812, -13.18359375, 0.5737991333007812, 0.42488861083984375, 1.3759002685546875, 6.0332794189453125, -2.6462020874023438, -1.7277069091796875, -0.4729766845703125, -1.0997161865234375, 0.15259552001953125, 1.5991439819335938, -0.46649169921875, 0.2414093017578125, -0.9547195434570312, 1.9578170776367188, -1.1643295288085938, -0.08274078369140625, -0.5944976806640625, 21.5155029296875, 2.411895751953125, -5.630859375, 1.085235595703125, -1.8012771606445312, -11.941093444824219, -85.111572265625, -0.36994171142578125, 0.233154296875, -9.43109130859375, 80.06147003173828, 34.511962890625, 1.6276473999023438, -6.082000732421875, 2.3197021484375, 1.90777587890625, -1.5569305419921875, -3.8819503784179688, 82.092529296875, -0.09175872802734375, 2.6729812622070312, 0.6443634033203125, -1.5647201538085938], "mean_td_error": 0.2942279577255249}}, "num_steps_sampled": 69000, "num_agent_steps_sampled": 138000, "num_steps_trained": 2176256, "num_steps_trained_this_iter": 256, "num_agent_steps_trained": 4352512, "last_target_update_ts": 68536, "num_target_updates": 135}, "done": false, "episodes_total": 114, "training_iteration": 69, "trial_id": "e21e6_00000", "experiment_id": "434cd42d33fd4b638f17fc69fa01d74f", "date": "2022-06-14_19-26-33", "timestamp": 1655249193, "time_this_iter_s": 21.832963228225708, "time_total_s": 1470.8816192150116, "pid": 2568314, "hostname": "lambda-dual", "node_ip": "10.104.51.19", "config": {"num_workers": 2, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.0005, "train_batch_size": 256, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "cnet", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "1v1env", "observation_space": null, "action_space": null, "env_config": {}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": true, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": null, "evaluation_duration": 10, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"num_workers": 2, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.0005, "train_batch_size": 256, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "cnet", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "1v1env", "observation_space": null, "action_space": null, "env_config": {}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": false, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": null, "evaluation_duration": 10, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"explore": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": true, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": null, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 2, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"policy_01": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]], [[[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]], (3, 64, 64), float32)", "Discrete(7)", {}], "policy_02": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]], [[[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]], (3, 64, 64), float32)", "Discrete(7)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": "<function <lambda> at 0x7f1aa2ec4e60>", "policies_to_train": ["policy_01"], "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": 100000, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": false, "hiddens": [], "double_q": false, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": true, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": null, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 2, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"policy_01": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]], [[[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]], (3, 64, 64), float32)", "Discrete(7)", {}], "policy_02": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]], [[[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]], (3, 64, 64), float32)", "Discrete(7)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": "<function <lambda> at 0x7f1aa2ec4e60>", "policies_to_train": ["policy_01"], "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": 100000, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": false, "hiddens": [], "double_q": false, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "time_since_restore": 1470.8816192150116, "timesteps_since_restore": 17664, "iterations_since_restore": 69, "warmup_time": 45.46659183502197, "perf": {"cpu_util_percent": 26.912903225806453, "ram_util_percent": 14.964516129032255}}
{"episode_reward_max": 441.0, "episode_reward_min": 0.0, "episode_reward_mean": 17.64, "episode_len_mean": 601.0, "episode_media": {}, "episodes_this_iter": 2, "policy_reward_min": {"policy_01": 0.0, "policy_02": 0.0}, "policy_reward_max": {"policy_01": 441.0, "policy_02": 441.0}, "policy_reward_mean": {"policy_01": 8.82, "policy_02": 8.82}, "custom_metrics": {}, "hist_stats": {"episode_reward": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "episode_lengths": [601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601], "policy_policy_01_reward": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "policy_policy_02_reward": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.33364188500155834, "mean_inference_ms": 5.781772797260474, "mean_action_processing_ms": 0.08870050598248339, "mean_env_wait_ms": 7.831027930593373, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 2, "timesteps_total": 70000, "timesteps_this_iter": 256, "agent_timesteps_total": 140000, "timers": {"load_time_ms": 1.382, "load_throughput": 185262.056, "learn_time_ms": 11.721, "learn_throughput": 21840.357, "update_time_ms": 2.48}, "info": {"learner": {"policy_01": {"custom_metrics": {}, "learner_stats": {"mean_q": 236.6796112060547, "min_q": 80.57879638671875, "max_q": 3033.47412109375, "cur_lr": 0.0005}, "model": {}, "td_error": [-20.265533447265625, 0.8053970336914062, 4.825401306152344, -6.7773895263671875, 45.192047119140625, -2.0065078735351562, -41.64093017578125, -1.0952682495117188, 0.486175537109375, -2.5721359252929688, -0.08770751953125, 0.02274322509765625, 0.39830780029296875, -0.527313232421875, -0.5147247314453125, -0.6437835693359375, 1.5531387329101562, -1.062042236328125, -1.1522293090820312, -1.2249221801757812, -18.577972412109375, -0.7266464233398438, 0.5715866088867188, -0.8397598266601562, -0.04193115234375, -2.6351089477539062, -0.15077972412109375, 0.09545135498046875, 1.5794601440429688, -0.12451171875, -0.7841873168945312, 1.247283935546875, 0.210113525390625, 0.552398681640625, 0.04180145263671875, -0.01663970947265625, -26.198486328125, -0.1932525634765625, 0.7150726318359375, 0.3982696533203125, -2.589996337890625, -1.7271347045898438, -5.707275390625, -0.15517425537109375, 3.6676254272460938, -9.974334716796875, 1.434783935546875, -0.835113525390625, 3.0454254150390625, -0.9935455322265625, -1.1067047119140625, -3.0855712890625, -0.9637298583984375, -120.059814453125, 0.34455108642578125, -1.0606231689453125, -44.684814453125, 0.5772933959960938, -45.654541015625, 5.495269775390625, 0.414642333984375, 1.4314804077148438, 0.45531463623046875, -0.9412689208984375, -1.1615524291992188, -0.22948455810546875, -1.0429534912109375, -35.162078857421875, 2.250518798828125, -1.0340118408203125, 1.5543746948242188, -0.856536865234375, 0.12654876708984375, -0.598785400390625, 2.223052978515625, -44.989990234375, -0.9444351196289062, -33.194580078125, 0.2989959716796875, -1.0872955322265625, 0.7118759155273438, 1.0277786254882812, -0.27216339111328125, -0.4544830322265625, -0.6574172973632812, -1.3064956665039062, 0.19339752197265625, 1.0052871704101562, 0.32587432861328125, 0.6754379272460938, 84.11366271972656, -0.5566635131835938, -0.9122848510742188, -0.4425506591796875, 0.3438720703125, 0.03816986083984375, 1.9898681640625, 1.67095947265625, 1.4408493041992188, -0.7109603881835938, -1.0199050903320312, -3.504150390625, 89.49748229980469, -1.0987701416015625, -0.422393798828125, 14.375625610351562, -0.4011993408203125, 1.141510009765625, -1.97882080078125, 0.09820556640625, 1.1052627563476562, 2.3463287353515625, 0.5140914916992188, 19.379364013671875, -1.7027664184570312, 59.464805603027344, -1.4080963134765625, 4.00250244140625, -0.18544769287109375, -1.267181396484375, -0.21608734130859375, 0.5046310424804688, -0.3223114013671875, 6.225959777832031, -0.36861419677734375, -0.113555908203125, 0.3224945068359375, -0.0103759765625, 0.144989013671875, 5.925346374511719, -2.2569122314453125, 0.37087249755859375, -61.268798828125, 0.527679443359375, -0.9978790283203125, -0.8137664794921875, -1.89630126953125, -0.0009918212890625, -7.514923095703125, 2.1395416259765625, -85.085693359375, -1.1911544799804688, 0.5157623291015625, -73.7081298828125, -0.13559722900390625, -0.16278076171875, 1.2989501953125, -58.8780517578125, -1.0385055541992188, 1.4151535034179688, 0.5167388916015625, 1.4175567626953125, -1.0414276123046875, 23.152099609375, -0.35166168212890625, -52.931793212890625, -0.7896804809570312, 1.3105926513671875, -74.16476440429688, -1.3230361938476562, -0.6892318725585938, -0.48789215087890625, 0.06011199951171875, 0.0860137939453125, -4.40911865234375, -3.504547119140625, 0.5040817260742188, -0.7575225830078125, 89.75083923339844, 32.04803466796875, 8.780502319335938, -0.16030120849609375, -1.1262969970703125, -19.23748779296875, -1.5023651123046875, 1.4454574584960938, -0.05858612060546875, 0.635711669921875, -0.08582305908203125, 1.3762054443359375, 0.7564315795898438, 1.3245468139648438, 2.9513473510742188, -4.068695068359375, -0.538970947265625, 5.6784820556640625, -4.275871276855469, 0.28699493408203125, 2.8005523681640625, -1.2148513793945312, 3.6116485595703125, 1.1765594482421875, -0.9481201171875, -1.1958465576171875, 3.4198837280273438, -0.8709335327148438, 0.04148101806640625, 1.1498794555664062, 1.3142166137695312, -0.301361083984375, 0.23239898681640625, 0.63836669921875, -67.472900390625, 0.19554901123046875, 0.7400131225585938, -11.85736083984375, -0.5347747802734375, -0.7397003173828125, 0.8325347900390625, 30.998367309570312, 0.9248275756835938, 0.1244659423828125, 0.4157867431640625, -56.07568359375, -0.7538986206054688, -0.98095703125, 1.0246505737304688, -0.9053726196289062, -0.83642578125, -1.2359161376953125, -58.11199951171875, 1.4759063720703125, 1.8523712158203125, 0.6837310791015625, 13.6689453125, -1.5784378051757812, 84.5877685546875, -56.526123046875, 2.0942535400390625, -3.51092529296875, -0.7932510375976562, -0.6463851928710938, -18.577972412109375, 0.798583984375, 4.846565246582031, -1.2078857421875, -116.63446044921875, -56.07568359375, -2.1263504028320312, -0.8408203125, 0.12059783935546875, -13.8148193359375, 1.1693878173828125, 0.9846725463867188, -91.64657592773438, -1.4375381469726562, -0.11589813232421875, 0.8208770751953125, -1.77984619140625, 83.54991149902344, -0.8329849243164062, 7.221038818359375, 13.341529846191406, -2.418182373046875, 8.914947509765625, -0.5544357299804688], "mean_td_error": -2.7965660095214844}}, "num_steps_sampled": 70000, "num_agent_steps_sampled": 140000, "num_steps_trained": 2208256, "num_steps_trained_this_iter": 256, "num_agent_steps_trained": 4416512, "last_target_update_ts": 69544, "num_target_updates": 137}, "done": false, "episodes_total": 116, "training_iteration": 70, "trial_id": "e21e6_00000", "experiment_id": "434cd42d33fd4b638f17fc69fa01d74f", "date": "2022-06-14_19-26-55", "timestamp": 1655249215, "time_this_iter_s": 22.07417583465576, "time_total_s": 1492.9557950496674, "pid": 2568314, "hostname": "lambda-dual", "node_ip": "10.104.51.19", "config": {"num_workers": 2, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.0005, "train_batch_size": 256, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "cnet", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "1v1env", "observation_space": null, "action_space": null, "env_config": {}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": true, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": null, "evaluation_duration": 10, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"num_workers": 2, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.0005, "train_batch_size": 256, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "cnet", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "1v1env", "observation_space": null, "action_space": null, "env_config": {}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": false, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": null, "evaluation_duration": 10, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"explore": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": true, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": null, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 2, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"policy_01": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]], [[[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]], (3, 64, 64), float32)", "Discrete(7)", {}], "policy_02": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]], [[[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]], (3, 64, 64), float32)", "Discrete(7)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": "<function <lambda> at 0x7f1aa2ec4710>", "policies_to_train": ["policy_01"], "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": 100000, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": false, "hiddens": [], "double_q": false, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": true, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": null, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 2, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"policy_01": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]], [[[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]], (3, 64, 64), float32)", "Discrete(7)", {}], "policy_02": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]], [[[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]], (3, 64, 64), float32)", "Discrete(7)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": "<function <lambda> at 0x7f1aa2ec4710>", "policies_to_train": ["policy_01"], "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": 100000, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": false, "hiddens": [], "double_q": false, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "time_since_restore": 1492.9557950496674, "timesteps_since_restore": 17920, "iterations_since_restore": 70, "warmup_time": 45.46659183502197, "perf": {"cpu_util_percent": 28.343749999999996, "ram_util_percent": 15.328125}}
{"episode_reward_max": 441.0, "episode_reward_min": 0.0, "episode_reward_mean": 17.64, "episode_len_mean": 601.0, "episode_media": {}, "episodes_this_iter": 2, "policy_reward_min": {"policy_01": 0.0, "policy_02": 0.0}, "policy_reward_max": {"policy_01": 441.0, "policy_02": 441.0}, "policy_reward_mean": {"policy_01": 8.82, "policy_02": 8.82}, "custom_metrics": {}, "hist_stats": {"episode_reward": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "episode_lengths": [601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601], "policy_policy_01_reward": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "policy_policy_02_reward": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.33349685250246763, "mean_inference_ms": 5.779991943485966, "mean_action_processing_ms": 0.08867938261045576, "mean_env_wait_ms": 7.8243518086206585, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 2, "timesteps_total": 71000, "timesteps_this_iter": 256, "agent_timesteps_total": 142000, "timers": {"load_time_ms": 1.315, "load_throughput": 194652.446, "learn_time_ms": 12.055, "learn_throughput": 21235.596, "update_time_ms": 2.525}, "info": {"learner": {"policy_01": {"custom_metrics": {}, "learner_stats": {"mean_q": 241.3353271484375, "min_q": 81.70144653320312, "max_q": 3307.04833984375, "cur_lr": 0.0005}, "model": {}, "td_error": [2.0370635986328125, 1.2799072265625, 1.0866928100585938, -45.887451171875, 0.8036041259765625, -0.09024810791015625, 0.8794479370117188, 0.970062255859375, 1.0247650146484375, 1.1224822998046875, 2.0913925170898438, 96.50556945800781, 2.268798828125, 2.1592636108398438, 1.4044647216796875, 1.1062240600585938, -0.6120529174804688, 0.6851043701171875, 1.4344329833984375, 1.0889053344726562, 0.05718994140625, 0.45545196533203125, 0.8736648559570312, 0.23636627197265625, -0.5224609375, -0.71099853515625, 2.3104248046875, -17.990310668945312, 0.5723724365234375, 2.0391006469726562, 0.28731536865234375, 2.0491714477539062, 0.8924636840820312, -0.7375640869140625, 12.49017333984375, 0.8171234130859375, 0.42885589599609375, 2.0246200561523438, 1.6948928833007812, 1.79608154296875, -19.833160400390625, 1.5051727294921875, -0.6120529174804688, 2.7066726684570312, 1.9636917114257812, -6.469024658203125, 1.1559600830078125, 0.4206695556640625, -17.86468505859375, 3.10693359375, -16.66064453125, 15.43988037109375, 1.3795394897460938, 0.3798980712890625, 1.1365966796875, 0.2937774658203125, 1.2449188232421875, -11.5185546875, 1.508941650390625, 11.508041381835938, 19.031265258789062, 0.6663970947265625, -0.29126739501953125, -1.1236801147460938, 1.2778244018554688, 2.0282516479492188, 0.5821533203125, -0.22870635986328125, 4.105216979980469, 0.31101226806640625, -0.04785919189453125, -0.08740997314453125, 3.6633148193359375, 23.939971923828125, -1.353485107421875, 22.511070251464844, 18.808189392089844, 2.073516845703125, 1.6529922485351562, 2.4805984497070312, 3.138885498046875, 1.1004562377929688, 1.6090316772460938, -4.4766998291015625, 1.9289779663085938, 1.478118896484375, 1.5145034790039062, 2.335968017578125, 7.4770050048828125, 86.7588882446289, -30.823760986328125, 0.8691635131835938, 2.0425186157226562, 2.0956039428710938, 93.94341278076172, -35.361572265625, 0.7113418579101562, 0.5812911987304688, 0.16277313232421875, 330.2005615234375, 1.5607223510742188, -7.169883728027344, 1.11383056640625, 1.7041244506835938, -11.4100341796875, 1.1904678344726562, 2.1567916870117188, 1.6646347045898438, 1.9121551513671875, 2.0256195068359375, 1.7183151245117188, -0.5256423950195312, 84.92475128173828, -42.75, 3.829864501953125, -0.09340667724609375, 0.604766845703125, -32.761474609375, 1.8315887451171875, 5.420387268066406, 1.2041397094726562, -10.321418762207031, -88.65554809570312, -4.170051574707031, 2.9672317504882812, 1.4315338134765625, 1.0631027221679688, 1.3765487670898438, 0.9056320190429688, 1.7226791381835938, -68.58393859863281, -77.35554504394531, 2.2470703125, -0.5486831665039062, 1.0658187866210938, -0.214813232421875, 1.337005615234375, -3.0655364990234375, 21.576744079589844, 1.8201522827148438, 10.958343505859375, 0.7524948120117188, 2.1643753051757812, 0.773712158203125, 0.9639053344726562, 46.291282653808594, 4.1168060302734375, 2.1392135620117188, 0.8798294067382812, 5.278572082519531, -0.7470855712890625, -1.4022064208984375, 3.2989730834960938, 1.81494140625, -0.10488128662109375, 1.4570770263671875, -4.8650054931640625, 2.2896728515625, 1.241180419921875, -11.40338134765625, 0.01233673095703125, -3.380767822265625, 1.085968017578125, 0.6876449584960938, 1.2111129760742188, -0.1594085693359375, -1.3775253295898438, -1.2908401489257812, 1.3316726684570312, 10.001396179199219, 1.8644027709960938, 3.8904037475585938, 28.0430908203125, 0.9700164794921875, 1.9530487060546875, 0.9014968872070312, 0.9109573364257812, -1.3717803955078125, 7.15374755859375, 1.0289077758789062, 0.4468841552734375, 6.250701904296875, 0.4282989501953125, 5.634361267089844, 1.3451309204101562, 1.2671127319335938, 21.11663818359375, 1.4323196411132812, -62.413848876953125, -212.27685546875, -1.3296661376953125, -12.322029113769531, -0.1201324462890625, 4.139129638671875, -60.10205078125, -0.203948974609375, 0.8648223876953125, -8.142921447753906, 11.35650634765625, 1.1953659057617188, 0.01645660400390625, 1.3663711547851562, 3.34600830078125, 7.783203125, 1.46929931640625, -0.06640625, 0.932861328125, 1.0070419311523438, 5.801216125488281, 0.3826904296875, 1.7804794311523438, -0.2253570556640625, 0.329833984375, 1.841217041015625, 2.9415512084960938, 0.442413330078125, 1.74578857421875, -14.29278564453125, 0.8669662475585938, 1.71783447265625, 1.5827560424804688, 0.7072830200195312, 0.44666290283203125, 0.5117034912109375, 2.5252227783203125, 1.9595794677734375, 3.2008438110351562, 0.34859466552734375, 1.8385086059570312, -70.30271911621094, -0.00765228271484375, -10.44677734375, 1.9815139770507812, 1.2002487182617188, 0.87542724609375, 2.4973297119140625, 1.9750442504882812, 2.9860916137695312, 3.114990234375, 1.4669647216796875, 1.7978286743164062, 20.942047119140625, 4.61834716796875, 67.19540405273438, -24.63996124267578, 5.965972900390625, -0.1190948486328125, 0.17230224609375, -79.54316711425781, 1.5493698120117188, -125.788330078125, 2.0391464233398438, 17.244651794433594, 0.621551513671875, 0.922210693359375, 1.1570816040039062], "mean_td_error": 0.39438939094543457}}, "num_steps_sampled": 71000, "num_agent_steps_sampled": 142000, "num_steps_trained": 2240256, "num_steps_trained_this_iter": 256, "num_agent_steps_trained": 4480512, "last_target_update_ts": 70552, "num_target_updates": 139}, "done": false, "episodes_total": 118, "training_iteration": 71, "trial_id": "e21e6_00000", "experiment_id": "434cd42d33fd4b638f17fc69fa01d74f", "date": "2022-06-14_19-27-17", "timestamp": 1655249237, "time_this_iter_s": 22.075677633285522, "time_total_s": 1515.0314726829529, "pid": 2568314, "hostname": "lambda-dual", "node_ip": "10.104.51.19", "config": {"num_workers": 2, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.0005, "train_batch_size": 256, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "cnet", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "1v1env", "observation_space": null, "action_space": null, "env_config": {}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": true, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": null, "evaluation_duration": 10, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"num_workers": 2, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.0005, "train_batch_size": 256, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "cnet", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "1v1env", "observation_space": null, "action_space": null, "env_config": {}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": false, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": null, "evaluation_duration": 10, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"explore": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": true, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": null, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 2, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"policy_01": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]], [[[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]], (3, 64, 64), float32)", "Discrete(7)", {}], "policy_02": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]], [[[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]], (3, 64, 64), float32)", "Discrete(7)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": "<function <lambda> at 0x7f1aa3700170>", "policies_to_train": ["policy_01"], "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": 100000, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": false, "hiddens": [], "double_q": false, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": true, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": null, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 2, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"policy_01": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]], [[[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]], (3, 64, 64), float32)", "Discrete(7)", {}], "policy_02": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]], [[[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]], (3, 64, 64), float32)", "Discrete(7)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": "<function <lambda> at 0x7f1aa3700170>", "policies_to_train": ["policy_01"], "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": 100000, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": false, "hiddens": [], "double_q": false, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "time_since_restore": 1515.0314726829529, "timesteps_since_restore": 18176, "iterations_since_restore": 71, "warmup_time": 45.46659183502197, "perf": {"cpu_util_percent": 28.16451612903226, "ram_util_percent": 15.503225806451614}}
{"episode_reward_max": 441.0, "episode_reward_min": 0.0, "episode_reward_mean": 17.64, "episode_len_mean": 601.0, "episode_media": {}, "episodes_this_iter": 0, "policy_reward_min": {"policy_01": 0.0, "policy_02": 0.0}, "policy_reward_max": {"policy_01": 441.0, "policy_02": 441.0}, "policy_reward_mean": {"policy_01": 8.82, "policy_02": 8.82}, "custom_metrics": {}, "hist_stats": {"episode_reward": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "episode_lengths": [601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601], "policy_policy_01_reward": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "policy_policy_02_reward": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.33349685250246763, "mean_inference_ms": 5.779991943485966, "mean_action_processing_ms": 0.08867938261045576, "mean_env_wait_ms": 7.8243518086206585, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 2, "timesteps_total": 72000, "timesteps_this_iter": 256, "agent_timesteps_total": 144000, "timers": {"load_time_ms": 1.318, "load_throughput": 194268.572, "learn_time_ms": 12.154, "learn_throughput": 21062.269, "update_time_ms": 2.414}, "info": {"learner": {"policy_01": {"custom_metrics": {}, "learner_stats": {"mean_q": 319.1303405761719, "min_q": 81.78752136230469, "max_q": 4439.62890625, "cur_lr": 0.0005}, "model": {}, "td_error": [1.5269775390625, 76.42840576171875, -1.1141433715820312, -0.6792831420898438, -0.3471832275390625, 102.11551666259766, 85.07211303710938, -2.0242919921875, 1.7837066650390625, -2.9244461059570312, -1.7022476196289062, -0.6377639770507812, -1.36016845703125, -1.8275833129882812, -0.18231201171875, 4.9982757568359375, -0.24423980712890625, -0.5466766357421875, -1.046356201171875, 0.25164794921875, -89.0986328125, -0.8706512451171875, 88.75605773925781, -0.21308135986328125, 0.21523284912109375, -4.721183776855469, -2.506195068359375, -5.16046142578125, 1.0023574829101562, -1.1405715942382812, 1.683258056640625, 0.6273956298828125, -0.3480377197265625, -2.808837890625, 1.8771591186523438, -1.3147354125976562, -1.9347763061523438, -1.4256210327148438, 104.59001159667969, -1.5255661010742188, -0.1273345947265625, -41.395751953125, -5.3252716064453125, -1.9480438232421875, -2.642242431640625, -1.313079833984375, -1.246673583984375, 0.3180389404296875, 0.5419845581054688, 0.5578994750976562, 0.6338348388671875, -0.080596923828125, 1.0777587890625, 0.48249053955078125, -1.7658615112304688, -17.144775390625, 0.4680938720703125, -89.0986328125, -1.8372955322265625, -1.4298095703125, -56.530517578125, -0.1226043701171875, 85.01228332519531, 2.454742431640625, 0.07831573486328125, 1.9939651489257812, 0.8855819702148438, 0.81671142578125, 88.29196166992188, -3.2685470581054688, -1.433685302734375, -39.0101318359375, -3.037139892578125, -0.20403289794921875, 0.7996749877929688, -0.7572784423828125, 1.9543609619140625, 0.8916473388671875, -28.09814453125, -0.8320846557617188, -1.804046630859375, -0.00922393798828125, -1.7346038818359375, -4.981719970703125, 27.74494171142578, -0.819183349609375, -0.9534454345703125, -182.931640625, 2.33184814453125, 2.3945388793945312, -4.962898254394531, -0.8043289184570312, -0.8227996826171875, -0.5087356567382812, 7.2745361328125, 0.8154449462890625, -0.4915008544921875, 1.6787567138671875, -4.851142883300781, -223.34402465820312, -2.112091064453125, 4.846992492675781, -12.20953369140625, -1.823394775390625, -10.28765869140625, 1.58062744140625, 1.2286148071289062, -48.760498046875, 0.1927947998046875, -0.997222900390625, 0.4395294189453125, 85.97409057617188, 0.36971282958984375, 0.5905990600585938, -0.244049072265625, 5.47998046875, -1.179779052734375, -233.96075439453125, 0.917816162109375, -1.0142822265625, -9.597190856933594, 0.41629791259765625, -1.8009414672851562, -0.29354095458984375, -3.7483444213867188, 0.7663192749023438, 38.296142578125, -1.1096572875976562, 0.145355224609375, -0.7200927734375, -3.0843963623046875, -70.45849609375, -1.0055084228515625, -0.3608551025390625, -0.794036865234375, -0.9561538696289062, -0.440704345703125, 1.6994781494140625, -0.8293685913085938, -1.7777557373046875, 1.50439453125, -0.6141281127929688, -21.007064819335938, -2.3580322265625, -0.8239593505859375, -1.6080322265625, 0.426300048828125, -156.939453125, 0.490203857421875, 0.02297210693359375, 1.9495010375976562, -1.981353759765625, 0.7137985229492188, -4.13983154296875, 56.40924072265625, -1.523101806640625, 9.206794738769531, -2.9535064697265625, -57.441802978515625, -0.05145263671875, 3.5180511474609375, -1.740325927734375, 0.3500518798828125, -0.02312469482421875, -0.69671630859375, 1.2871551513671875, 35.96142578125, -3.2265548706054688, -147.8125, 85.01228332519531, -4.027679443359375, -156.939453125, -0.609710693359375, -5.6984405517578125, 85.43798065185547, -1.70770263671875, -0.2773895263671875, -0.34588623046875, -1.568878173828125, -5.02728271484375, -279.8662109375, -21.662628173828125, 1.1021041870117188, -48.141693115234375, 139.8587646484375, -2.4913177490234375, -0.4937591552734375, -5.4355926513671875, -0.9956893920898438, -2.7318496704101562, -0.8399658203125, -3.2142181396484375, 0.26245880126953125, -0.2771148681640625, 2.4940719604492188, -1.8410568237304688, -5.483940124511719, 0.5509109497070312, -1.6540908813476562, -71.197021484375, -0.6629486083984375, -0.41680145263671875, -9.446624755859375, 0.2469024658203125, -19.52056884765625, 2.0323944091796875, -1.805999755859375, -1.5324172973632812, -1.3660659790039062, -0.9383468627929688, -1.1503143310546875, 0.28823089599609375, 1.864501953125, 4.229393005371094, 1.6939315795898438, -2.56463623046875, 4417.14453125, 2.467132568359375, -8.238105773925781, -0.8635406494140625, -38.53172302246094, -0.57330322265625, 0.5621414184570312, 0.8388214111328125, -5.546165466308594, -148.9144287109375, -29.718505859375, -0.091583251953125, -4.550621032714844, 0.5899810791015625, -0.5028839111328125, 49.412353515625, 83.3558578491211, -0.7076263427734375, -1.297393798828125, -0.8488235473632812, -6.101936340332031, -1.7695999145507812, -0.9558181762695312, -2.9915008544921875, 0.86334228515625, 2.0681915283203125, -2.2777938842773438, -3.2884979248046875, -1.3998336791992188, -0.5039520263671875, -7.845001220703125, -1.8648147583007812, 67.64605712890625, 87.52848815917969, -1.7378387451171875, -1.6706161499023438, -0.22791290283203125, 0.79083251953125, 0.3142242431640625, 65.490234375], "mean_td_error": 13.494230270385742}}, "num_steps_sampled": 72000, "num_agent_steps_sampled": 144000, "num_steps_trained": 2272256, "num_steps_trained_this_iter": 256, "num_agent_steps_trained": 4544512, "last_target_update_ts": 71560, "num_target_updates": 141}, "done": false, "episodes_total": 118, "training_iteration": 72, "trial_id": "e21e6_00000", "experiment_id": "434cd42d33fd4b638f17fc69fa01d74f", "date": "2022-06-14_19-27-39", "timestamp": 1655249259, "time_this_iter_s": 21.9298312664032, "time_total_s": 1536.961303949356, "pid": 2568314, "hostname": "lambda-dual", "node_ip": "10.104.51.19", "config": {"num_workers": 2, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.0005, "train_batch_size": 256, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "cnet", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "1v1env", "observation_space": null, "action_space": null, "env_config": {}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": true, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": null, "evaluation_duration": 10, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"num_workers": 2, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.0005, "train_batch_size": 256, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "cnet", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "1v1env", "observation_space": null, "action_space": null, "env_config": {}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": false, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": null, "evaluation_duration": 10, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"explore": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": true, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": null, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 2, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"policy_01": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]], [[[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]], (3, 64, 64), float32)", "Discrete(7)", {}], "policy_02": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]], [[[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]], (3, 64, 64), float32)", "Discrete(7)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": "<function <lambda> at 0x7f1aa2ec4e60>", "policies_to_train": ["policy_01"], "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": 100000, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": false, "hiddens": [], "double_q": false, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": true, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": null, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 2, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"policy_01": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]], [[[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]], (3, 64, 64), float32)", "Discrete(7)", {}], "policy_02": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]], [[[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]], (3, 64, 64), float32)", "Discrete(7)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": "<function <lambda> at 0x7f1aa2ec4e60>", "policies_to_train": ["policy_01"], "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": 100000, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": false, "hiddens": [], "double_q": false, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "time_since_restore": 1536.961303949356, "timesteps_since_restore": 18432, "iterations_since_restore": 72, "warmup_time": 45.46659183502197, "perf": {"cpu_util_percent": 25.140625, "ram_util_percent": 15.537500000000001}}
{"episode_reward_max": 441.0, "episode_reward_min": 0.0, "episode_reward_mean": 17.64, "episode_len_mean": 601.0, "episode_media": {}, "episodes_this_iter": 2, "policy_reward_min": {"policy_01": 0.0, "policy_02": 0.0}, "policy_reward_max": {"policy_01": 441.0, "policy_02": 441.0}, "policy_reward_mean": {"policy_01": 8.82, "policy_02": 8.82}, "custom_metrics": {}, "hist_stats": {"episode_reward": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "episode_lengths": [601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601], "policy_policy_01_reward": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "policy_policy_02_reward": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.3334450932228926, "mean_inference_ms": 5.778625790444446, "mean_action_processing_ms": 0.088663124401449, "mean_env_wait_ms": 7.819338601751681, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 2, "timesteps_total": 73000, "timesteps_this_iter": 256, "agent_timesteps_total": 146000, "timers": {"load_time_ms": 1.321, "load_throughput": 193749.765, "learn_time_ms": 11.787, "learn_throughput": 21718.43, "update_time_ms": 2.366}, "info": {"learner": {"policy_01": {"custom_metrics": {}, "learner_stats": {"mean_q": 322.73895263671875, "min_q": 76.5906982421875, "max_q": 4745.181640625, "cur_lr": 0.0005}, "model": {}, "td_error": [0.5014801025390625, -7.49957275390625, 2.185943603515625, 29.64111328125, 0.8427658081054688, 0.534332275390625, 92.8365478515625, 3.5666275024414062, -22.43408966064453, 1.6315155029296875, -0.0635833740234375, 2.52716064453125, -17.638671875, 2.5830535888671875, -244.42385864257812, -16.63525390625, -16.436569213867188, -33.76806640625, 1.6224365234375, 5.2183074951171875, 2.2788314819335938, 8.651008605957031, 29.7529296875, 1.7547149658203125, 1.90472412109375, -0.26071929931640625, -17.536483764648438, 0.3242340087890625, 0.3979644775390625, 0.2444610595703125, 1.295440673828125, 0.181549072265625, 0.908111572265625, 0.10036468505859375, 6.073516845703125, 3.4530258178710938, -269.734130859375, 17.96117401123047, -2.5207443237304688, 8.409721374511719, 2.6312255859375, 108.4373550415039, 24.94744873046875, 0.750091552734375, -0.23015594482421875, -1.50445556640625, -3.052947998046875, 1.2403335571289062, 1.7603836059570312, 3.3903350830078125, 0.260833740234375, 1.0778884887695312, 3.1032257080078125, 1.4249649047851562, 148.56640625, 90.99288940429688, -0.23015594482421875, 6.937843322753906, 1.3189163208007812, 1.9162826538085938, 93.8248062133789, 4593.3701171875, 5.0016937255859375, 1.0985488891601562, -174.74847412109375, -0.47678375244140625, -1.6676101684570312, -6.18798828125, 1.5780715942382812, 1.669158935546875, 3.103851318359375, 0.5100860595703125, -25.846832275390625, 95.4053726196289, 1.740325927734375, -91.16116333007812, -13.734848022460938, 2.2005157470703125, 0.47601318359375, -18.754180908203125, -0.4310455322265625, 0.7542877197265625, 2.043853759765625, 1.5105972290039062, 2.1799087524414062, -15.368011474609375, -3.625732421875, 69.46031951904297, -6.088630676269531, -0.25064849853515625, -1.5488433837890625, 32.38236999511719, 1.4095001220703125, 2.2275543212890625, 1.7061691284179688, 2.2310562133789062, -0.0819091796875, -0.31861114501953125, 3.5134429931640625, -33.07861328125, -2.55096435546875, -20.36151123046875, 0.30047607421875, 1.849334716796875, 0.645782470703125, 0.263519287109375, 66.2308349609375, 0.06922149658203125, 3.062744140625, 0.9034271240234375, -0.9691009521484375, 2.6410140991210938, -11.1373291015625, 3.491790771484375, 23.5322265625, 0.61932373046875, -0.0990142822265625, -1.5560836791992188, -17.83624267578125, -0.819305419921875, 0.7321395874023438, -65.73573303222656, 1.4678955078125, 1.3571929931640625, 2.1742477416992188, -21.17957305908203, 1.70928955078125, -10.516273498535156, 0.18210601806640625, -2.1954879760742188, 0.51458740234375, 2.6665115356445312, 1.8587799072265625, -0.33533477783203125, 8.034149169921875, 2.9304580688476562, 1.6018447875976562, 1.7241668701171875, 0.4908447265625, 9.682441711425781, 1.0060272216796875, -91.16116333007812, 0.449859619140625, 2.0911178588867188, -0.383544921875, 1.4827957153320312, 1.3437347412109375, 7.653289794921875, 0.37454986572265625, 0.0904693603515625, 2.1206130981445312, -0.214080810546875, 2.1339492797851562, 2.288665771484375, -1.7294692993164062, -180.0380859375, -3.68603515625, 0.6971969604492188, -85.7579345703125, -0.19150543212890625, 10.232147216796875, -9.800735473632812, 2.192230224609375, 7.1674957275390625, 0.7023544311523438, 1.7577133178710938, 4.58074951171875, 1.3253173828125, 9.669754028320312, 1.938751220703125, -1.1662216186523438, -0.16901397705078125, -25.6678466796875, 5.553276062011719, 0.06231689453125, 2.0043792724609375, -38.50288391113281, -2.5784530639648438, -10.57666015625, -37.07007598876953, -1.4623260498046875, 2.4025802612304688, 9.926063537597656, -0.5067367553710938, -2.504364013671875, 0.19915771484375, 108.40528106689453, 0.5271377563476562, -26.091064453125, 2.82159423828125, 0.858367919921875, 2.6193161010742188, 0.1487579345703125, 7.898490905761719, 8.682243347167969, 5.615928649902344, -1.7558364868164062, 3.6969070434570312, 213.9936981201172, -2.3006973266601562, 0.7138442993164062, 1.3873367309570312, -9.23602294921875, 24.413330078125, 0.34889984130859375, 1.9870147705078125, 4.055244445800781, 0.6526641845703125, 3.743621826171875, -31.694488525390625, 1.9479293823242188, 2.7549285888671875, 103.28341674804688, 0.560699462890625, -10.717658996582031, -17.10687255859375, 74.7701416015625, -2.4145660400390625, 0.659759521484375, 0.34889984130859375, 3.79132080078125, 1.7801895141601562, 1.5500106811523438, 0.6986236572265625, 2.0106658935546875, 2.5976486206054688, 1.6288681030273438, -0.8192596435546875, 1.9232101440429688, 0.17282867431640625, 145.89892578125, 1.29705810546875, 1.95672607421875, -46.56703186035156, -11.74749755859375, 8.142768859863281, 40.09326171875, 4.42401123046875, -9.39129638671875, -0.4296417236328125, 22.52545166015625, 0.03029632568359375, 3.6259307861328125, -3.337799072265625, 94.51910400390625, -2.0145950317382812, 0.44203948974609375, 1.3686447143554688, 1.6025772094726562, -0.27117919921875, 0.3243408203125, 2.85400390625, 2.2225112915039062, 0.15198516845703125, -3.861083984375, 89.559326171875], "mean_td_error": 19.165048599243164}}, "num_steps_sampled": 73000, "num_agent_steps_sampled": 146000, "num_steps_trained": 2304256, "num_steps_trained_this_iter": 256, "num_agent_steps_trained": 4608512, "last_target_update_ts": 72568, "num_target_updates": 143}, "done": false, "episodes_total": 120, "training_iteration": 73, "trial_id": "e21e6_00000", "experiment_id": "434cd42d33fd4b638f17fc69fa01d74f", "date": "2022-06-14_19-28-01", "timestamp": 1655249281, "time_this_iter_s": 22.36423373222351, "time_total_s": 1559.3255376815796, "pid": 2568314, "hostname": "lambda-dual", "node_ip": "10.104.51.19", "config": {"num_workers": 2, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.0005, "train_batch_size": 256, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "cnet", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "1v1env", "observation_space": null, "action_space": null, "env_config": {}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": true, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": null, "evaluation_duration": 10, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"num_workers": 2, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.0005, "train_batch_size": 256, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "cnet", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "1v1env", "observation_space": null, "action_space": null, "env_config": {}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": false, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": null, "evaluation_duration": 10, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"explore": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": true, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": null, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 2, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"policy_01": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]], [[[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]], (3, 64, 64), float32)", "Discrete(7)", {}], "policy_02": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]], [[[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]], (3, 64, 64), float32)", "Discrete(7)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": "<function <lambda> at 0x7f1aa2ec4cb0>", "policies_to_train": ["policy_01"], "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": 100000, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": false, "hiddens": [], "double_q": false, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": true, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": null, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 2, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"policy_01": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]], [[[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]], (3, 64, 64), float32)", "Discrete(7)", {}], "policy_02": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]], [[[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]], (3, 64, 64), float32)", "Discrete(7)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": "<function <lambda> at 0x7f1aa2ec4cb0>", "policies_to_train": ["policy_01"], "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": 100000, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": false, "hiddens": [], "double_q": false, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "time_since_restore": 1559.3255376815796, "timesteps_since_restore": 18688, "iterations_since_restore": 73, "warmup_time": 45.46659183502197, "perf": {"cpu_util_percent": 30.477419354838712, "ram_util_percent": 15.770967741935486}}
{"episode_reward_max": 441.0, "episode_reward_min": 0.0, "episode_reward_mean": 17.64, "episode_len_mean": 601.0, "episode_media": {}, "episodes_this_iter": 2, "policy_reward_min": {"policy_01": 0.0, "policy_02": 0.0}, "policy_reward_max": {"policy_01": 441.0, "policy_02": 441.0}, "policy_reward_mean": {"policy_01": 8.82, "policy_02": 8.82}, "custom_metrics": {}, "hist_stats": {"episode_reward": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "episode_lengths": [601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601], "policy_policy_01_reward": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "policy_policy_02_reward": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.333372885786276, "mean_inference_ms": 5.777239834787025, "mean_action_processing_ms": 0.08864833424765621, "mean_env_wait_ms": 7.814778821636889, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 2, "timesteps_total": 74000, "timesteps_this_iter": 256, "agent_timesteps_total": 148000, "timers": {"load_time_ms": 1.368, "load_throughput": 187196.748, "learn_time_ms": 11.661, "learn_throughput": 21953.197, "update_time_ms": 2.275}, "info": {"learner": {"policy_01": {"custom_metrics": {}, "learner_stats": {"mean_q": 304.8521728515625, "min_q": 83.98988342285156, "max_q": 3489.268798828125, "cur_lr": 0.0005}, "model": {}, "td_error": [1.6898193359375, 1.9136810302734375, 0.07189178466796875, 9.392242431640625, -0.7054519653320312, 2.2246856689453125, 30.8302001953125, 0.3591766357421875, 1.4364547729492188, -154.45602416992188, 1.287841796875, 1.5973358154296875, -0.31980133056640625, 2.738555908203125, 44.2822265625, -3.072021484375, -5.8233795166015625, -0.017822265625, -67.384033203125, 2.792205810546875, 0.17806243896484375, 1.2587356567382812, 0.6580429077148438, 3.446502685546875, 8.11590576171875, 0.05567169189453125, 3.1865692138671875, 1.49359130859375, 4.622406005859375, 0.768890380859375, 1.0034866333007812, -7.5353851318359375, 4.6630859375, -0.7125396728515625, -0.10475921630859375, 0.9844818115234375, 4.0769500732421875, -10.002059936523438, 49.97412109375, 1.2607498168945312, 0.13291168212890625, 87.40467834472656, 1.6249618530273438, -24.875335693359375, -0.12152862548828125, 3.1022262573242188, 19.018951416015625, 2.4496994018554688, 1.5949020385742188, -0.33010101318359375, 0.11251068115234375, 1.3012161254882812, 1.1547622680664062, 2.4539947509765625, 2.08599853515625, 3.2711563110351562, 16.312942504882812, 0.9799652099609375, -19.093048095703125, -8.842231750488281, 0.56488037109375, 0.7659378051757812, 2.2899932861328125, 0.8374786376953125, -0.8823623657226562, 13.71563720703125, -1.0348052978515625, 1.1074371337890625, 1.6070480346679688, -198.83653259277344, 0.08127593994140625, 0.768890380859375, -0.5425033569335938, 3.0696563720703125, 1.4385833740234375, -4.204063415527344, 0.7923812866210938, 0.6813735961914062, 1.2168807983398438, -34.1630859375, -1.9093780517578125, 0.08580780029296875, -0.9596099853515625, -0.8128738403320312, 1.6081619262695312, 0.6561355590820312, 2.9228744506835938, 1.3199920654296875, 88.68392944335938, 3.5660171508789062, 10.725006103515625, 59.25323486328125, 0.0614166259765625, 0.41176605224609375, -0.43520355224609375, 21.752685546875, 3.0224533081054688, 1.3346939086914062, 2.8357086181640625, 11.2637939453125, -0.7957916259765625, 87.40467834472656, 20.989349365234375, -1.4570083618164062, -1.0442428588867188, 1.2633132934570312, 2.8602523803710938, 1.4841537475585938, 3.7280197143554688, 155.64878845214844, -8.330322265625, -45.05517578125, -31.559951782226562, 3.2378311157226562, 1.706207275390625, -0.06867218017578125, 0.7216567993164062, 0.42716217041015625, 11.065704345703125, 0.3112640380859375, 1.2134933471679688, 2.7894287109375, 9.300048828125, 0.6274795532226562, 3.430328369140625, -236.4593963623047, -1.4720382690429688, 1.0373077392578125, 1.867767333984375, 1.5033111572265625, 3.5823593139648438, 0.6084136962890625, -6.105804443359375, 2.9679641723632812, -53.24829864501953, 1.0615463256835938, 2.0177383422851562, -1.6173782348632812, 103.45513916015625, -0.7423095703125, -1.3454666137695312, -0.40427398681640625, 3.0700607299804688, 0.9853668212890625, 0.110076904296875, -3.1278610229492188, 86.3267593383789, -0.0936279296875, 15.676971435546875, 3.029327392578125, 2.2252044677734375, -179.41390991210938, 4.02337646484375, -200.78042602539062, 3.1456298828125, 0.04662322998046875, 42.103271484375, -0.0980072021484375, -1.4586410522460938, 15.443901062011719, 14.164703369140625, 38.223876953125, -0.41410064697265625, 2.0157546997070312, 2.1832046508789062, 93.65325927734375, -0.6889877319335938, -17.826416015625, -49.82396697998047, 2.1583709716796875, -8.4361572265625, -0.083709716796875, 0.14675140380859375, 1.32763671875, 1.0727310180664062, -32.76020812988281, 2.5125732421875, -0.801849365234375, -107.204345703125, -0.266204833984375, 34.94474792480469, 1.4624786376953125, 59.386749267578125, 4.423942565917969, 7.2294158935546875, 3.80267333984375, -0.02147674560546875, 0.044830322265625, -1.3045654296875, -2.595245361328125, 2.1211013793945312, 0.01728057861328125, 1.8938674926757812, 1.76141357421875, -0.070098876953125, 45.827392578125, 3.208892822265625, 6.2072906494140625, -0.12372589111328125, 1.2265396118164062, -91.574951171875, 3.1927413940429688, -0.43660736083984375, 8.128509521484375, -309.54248046875, 19.158447265625, 3.6612472534179688, 5.917060852050781, -55.72705078125, 3.4765625, -18.008224487304688, -98.50390625, 1.6522979736328125, 2.8708572387695312, -53.24829864501953, 8.89068603515625, -25.20361328125, -10.194900512695312, -2.2703857421875, 1.6847000122070312, 0.41650390625, 1.0814666748046875, 3.0267410278320312, -0.4760589599609375, -2.3500442504882812, -0.2444305419921875, -0.02048492431640625, 0.7645492553710938, 2.3734359741210938, 1.5413436889648438, -0.3737030029296875, -1.0085067749023438, 119.61195373535156, 1.303253173828125, 4.579200744628906, -2.5172119140625, -186.50082397460938, -1.32281494140625, 1.3620529174804688, -0.1572265625, 0.93310546875, -0.36318206787109375, 47.407958984375, 3.5573272705078125, 0.04283905029296875, 2.1454315185546875, -0.16983795166015625, 0.8798599243164062, 2.5318222045898438, 11.371185302734375, -7.557861328125, 1.4712677001953125, 0.3539886474609375, 0.31980133056640625, 18.618453979492188, 3.0896530151367188], "mean_td_error": -2.4348106384277344}}, "num_steps_sampled": 74000, "num_agent_steps_sampled": 148000, "num_steps_trained": 2336256, "num_steps_trained_this_iter": 256, "num_agent_steps_trained": 4672512, "last_target_update_ts": 73576, "num_target_updates": 145}, "done": false, "episodes_total": 122, "training_iteration": 74, "trial_id": "e21e6_00000", "experiment_id": "434cd42d33fd4b638f17fc69fa01d74f", "date": "2022-06-14_19-28-23", "timestamp": 1655249303, "time_this_iter_s": 21.919382333755493, "time_total_s": 1581.244920015335, "pid": 2568314, "hostname": "lambda-dual", "node_ip": "10.104.51.19", "config": {"num_workers": 2, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.0005, "train_batch_size": 256, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "cnet", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "1v1env", "observation_space": null, "action_space": null, "env_config": {}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": true, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": null, "evaluation_duration": 10, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"num_workers": 2, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.0005, "train_batch_size": 256, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "cnet", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "1v1env", "observation_space": null, "action_space": null, "env_config": {}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": false, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": null, "evaluation_duration": 10, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"explore": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": true, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": null, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 2, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"policy_01": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]], [[[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]], (3, 64, 64), float32)", "Discrete(7)", {}], "policy_02": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]], [[[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]], (3, 64, 64), float32)", "Discrete(7)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": "<function <lambda> at 0x7f1aa3767b00>", "policies_to_train": ["policy_01"], "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": 100000, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": false, "hiddens": [], "double_q": false, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": true, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": null, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 2, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"policy_01": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]], [[[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]], (3, 64, 64), float32)", "Discrete(7)", {}], "policy_02": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]], [[[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]], (3, 64, 64), float32)", "Discrete(7)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": "<function <lambda> at 0x7f1aa3767b00>", "policies_to_train": ["policy_01"], "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": 100000, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": false, "hiddens": [], "double_q": false, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "time_since_restore": 1581.244920015335, "timesteps_since_restore": 18944, "iterations_since_restore": 74, "warmup_time": 45.46659183502197, "perf": {"cpu_util_percent": 25.24375, "ram_util_percent": 15.815625}}
{"episode_reward_max": 441.0, "episode_reward_min": 0.0, "episode_reward_mean": 17.64, "episode_len_mean": 601.0, "episode_media": {}, "episodes_this_iter": 2, "policy_reward_min": {"policy_01": 0.0, "policy_02": 0.0}, "policy_reward_max": {"policy_01": 441.0, "policy_02": 441.0}, "policy_reward_mean": {"policy_01": 8.82, "policy_02": 8.82}, "custom_metrics": {}, "hist_stats": {"episode_reward": [0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "episode_lengths": [601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601], "policy_policy_01_reward": [0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "policy_policy_02_reward": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 441.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.333287396945738, "mean_inference_ms": 5.775869716133557, "mean_action_processing_ms": 0.0886348216969506, "mean_env_wait_ms": 7.810585228975349, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 2, "timesteps_total": 75000, "timesteps_this_iter": 256, "agent_timesteps_total": 150000, "timers": {"load_time_ms": 1.306, "load_throughput": 195970.474, "learn_time_ms": 12.007, "learn_throughput": 21320.138, "update_time_ms": 2.418}, "info": {"learner": {"policy_01": {"custom_metrics": {}, "learner_stats": {"mean_q": 347.06982421875, "min_q": 86.8983154296875, "max_q": 4444.9833984375, "cur_lr": 0.0005}, "model": {}, "td_error": [0.445220947265625, 7.989860534667969, 2.1655807495117188, -6.542236328125, 0.1077880859375, -0.2677459716796875, 1.4079360961914062, -26.56781005859375, -0.5610504150390625, -0.5600433349609375, -33.4530029296875, 1.7964019775390625, 2.0543136596679688, -1.2066726684570312, 123.47905731201172, 1.0375137329101562, 125.82943725585938, 1.1145401000976562, 1.4819488525390625, 19.0076904296875, 122.89982604980469, 103.052734375, -8.21136474609375, -0.8400726318359375, 0.325164794921875, -0.5639419555664062, 1.42138671875, 0.07039642333984375, 0.9778976440429688, -0.3174591064453125, 1.2758255004882812, -1.1751861572265625, -0.48845672607421875, 123.83214569091797, -0.996429443359375, -0.776824951171875, -10.682373046875, 8.000167846679688, -0.6149444580078125, 0.03380584716796875, 0.1602630615234375, -1.6325225830078125, 1.0725784301757812, -2.3961105346679688, 55.02471923828125, 4.451728820800781, 7.8833465576171875, -1.741485595703125, -0.877349853515625, 2.6721343994140625, -183.9635772705078, 206.742919921875, -13.405220031738281, 0.7691268920898438, -16.079299926757812, 0.8368682861328125, -1.3725967407226562, 2.4369354248046875, -10.95849609375, 1.4507904052734375, -3.2251358032226562, -23.57958984375, 3.4302825927734375, -1.0926284790039062, 0.39892578125, -5.095245361328125, -27.4541015625, -3.92938232421875, 2.2014617919921875, 1.1869430541992188, 49.132110595703125, -0.5210189819335938, 8.113594055175781, -0.5783309936523438, -0.065643310546875, -16.535659790039062, 2.5417938232421875, -0.06960296630859375, 3.3394546508789062, 0.6927490234375, -1.1192855834960938, -0.8354415893554688, -5.6243438720703125, -21.9925537109375, -1.3240203857421875, 2.2185897827148438, -0.2405242919921875, 2.2856292724609375, -17.917205810546875, -18.08349609375, 0.5066146850585938, 10.472915649414062, 2.5802154541015625, 0.5174102783203125, 1.8466644287109375, 0.8323593139648438, 0.09780120849609375, 2.92041015625, 7.5118408203125, -0.3760833740234375, -2.1391983032226562, 1.2849044799804688, -1.6436920166015625, 1.6219100952148438, -6.6356201171875, -1.4196929931640625, 8.434967041015625, -0.9262466430664062, -12.53204345703125, 0.47637176513671875, -0.3492584228515625, 58.463134765625, -0.43106842041015625, -0.2803497314453125, -1.1192855834960938, 1.2030715942382812, -0.9713516235351562, 0.509063720703125, 1.9197311401367188, 0.1133575439453125, 5.9857940673828125, 0.450164794921875, 7.4598388671875, -17.182037353515625, 1.8247528076171875, -1.0762100219726562, -187.37744140625, -0.1927032470703125, 1.5282669067382812, 1.5685577392578125, 1.9794464111328125, -0.46240997314453125, 1.299591064453125, 0.9975814819335938, 0.8476638793945312, 5.4385223388671875, -0.09090423583984375, -0.39557647705078125, 0.5339889526367188, 0.5319976806640625, 3.5909042358398438, -1.2379989624023438, 0.9721755981445312, 0.9018402099609375, -0.123443603515625, 1.4670486450195312, 0.49903106689453125, 1.3301925659179688, 31.533447265625, -0.0782012939453125, -2.5892791748046875, -23.18115234375, 1.6414337158203125, 0.8656463623046875, 0.51361083984375, 89.4085693359375, -1.0087738037109375, -9.837310791015625, -23.18115234375, 41.48072814941406, 110.4853515625, -2.8869857788085938, 1.9696044921875, 2.7237014770507812, 2.5708999633789062, -0.08425140380859375, 3.3387908935546875, -0.6125259399414062, 0.326568603515625, 2.37371826171875, 1.632537841796875, 2.1706390380859375, -208.6904296875, -2.3610763549804688, 2.9820785522460938, -53.17833709716797, 6.484962463378906, 8.12371826171875, -1.3808746337890625, -19.841552734375, -0.7344818115234375, -0.8627090454101562, -0.372314453125, -1.2976531982421875, -1.4296112060546875, -1.60400390625, 0.244232177734375, 8.718086242675781, 1.5135726928710938, 0.7371444702148438, -16.597564697265625, -3.1975936889648438, -1.8289337158203125, 1.5072250366210938, 1.3177261352539062, -1.6599578857421875, 0.16705322265625, 2.5745315551757812, 0.6246337890625, 1.7152252197265625, 0.01200103759765625, -1.7818603515625, 0.014892578125, 0.9808731079101562, 0.08806610107421875, 4.5766754150390625, -6.1333770751953125, -0.298187255859375, -1.1615676879882812, -0.9063034057617188, -0.8662185668945312, -21.159011840820312, -5.254730224609375, -0.19542694091796875, -0.2917938232421875, 0.7176361083984375, -0.7809371948242188, -76.14822387695312, 0.6675567626953125, -6.339080810546875, 1.963348388671875, -1.0707550048828125, -0.12928009033203125, 0.1775665283203125, 6.203620910644531, 2.3757705688476562, 3.8388137817382812, 8.000167846679688, -0.7342910766601562, -49.76518249511719, 7.452156066894531, 0.38433837890625, -4.0117950439453125, -0.6067352294921875, 0.336151123046875, 52.482421875, 0.7769241333007812, 0.0106964111328125, -1.3366165161132812, 1.3240280151367188, -2.2302627563476562, 2.6472320556640625, 46.88456726074219, 18.06658935546875, -47.56005859375, -0.888397216796875, 1.7442855834960938, -1.3481369018554688, -65.7724609375, -0.13703155517578125, 119.37515258789062, -10.461944580078125, -1.2834701538085938, 1.1822128295898438, -0.020416259765625, 1.5382766723632812], "mean_td_error": 1.4190185070037842}}, "num_steps_sampled": 75000, "num_agent_steps_sampled": 150000, "num_steps_trained": 2368256, "num_steps_trained_this_iter": 256, "num_agent_steps_trained": 4736512, "last_target_update_ts": 74584, "num_target_updates": 147}, "done": false, "episodes_total": 124, "training_iteration": 75, "trial_id": "e21e6_00000", "experiment_id": "434cd42d33fd4b638f17fc69fa01d74f", "date": "2022-06-14_19-28-45", "timestamp": 1655249325, "time_this_iter_s": 21.94019389152527, "time_total_s": 1603.1851139068604, "pid": 2568314, "hostname": "lambda-dual", "node_ip": "10.104.51.19", "config": {"num_workers": 2, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.0005, "train_batch_size": 256, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "cnet", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "1v1env", "observation_space": null, "action_space": null, "env_config": {}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": true, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": null, "evaluation_duration": 10, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"num_workers": 2, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.0005, "train_batch_size": 256, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "cnet", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "1v1env", "observation_space": null, "action_space": null, "env_config": {}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": false, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": null, "evaluation_duration": 10, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"explore": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": true, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": null, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 2, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"policy_01": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]], [[[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]], (3, 64, 64), float32)", "Discrete(7)", {}], "policy_02": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]], [[[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]], (3, 64, 64), float32)", "Discrete(7)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": "<function <lambda> at 0x7f1aa3767ef0>", "policies_to_train": ["policy_01"], "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": 100000, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": false, "hiddens": [], "double_q": false, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": true, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": null, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 2, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"policy_01": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]], [[[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]], (3, 64, 64), float32)", "Discrete(7)", {}], "policy_02": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]], [[[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]\n\n [[255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  ...\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]\n  [255. 255. 255. ... 255. 255. 255.]]], (3, 64, 64), float32)", "Discrete(7)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": "<function <lambda> at 0x7f1aa3767ef0>", "policies_to_train": ["policy_01"], "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": 100000, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": false, "hiddens": [], "double_q": false, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "time_since_restore": 1603.1851139068604, "timesteps_since_restore": 19200, "iterations_since_restore": 75, "warmup_time": 45.46659183502197, "perf": {"cpu_util_percent": 24.858064516129033, "ram_util_percent": 15.899999999999995}}
